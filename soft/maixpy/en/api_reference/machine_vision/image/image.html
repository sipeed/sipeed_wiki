<!DOCTYPE html>

<html lang="en"  class="">


<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="keywords" content="maixpy, k210, AIOT, edge computing">
    
    
    <meta name="description" content="maixpy  image (machine vision)">
    
    <meta name="generator" content="teedoc">
    <meta name="theme" content="teedoc-plugin-theme-default">
    
        
        <meta name="markdown-generator" content="teedoc-plugin-markdown-parser">
        
        <script>
MathJax = {"loader": {"load": ["output/svg"]}, "tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}};
</script>
        
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
        <meta name="html-generator" content="teedoc-plugin-jupyter-notebook-parser">
        
        <meta name="blog-generator" content="teedoc-plugin-blog">
        
        <script src="/static/js/theme_default/pre_main.js"></script>
        
        <link rel="stylesheet" href="/static/css/theme_default/viewer.min.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/css/prism.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/css/theme_default/dark.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/css/theme_default/light.css" type="text/css"/>
        
        <script src="/static/js/theme_default/jquery.min.js"></script>
        
        <script src="/static/js/theme_default/split.js"></script>
        
        <link rel="stylesheet" href="/static/css/search/style.css" type="text/css"/>
        
        <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?9cb07365544a53067c56c346c838181a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
        
            <script async src="https://www.googletagmanager.com/gtag/js?id=UA-119047820-5"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'UA-119047820-5');
    </script>
        
        <link rel="stylesheet" href="/static/css/gitalk/gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/css/gitalk/custom_gitalk.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/css/custom.css" type="text/css"/>
        
        <link rel="stylesheet" href="/static/js/thumbs_up/style.css" type="text/css"/>
        
    
    
    <title>image (machine vision) - Sipeed Wiki</title>
    
    <script type="text/javascript">js_vars = {"teedoc-plugin-ad-hint": {"type": "hint", "label": "NEW", "content": "<a href='https://www.kickstarter.com/projects/zepan/maixcam2-build-your-next-gen-4k-ai-camera' target='_blank'>Next-Gen 4K AI Camera MaixCAM2 is now crowdfunding! 10X faster than K230 in yolo task! </a>", "show_times": 2, "show_after_s": 432000, "date": "2026-03-02 19:00", "color": "#a0421d", "link_color": "#e53935", "link_bg_color": "#e6ae5c", "bg_color": "#ffcf89", "color_hover": "white", "bg_color_hover": "#f57c00", "close_color": "#eab971"}, "teedoc-plugin-thumbs-up": {"label_up": "Helpful", "label_down": "Not Helpful", "icon": "/static/images/thumbs_up/up.svg", "icon_clicked": "/static/images/thumbs_up/upped.svg", "url": "https://thumbs-up.sipeed.com", "show_up_count": true, "show_down_count": false, "msg_already_voted": "You have already voted", "msg_thanks": "Thanks for your vote", "msg_down_prompt": "Thanks to tell us where we can improve?(At least 10 characters)", "msg_down_prompt_error": "Message should be at least 10 characters and less than 256 characters", "msg_error": "Request server failed!"}}</script>
    <script type="text/javascript">metadata = {"tags": [], "date": false, "update": [], "ts": 0, "author": "", "brief": "", "cover": ""}</script>
</head>


<body class="type_doc">
    
    <div id="navbar">
        <div id="navbar_menu">
            <a class="site_title" href="/en/">
                
                    <img class="site_logo" src="/static/image/logo.svg" alt="sipeed wiki logo">
                
                
                    <h2>wiki</h2>
                
        </a>
            <a id="navbar_menu_btn"></a>
        </div>
        <div id="navbar_items">
            <div>
                <ul id="nav_left">
<li class=""><a  href="/en/">Products</a></li>
<li class="sub_items "><a >Software Docs</a><ul><li class=""><a  href="/maixpy/en/">MaixPy</a></li>
<li class=""><a  href="/soft/maixpy/zh/index.html">MaixPy_v1</a></li>
<li class=""><a  href="/soft/Lichee/zh/index.html">Lichee</a></li>
<li class=""><a  href="/ai/en/index.html">AI Guide</a></li>
</ul>
</li>
<li class=""><a target="_blank" href="https://maixhub.com">MaixHub</a></li>
<li class=""><a  href="/news/">News</a></li>
<li class=""><a  href="/en/faq.html">FAQ</a></li>
</ul>

            </div>
            <div>
                <ul id="nav_right">
<li class=""><a  href="/en/store.html"><img src='/static/image/shop.svg' style='height: 1.5em;vertical-align: middle;'></a></li>
<li class=""><a target="_blank" href="https://github.com/sipeed"><img src='/static/image/github.svg' style='height: 1.5em;vertical-align: middle;'></a></li>
<li class="sub_items "><a  ><img src='/static/image/language.svg' style='height: 1.5em;vertical-align: middle;'>&nbsp;English</a><ul><li class="active"><a  href="/soft/maixpy/en/api_reference/machine_vision/image/image.html">English</a></li>
<li class=""><a  href="/soft/maixpy/zh/api_reference/machine_vision/image/image.html">中文</a></li>
</ul></li>
</ul>

                <ul class="nav_plugins"><li><a id="google_translate_element"><img class="icon" src="/static/image/google_translate/translate.svg"/>Translate</a></li></ul><ul class="nav_plugins"><li><a id="themes" class="light"></a></li></ul><ul class="nav_plugins"><li><a id="search"><span class="icon"></span><span class="placeholder">Search</span>
                            <div id="search_hints">
                                <span id="search_input_hint">Input keyword. Using Space to separate multiple keywords</span>
                                <span id="search_loading_hint">Loading, please wait</span>
                                <span id="search_download_err_hint">Fail download, Please retry or check network</span>
                                <span id="search_other_docs_result_hint">Results form other doc</span>
                                <span id="search_curr_doc_result_hint">Result form current doc</span>
                            </div></a></li></ul>
            </div>
        </div>
    </div>
    
    <div id="wrapper">
        <div id="sidebar_wrapper">
            <div id="sidebar">
                <div id="sidebar_title">
                    
                </div>
                <ul class="show">
<li class="not_active with_link"><a href="/soft/maixpy/en/index.html"><span class="label">What is MaixPy-v1?</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/what_maix_do.html"><span class="label">What can MaixPy-v1 do?</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/maixpy_history.html"><span class="label">MaixPy Development History</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Getting started(Important)</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/how_to_read.html"><span class="label">How to read documents (Important!!!)</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/how_to_ask.html"><span class="label">How to ask correctly (Important!!!)</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Development Boards</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/get_hardware.html"><span class="label">Boards Comparison</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Boards introduction</span><span class=""></span></a></li>
<ul class="show">
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_dock.html"><span class="label">Maix Dock</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_bit.html"><span class="label">Maix Bit</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_amigo.html"><span class="label">Maix Amigo</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_duino.html"><span class="label">Maix Duino</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_cube.html"><span class="label">Maix Cube</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_go.html"><span class="label">Maix Go</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/maix_nano.html"><span class="label">Maix Nano</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/grove_ai_hat.html"><span class="label">Grove AI HAT</span><span class=""></span></a></li>
</ul>
<li class="not_active no_link"><a><span class="label">Related peripheral Modules (Accessories)</span><span class=""></span></a></li>
<ul class="show">
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/index.html"><span class="label">SP-MOD</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/grove/index.html"><span class="label">Grove</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/index.html"><span class="label">Other</span><span class=""></span></a></li>
</ul>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Setup Environment</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/env_install_driver.html"><span class="label">Install driver for development board</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/install_driver/dock.html"><span class="label">Maix Dock</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/install_driver/bit.html"><span class="label">Maix Bit</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/get_started/install_driver/amigo.html&from=/soft/maixpy/en/get_started/install_driver/amigo.html"><span class="label">Maix Amigo</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/get_started/install_driver/cube.html&from=/soft/maixpy/en/get_started/install_driver/cube.html"><span class="label">Maix Cube</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/install_driver/go.html"><span class="label">Maix Go</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/install_driver/nano.html"><span class="label">Maix Nano</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/get_started/install_driver/ai_hat.html&from=/soft/maixpy/en/get_started/install_driver/ai_hat.html"><span class="label">Grove AI HAT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/install_driver/duino.html"><span class="label">Maix Duino</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/upgrade_maixpy_firmware.html"><span class="label">How to update MaixPy firmware</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/env_maixpyide.html"><span class="label">Install and use MaixPy IDE</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/env_serial_tools.html"><span class="label">How to use serial terminal tool</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/get_started/mpfshell-lite/mpfshell-lite.html&from=/soft/maixpy/en/get_started/mpfshell-lite/mpfshell-lite.html"><span class="label">mpfshell-lite工具介绍</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/get_started/mpfshell-lite/mpfshell-lite-help.html&from=/soft/maixpy/en/get_started/mpfshell-lite/mpfshell-lite-help.html"><span class="label">Mpfshell-lite 使用手册</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/env_maixpyide.html"><span class="label">MaixPy IDE Instructions</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Update WIFI module firmware</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/upgrade_esp32_firmware.html"><span class="label">Update onboard ESP32 firmware</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/upgrade_esp8285_firmware.html"><span class="label">Update onboard ESP8285 firmware</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Start to run program</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_power_on.html"><span class="label">Power up your board</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_cam_lcd.html"><span class="label">First program: screen and camera</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_led_blink.html"><span class="label">Second program: LED</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Basic programming knowledge</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/knowledge_micropython.html"><span class="label">MaixPy Programming</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/knowledge_git_github.html"><span class="label">What is git and github?</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/knowledge_image.html"><span class="label">MaixPy Image Basics</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/knowledge_audio.html"><span class="label">MaixPy audio basics</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_fs.html"><span class="label">Introduction of Storage System</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_edit_file.html"><span class="label">Edit and run scripts code</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_upload_script.html"><span class="label">Upload scripts code to board</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/get_started_boot.html"><span class="label">Run scripts code when booting</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/board_info.html"><span class="label">Board configuration file</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/get_started/maixpy_get_started_video.html"><span class="label">Video tutorial</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Custome your own firmware</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/firmware/why_customize_firware.html"><span class="label">Why customizing firmware</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/firmware/online_compile.html"><span class="label">Online Compile</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/firmware/compile.html"><span class="label">Compile from source code</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/index.html"><span class="label">More usages</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">Basic Usage</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">CPU & RAM</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/others/system.html"><span class="label">Frequency and reset</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/others/mem.html"><span class="label">Memory configuration</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">GUI support</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/image_draw_font/image_draw_font.html"><span class="label">Chinese and other multi-language support</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/others/maixui.html"><span class="label">Maix UI</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/others/lvgl.html"><span class="label">Lvgl UI</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/others/pye.html"><span class="label">pye Editor </span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Network</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/network/network_config.html"><span class="label">Configure network</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/network/socket_usage.html"><span class="label">Socket communication</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Multimedia</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/media/audio.html"><span class="label">audio</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/media/video.html"><span class="label">video</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Game simulator</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/media/nes.html"><span class="label">NES game console</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Peripheral module expansion</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">On-chip peripherals</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/gpio.html"><span class="label">GPIO</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/i2c.html"><span class="label">I2C</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/pwm.html"><span class="label">PWM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/spi.html"><span class="label">SPI</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/timer.html"><span class="label">Timer</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/uart.html"><span class="label">UART</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/i2s.html"><span class="label">I2S</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/on_chip/wdt.html"><span class="label">WDT</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">SP-MOD</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_bt.html"><span class="label">BT Bluetooth transparent transmission</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_lora.html"><span class="label">LoRa Wireless Communication</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_rfid.html"><span class="label">RFID Radio Frequency Identification</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_tof.html"><span class="label">TOF Ranging</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_eink.html"><span class="label">Eink Electronic Ink Screen</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_lcd1.14.html"><span class="label">Lcd1.14 IPS screen</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_weather.html"><span class="label">Weather Module</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/sp_mod/sp_ethernet.html"><span class="label">Ethernet wired network port</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Grove</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/grove/grove_ultrasonic_ranger.html"><span class="label">Ultrasonic Ranger ranging</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/grove/grove_chainable_rgb_led.html"><span class="label">Chainable RGB LED light</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/grove/grove_rgb_led_ring.html"><span class="label">RGB LED Ring Strip</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Other peripherals</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/develop_kit_board/module_microphone.html"><span class="label">Sipeed Microphone Array</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/binocular_camera.html"><span class="label">Dual camera module</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/mlx90640.html"><span class="label">MLX90640 serial infrared lens</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/servo.html"><span class="label">Pwm servo</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/esp32_read_adc.html"><span class="label">ESP32 ADC</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/modules/others/onewire.html"><span class="label">onewire single bus</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Basic Image Processing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/basic/get_images.html"><span class="label">Get image</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/basic/display_images.html"><span class="label">Display image</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/basic/vary.html"><span class="label">MaixPy common images operations</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/basic/draw.html"><span class="label">Basic drawing and writing</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/basic/acc_image_deal.html"><span class="label">Hardware accelerated image processing</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Traditional Algorithm</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">Image Processing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/find_color_blob.html"><span class="label">Look for color blocks</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/image/find_qrcodes.html"><span class="label">QR code recognition</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Audio Processing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/speech/fft_waterfall.html"><span class="label">FFT waterfall chart</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/speech/recognizer_mfcc.html"><span class="label">Keyword Identification</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Neural Network Applications</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/basic/dnn_basic.html"><span class="label">Basic knowledge of deep neural networks</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/basic/maixpy_hardware_ai_basic.html"><span class="label">MaixPy AI hardware acceleration basic knowledge</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Image Processing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/image/face_detect.html"><span class="label">Face Detection</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/image/1000_type_classifier.html"><span class="label">1000 object classification</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/image/face_recognization.html"><span class="label">Face Recognition</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/image/self_learn_classifier.html"><span class="label">Self-learning classification</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Audio Processing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/speech/recognizer_cnn.html"><span class="label">Voice Recognition</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">How to tarin models</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">Train your own classification and detection model</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/train/maixhub.html"><span class="label">MaixHub Cloud Training</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/ai/train/local.html"><span class="label">Local Training</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/course/ai/train/local_windows_1.html&from=/soft/maixpy/en/course/ai/train/local_windows_1.html"><span class="label">Setup Windows local training envrioument</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/no_translate.html?ref=/soft/maixpy/zh/course/ai/train/local_windows_2.html&from=/soft/maixpy/en/course/ai/train/local_windows_2.html"><span class="label">Tutorial of Windows local training</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="active_parent no_link"><a><span class="label">API reference</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/index.html"><span class="label">Standard Library</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/cmath.html"><span class="label">cmath</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/gc.html"><span class="label">gc</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/math.html"><span class="label">math</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/sys.html"><span class="label">sys</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/ubinascii.html"><span class="label">ubinascii</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/ucollections.html"><span class="label">ucollections</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uctypes.html"><span class="label">uctypes</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uerrno.html"><span class="label">uerrno</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uhashlib.html"><span class="label">uhashlib</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uheapq.html"><span class="label">uheapq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/ujson.html"><span class="label">ujson</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uos.html"><span class="label">uos</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/ure.html"><span class="label">ure</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/usocket.html"><span class="label">usocket</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/ustruct.html"><span class="label">ustruct</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/utime.html"><span class="label">utime</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/standard/uzlib.html"><span class="label">uzlib</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/index.html"><span class="label">machinie</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/i2c.html"><span class="label">I2C</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/pwm.html"><span class="label">PWM</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/spi.html"><span class="label">SPI</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/timer.html"><span class="label">Timer</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/uart.html"><span class="label">UART</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/network.html"><span class="label">network</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine/wdt.html"><span class="label">WDT</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Maix</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/fpioa.html"><span class="label">FPIOA</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/gpio.html"><span class="label">GPIO</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/kpu.html"><span class="label">KPU</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/fft.html"><span class="label">FFT</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/i2s.html"><span class="label">I2S</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/freq.html"><span class="label">freq</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/Maix/utils.html"><span class="label">utils</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/builtin_py/index.html"><span class="label">helper</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/builtin_py/fm.html"><span class="label">fpioa_manager</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/builtin_py/board_info.html"><span class="label">board_info</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/builtin_py/pye.html"><span class="label">Micropython Editor</span><span class=""></span></a></li>
</ul>
</li>
<li class="active_parent with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/index.html"><span class="label">media</span><span class="sub_indicator"></span></a><ul class="show">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/lcd.html"><span class="label">lcd</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/sensor.html"><span class="label">sensor</span><span class=""></span></a></li>
<li class="active with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/image/image.html"><span class="label">image</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/media/video.html"><span class="label">video</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/media/audio.html"><span class="label">audio</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/media/nes.html"><span class="label">nes</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/isolated_word.html"><span class="label">isolated_word</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/machine_vision/maix_asr.html"><span class="label">maix_asr</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/index.html"><span class="label">extend</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/touchscreen.html"><span class="label">touchscreen</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/ultrasonic.html"><span class="label">modules.ultrasonic</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/ws2812.html"><span class="label">modules.ws2812</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/htpa.html"><span class="label">modules.htpa</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/api_reference/extend/onewire.html"><span class="label">modules.onewire</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/others/maixpy_faq.html"><span class="label">FAQ</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Advanced</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active no_link"><a><span class="label">Advanced Development</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/course/advance/project_framework.html"><span class="label">Source code Structure</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/advance/compile.html"><span class="label">How to compile MaixPy project</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/advance/add_c_module.html"><span class="label">How to add a MaixPy module in C</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/course/advance/pack_fs.html"><span class="label">Package File System</span><span class=""></span></a></li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Participate in Contribution</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/contribute/doc_convention.html"><span class="label">Participate in document writing (standard)</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/contribute/code_convention.html"><span class="label">Code Writing Specification</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active no_link"><a><span class="label">Sharing from Community & Users</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/share/recommend_articles.html"><span class="label">Featured Articles</span><span class=""></span></a></li>
<li class="not_active with_link"><a href="/soft/maixpy/en/share/open_projects.html"><span class="label">Open Source Project</span><span class=""></span></a></li>
<li class="not_active no_link"><a><span class="label">Everyone's experience sharing</span><span class="sub_indicator sub_indicator_collapsed"></span></a><ul class="">
<li class="not_active with_link"><a href="/soft/maixpy/en/share/my_share/index.html"><span class="label">Participate in experience sharing/sharing template</span><span class=""></span></a></li>
</ul>
</li>
</ul>
</li>
<li class="not_active with_link"><a href="/soft/maixpy/en/thanks.html"><span class="label">Open source thanks</span><span class=""></span></a></li>
</ul>

            </div>
        </div>
        <div id="article">
            <div id="menu_wrapper">
                <div id="menu">
                </div>
            </div>
            <div id="content_wrapper">
                <div id="content_body">
                    <div id="article_head">
                        <div id="article_title">
                            
                            <h1>image (machine vision)</h1>
                            
                        </div>
                        <div id="article_tags">
                            <ul>
                            
                            </ul>
                        </div>
                        <div id="article_info">
                        <div id="article_info_left">
                            <span class="article_author">
                                
                            </span>
                            
                        </div>
                        <div id="article_info_right">
                            
                            <div id="source_link">
                                <a href="https://github.com/sipeed/sipeed_wiki/blob/main/docs/soft/maixpy/en/api_reference/machine_vision/image/image.md" target="_blank">
                                    <span id='editPage'>Edit this page</span>
                                </a>
                            </div>
                            
                        </div>
                        </div>
                    </div>
                    <div id="article_tools">
                        <span></span>
                        <span id="toc_btn"></span>
                    </div>
                    <div id="update_history">
                        
                    </div>
                    <div id="article_content">
                        
                            <p>Ported to <code>openmv</code>, same function as <code>openmv</code></p>
<h2 id="Routine">Routine</h2>
<h3 id="Routine-1%3A-Find-green">Routine 1: Find green</h3>

<pre class="language-python"><code class="language-python">import sensor
import image
import lcd
import time
lcd.init()
sensor.reset()
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.run(1)
green_threshold = (0, 80, -70, -10, -0, 30)
while True:
img=sensor.snapshot()
blobs = img.find_blobs([green_threshold])
if blobs:
for b in blobs:
tmp=img.draw_rectangle(b[0:4])
tmp=img.draw_cross(b[5], b[6])
c=img.get_pixel(b[5], b[6])
lcd.display(img)
</code></pre>
<h3 id="Example-2%3A-Display-fps">Example 2: Display fps</h3>

<pre class="language-python"><code class="language-python">import sensor
import image
import lcd
import time

clock = time.clock()
lcd.init()
sensor.reset()
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.run(1)
sensor.skip_frames(30)
while True:
    clock.tick()
    img = sensor.snapshot()
    fps =clock.fps()
    img.draw_string(2,2, (&quot;%2.1ffps&quot; %(fps)), color=(0,128,0), scale=2)
    lcd.display(img)
</code></pre>
<h3 id="Example-3%3A-Scan-the-QR-code">Example 3: Scan the QR code</h3>

<pre class="language-python"><code class="language-python">import sensor
import image
import lcd
import time

clock = time.clock()
lcd.init()
sensor.reset()
sensor.set_pixformat(sensor.RGB565)
sensor.set_framesize(sensor.QVGA)
sensor.set_vflip(1)
sensor.run(1)
sensor.skip_frames(30)
while True:
    clock.tick()
    img = sensor.snapshot()
    res = img.find_qrcodes()
    fps =clock.fps()
    if len(res)&gt; 0:
        img.draw_string(2,2, res[0].payload(), color=(0,128,0), scale=2)
        print(res[0].payload())
    lcd.display(img)

</code></pre>
<blockquote>
<p>If a lens is used, the picture will be distorted and the picture needs to be corrected<br />
Use the <code>lens_corr</code> function to correct, such as <code>2.8</code>mm, <code>img.lens_corr(1.8)</code></p>
</blockquote>
<h2 id="Function">Function</h2>
<p>Function can also press <code>Ctrl+F</code> on this page and use the browser's search function to search <code>image.</code> to mark the function</p>
<h3 id="image.rgb_to_lab%28rgb_tuple%29">image.rgb_to_lab(rgb_tuple)</h3>
<p>Return the tuple of RGB888 format rgb_tuple (r, g, b) corresponding to the tuple (l, a, b) of LAB format.</p>
<blockquote>
<p>RGB888 means 8 bits each for red, green and blue (0-255). In LAB, the range of L is 0-100, and the range of a/b is -128 to 127.</p>
</blockquote>
<h3 id="image.lab_to_rgb%28lab_tuple%29">image.lab_to_rgb(lab_tuple)</h3>
<p>Return the tuple in LAB format lab_tuple (l, a, b) and the corresponding tuple (r, g, b) in RGB888 format.</p>
<blockquote>
<p>RGB888 means 8 bits each for red, green and blue (0-255). In LAB, the range of L is 0-100, and the range of a/b is -128 to 127.</p>
</blockquote>
<h3 id="image.rgb_to_grayscale%28rgb_tuple%29">image.rgb_to_grayscale(rgb_tuple)</h3>
<p>Returns the gray value corresponding to the tuple rgb_tuple (r, g, b) in RGB888 format.</p>
<blockquote>
<p>RGB888 means 8 bits each for red, green and blue (0-255). The gray value ranges from 0-255.</p>
</blockquote>
<h3 id="image.grayscale_to_rgb%28g_value%29">image.grayscale_to_rgb(g_value)</h3>
<p>Returns the tuple (r, g, b) in RGB888 format corresponding to the gray value g_value.</p>
<blockquote>
<p>RGB888 means 8 bits each for red, green and blue (0-255). The gray value ranges from 0-255.</p>
</blockquote>
<h3 id="image.load_decriptor%28path%29">image.load_decriptor(path)</h3>
<p>Load a descriptor object from the disk.</p>
<p>path is the path where the descriptor file is saved.</p>
<h3 id="image.save_descriptor%28path%2C-descriptor%29">image.save_descriptor(path, descriptor)</h3>
<p>Save the descriptor object descriptor to disk.</p>
<p>path is the path where the descriptor file is saved.</p>
<h3 id="image.match_descriptor%28descritor0%2C-descriptor1%5B%2C-threshold%3D70%5B%2C-filter_outliers%3DFalse%5D%5D%29">image.match_descriptor(descritor0, descriptor1[, threshold=70[, filter_outliers=False]])</h3>
<p>For LBP descriptors, this function returns an integer that reflects the difference between the two descriptors. This distance measurement is particularly necessary. This distance is a measure of similarity. The closer this measure is to 0, the better the matching of LBPF feature points.</p>
<p>For ORB descriptors, this function returns a kptmatch object. See above.</p>
<p>threshold is used to filter ambiguous matching services for ORB keys.<br />
A lower threshold value will be closely tied to the key point matching algorithm. The threshold value is between 0-100 (int). The default value is 70.</p>
<p>filter_outliers is used to filter outliers for ORB key points. Feature points allow users to increase the threshold value. The default setting is False.</p>
<h2 id="HaarCascade-class-feature-descriptor">HaarCascade class-feature descriptor</h2>
<p>Haar Cascade feature descriptors are used in the <code>image.find_features()</code> method. It has no methods for users to call.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.HaarCascade(path[, stages=Auto])</p>
<p>Load a Haar Cascade from a Haar Cascade binary file (format suitable for OpenMV Cam). If you pass the &quot;frontalface&quot; string instead of a path, this constructor will load a built-in frontalface Haar Cascade into memory. In addition, you can also load Haar Cascade into memory through &quot;eye&quot;. Finally, this method returns the loaded Haar Cascade object, which is used to use image.find_features().</p>
<p>The default value of stages is the number of stages in Haar Cascade. However, you can specify a lower value to speed up the running of the feature detector, which of course will bring a higher false alarm rate.</p>
<blockquote>
<p>You can make your own Haar Cascades to work with your OpenMV Cam. First, use Google to search for &quot;<thing> Haar Cascade&quot; to check if someone has made an OpenCV Haar Cascade for the object you want to detect. If not, then you need to do it yourself (huge workload). For how to make your own Haar Cascade, see here For how to convert OpenCV Haar Cascades into a mode that your OpenMV Cam can read, see this script</p>
</blockquote>
<p>Q: What is Haar Cascade?</p>
<p>Answer: Haar Cascade is a series of comparative checks to determine whether an object is present in the image. This series of comparative inspections is divided into multiple stages, and the operation of the latter stage is based on the completion of the previous stage. Contrast checking is not complicated, but a process like checking whether the center of the image is slightly more vertical than the edges. Large-scale inspections are carried out first in the early stage, and more and smaller areas are inspected in the later stage.</p>
<p>Q: How are Haar Cascades made?</p>
<p>Answer: Haar Cascades trains the generator algorithm through positive and negative images. For example, use hundreds of pictures containing cats (which have been marked as containing cats) and hundreds of pictures that do not contain cats (which have been marked differently) to train this generation algorithm. This generation algorithm will finally generate a Haar Cascades for detecting cats.</p>
<h2 id="Similarity-Class-Similarity-Object">Similarity Class-Similarity Object</h2>
<p>The similarity object is returned by <code>image.get_similarity</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.similarity</p>
<p>Please call the image.get_similarity() function to create this object.</p>
<h4 id="Method">Method</h4>
<h5 id="similarity.mean%28%29">similarity.mean()</h5>
<p>Returns the mean value of the similarity difference of 8x8 pixel block structure. The range is [-1/+1], where -1 is completely different and +1 is completely the same.</p>
<p>You can also get this value by index [0].</p>
<h5 id="similarity.stdev%28%29">similarity.stdev()</h5>
<p>Returns the standard deviation of the similarity difference of the 8x8 pixel block structure.</p>
<p>You can also get this value via index [1].</p>
<h5 id="similarity.min%28%29">similarity.min()</h5>
<p>Returns the minimum value of the similarity difference of the 8x8 pixel block structure. Where -1 is completely different and +1 is completely the same.</p>
<p>You can also get this value via index [2].</p>
<blockquote>
<p>By looking at this value, you can quickly determine whether any 8x8 pixel blocks between the two images are very different, that is, far below +1.</p>
</blockquote>
<h5 id="similarity.max%28%29">similarity.max()</h5>
<p>Returns the minimum value of the similarity difference of the 8x8 pixel block structure. Where -1 is completely different and +1 is completely the same.</p>
<p>You can also get this value through index [3].</p>
<blockquote>
<p>By looking at this value, you can quickly determine whether any 8x8 pixel blocks between the two images are the same. That is much larger than -1.</p>
</blockquote>
<h2 id="Histogram-class-histogram-object">Histogram class-histogram object</h2>
<p>The histogram object is returned by <code>image.get_histogram</code>. The grayscale histogram has a channel that contains multiple bins. All binaries are normalized so that their sum is 1. RGB565 has three channels containing multiple binary. All binaries are normalized so that their sum is 1.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.histogram</p>
<p>Please call the <code>image.get_histogram()</code> function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="histogram.bins%28%29">histogram.bins()</h4>
<p>Returns a list of floating point numbers in the gray histogram. You can also get this value by index [0].</p>
<h4 id="histogram.l_bins%28%29">histogram.l_bins()</h4>
<p>Returns the list of floating point numbers of the L channel of the RGB565 histogram LAB. You can also get this value by index [0].</p>
<h4 id="histogram.a_bins%28%29">histogram.a_bins()</h4>
<p>Returns the list of floating point numbers of the A channel of the RGB565 histogram LAB. You can also get this value via index [1].</p>
<h4 id="histogram.b_bins%28%29">histogram.b_bins()</h4>
<p>Returns the list of floating point numbers of channel B of RGB565 histogram LAB. You can also get this value via index [2].</p>
<h4 id="histogram.get_percentile%28percentile%29">histogram.get_percentile(percentile)</h4>
<p>Calculate the CDF of the histogram channel and return a value of the histogram passed in percentile (0.0-1.0) (floating point number).</p>
<p>Therefore, if you pass in 0.1, the method will tell you which binary will make the accumulator cross 0.1 when it is added to the accumulator.</p>
<p>This is very effective for determining the minimum value (0.1) and max (0.9) of the color distribution when there is no anomalous utility to spoil your adaptive color tracking results.</p>
<h4 id="histogram.get_threhsold%28%29">histogram.get_threhsold()</h4>
<p>Use Otsu’s method to calculate the optimal threshold, dividing each channel of the histogram into two halves. This method returns an image.threshold object. This method is particularly useful for determining the optimal image.binary() threshold.</p>
<h4 id="histogram.get_statistics%28%29">histogram.get_statistics()</h4>
<p>Calculate the average, median, mode, standard deviation, minimum, maximum, lower quartile, and upper quartile of each color channel in the histogram, and return a statistics object. You can also use histogram.statistics() and histogram.get_stats() as aliases for this method.</p>
<h2 id="Percentile-class-percentage-value-object">Percentile class-percentage value object</h2>
<p>The percentage value object is returned by <code>histogram.get_percentile</code>. The gray percentage value has one channel. Do not use l_<em>, a_</em> or b_* methods. The RGB565 percentage value has three channels. Use the l_<em>, a_</em> and b_* methods.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.percentile</p>
<p>Please call the histogram.get_percentile() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="percentile.value%28%29">percentile.value()</h4>
<p>Returns the gray percentage value (the value range is 0-255).</p>
<p>You can also get this value by index [0].</p>
<h4 id="percentile.l_value%28%29">percentile.l_value()</h4>
<p>Returns the percentage value of the L channel of RGB565 LAB (value range is 0-100).</p>
<p>You can also get this value by index [0].</p>
<h4 id="percentile.a_value%28%29">percentile.a_value()</h4>
<p>Returns the percentage value of the A channel of RGB565 LAB (the value range is -128-127).</p>
<p>You can also get this value via index [1].</p>
<h4 id="percentile.b_value%28%29">percentile.b_value()</h4>
<p>Returns the percentage value of the B channel of RGB565 LAB (the value range is -128-127).</p>
<p>You can also get this value via index [2].</p>
<h2 id="Threhsold-Class-Threshold-Object">Threhsold Class-Threshold Object</h2>
<p>The threshold object is returned by histogram.get_threshold.</p>
<p>Grayscale images have one channel. There are no l_<em>, a_</em>, and b_* methods.</p>
<p>The RGB565 threshold has three channels. Use l_<em>, a_</em>, and b_* methods.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.threshold</p>
<p>Please call the histogram.get_threshold() function to create this object.</p>
<h4 id="Method">Method</h4>
<h4 id="threhsold.value%28%29">threhsold.value()</h4>
<p>Returns the threshold of the grayscale image (between 0 and 255).</p>
<p>You can also get this value by index [0].</p>
<h4 id="threhsold.l_value%28%29">threhsold.l_value()</h4>
<p>Return the L threshold (between 0 and 100) in the RGB565 image LAB.</p>
<p>You can also get this value by index [0].</p>
<h4 id="threhsold.a_value%28%29">threhsold.a_value()</h4>
<p>Return the A threshold (between -128 and 127) in the RGB565 image LAB.</p>
<p>You can also get this value via index [1].</p>
<h4 id="threhsold.b_value%28%29">threhsold.b_value()</h4>
<p>Return the B threshold (between -128 and 127) in the RGB565 image LAB.</p>
<p>You can also get this value via index [2].</p>
<h2 id="class-Statistics-%E2%80%93-Statistical-data-object">class Statistics – Statistical data object</h2>
<p>The statistical data object is returned by histogram.get_statistics or image.get_statistics.</p>
<p>The grayscale statistics have one channel, using methods other than l_<em>, a_</em> or b_*.</p>
<p>The RGB565 percentage value has three channels. Use the l_<em>, a_</em> and b_* methods.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.statistics<br />
Please call histogram.get_statistics() or image.get_statistics() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="statistics.mean%28%29">statistics.mean()</h4>
<p>Returns the average gray value (0-255) (int).</p>
<p>You can also get this value by index [0].</p>
<h4 id="statistics.median%28%29">statistics.median()</h4>
<p>Returns the median gray value (0-255) (int).</p>
<p>You can also get this value via index [1].</p>
<h4 id="statistics.mode%28%29">statistics.mode()</h4>
<p>Returns the gray mode value (0-255) (int).</p>
<p>You can also get this value via index [2].</p>
<h4 id="statistics.stdev%28%29">statistics.stdev()</h4>
<p>Returns the gray standard deviation (0-255) (int).</p>
<p>You can also get this value through index [3].</p>
<h4 id="statistics.min%28%29">statistics.min()</h4>
<p>Returns the minimum value of gray scale (0-255) (int).</p>
<p>You can also get this value via index [4].</p>
<h4 id="statistics.max%28%29">statistics.max()</h4>
<p>Returns the maximum gray value (0-255) (int).</p>
<p>You can also get this value via index [5].</p>
<h4 id="statistics.lq%28%29">statistics.lq()</h4>
<p>Returns the quarter value (0-255) (int) in grayscale.</p>
<p>You can also get this value via index [6].</p>
<h4 id="statistics.uq%28%29">statistics.uq()</h4>
<p>Returns the quarter value of the gray scale (0-255) (int).</p>
<p>You can also get this value via index [7].</p>
<h4 id="statistics.l_mean%28%29">statistics.l_mean()</h4>
<p>Returns the mean value of L (0-255) (int) in RGB5656 LAB.</p>
<p>You can also get this value by index [0].</p>
<h4 id="statistics.l_median%28%29">statistics.l_median()</h4>
<p>Returns the median (0-255) (int) of L in RGB5656 LAB.</p>
<p>You can also get this value via index [1].</p>
<h4 id="statistics.l_mode%28%29">statistics.l_mode()</h4>
<p>Returns the mode (0-255) (int) of L in RGB5656 LAB.</p>
<p>You can also get this value via index [2].</p>
<h4 id="statistics.l_stdev%28%29">statistics.l_stdev()</h4>
<p>Returns the standard deviation value of L in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value through index [3].</p>
<h4 id="statistics.l_min%28%29">statistics.l_min()</h4>
<p>Returns the minimum value of L in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value via index [4].</p>
<h4 id="statistics.l_max%28%29">statistics.l_max()</h4>
<p>Returns the maximum value of L in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value via index [5].</p>
<h4 id="statistics.l_lq%28%29">statistics.l_lq()</h4>
<p>Returns the lower quartile of L in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value via index [6].</p>
<h4 id="statistics.l_uq%28%29">statistics.l_uq()</h4>
<p>Returns the upper quartile (0-255) (int) of L in RGB5656 LAB.</p>
<p>You can also get this value via index [7].</p>
<h4 id="statistics.a_mean%28%29">statistics.a_mean()</h4>
<p>Returns the mean value (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value through index [8].</p>
<h4 id="statistics.a_median%28%29">statistics.a_median()</h4>
<p>Returns the median value (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value via index [9].</p>
<h4 id="statistics.a_mode%28%29">statistics.a_mode()</h4>
<p>Returns the mode (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value via index [10].</p>
<h4 id="statistics.a_stdev%28%29">statistics.a_stdev()</h4>
<p>Returns the standard deviation value (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value through the index [11].</p>
<h4 id="statistics.a_min%28%29">statistics.a_min()</h4>
<p>Returns the minimum value of A in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value through the index [12].</p>
<h4 id="statistics.a_max%28%29">statistics.a_max()</h4>
<p>Returns the maximum value of A (0-255) (int) in RGB5656 LAB.</p>
<p>You can also get this value through the index [13].</p>
<h4 id="statistics.a_lq%28%29">statistics.a_lq()</h4>
<p>Returns the lower quartile (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value through the index [14].</p>
<h4 id="statistics.a_uq%28%29">statistics.a_uq()</h4>
<p>Returns the upper quartile (0-255) (int) of A in RGB5656 LAB.</p>
<p>You can also get this value via index [15].</p>
<h4 id="statistics.b_mean%28%29">statistics.b_mean()</h4>
<p>Returns the mean value of B in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value via index [16].</p>
<h4 id="statistics.b_median%28%29">statistics.b_median()</h4>
<p>Returns the median (0-255) (int) of B in RGB5656 LAB.</p>
<p>You can also get this value via index [17].</p>
<h4 id="statistics.b_mode%28%29">statistics.b_mode()</h4>
<p>Returns the mode (0-255) (int) of B in RGB5656 LAB.</p>
<p>You can also get this value through the index [18].</p>
<h4 id="statistics.b_stdev%28%29">statistics.b_stdev()</h4>
<p>Returns the standard deviation of B in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value through index [19].</p>
<h4 id="statistics.b_min%28%29">statistics.b_min()</h4>
<p>Returns the minimum value (0-255) (int) of B in RGB5656 LAB.</p>
<p>You can also get this value via index [20].</p>
<h4 id="statistics.b_max%28%29">statistics.b_max()</h4>
<p>Returns the maximum value of B in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value through the index [21].</p>
<h4 id="statistics.b_lq%28%29">statistics.b_lq()</h4>
<p>Returns the lower quartile of B in RGB5656 LAB (0-255) (int).</p>
<p>You can also get this value through the index [22].</p>
<h4 id="statistics.b_uq%28%29">statistics.b_uq()</h4>
<p>Returns the upper quartile (0-255) (int) of B in RGB5656 LAB.</p>
<p>You can also get this value through the index [23].</p>
<h2 id="Blob-class-color-block-object">Blob class-color block object</h2>
<p>The color block object is returned by <code>image.find_blobs</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.blob</p>
<p>Please call the image.find_blobs() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="blob.rect%28%29">blob.rect()</h4>
<p>Returns a rectangular tuple (x, y, w, h), which is used in other image methods such as image.draw_rectangle of the color block bounding box.</p>
<h4 id="blob.x%28%29">blob.x()</h4>
<p>Returns the x coordinate (int) of the bounding box of the color patch.</p>
<p>You can also get this value by index [0].</p>
<h4 id="blob.y%28%29">blob.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of the color patch.</p>
<p>You can also get this value via index [1].</p>
<h4 id="blob.w%28%29">blob.w()</h4>
<p>Returns the w coordinate (int) of the bounding box of the color patch.</p>
<p>You can also get this value via index [2].</p>
<h4 id="blob.h%28%29">blob.h()</h4>
<p>Returns the h coordinate (int) of the bounding box of the color patch.</p>
<p>You can also get this value through index [3].</p>
<h4 id="blob.pixels%28%29">blob.pixels()</h4>
<p>Returns the number of pixels that are part of the color block (int).</p>
<p>You can also get this value via index [4].</p>
<h4 id="blob.cx%28%29">blob.cx()</h4>
<p>Returns the center x position of the color block (int).</p>
<p>You can also get this value via index [5].</p>
<h4 id="blob.cy%28%29">blob.cy()</h4>
<p>Returns the center x position of the color block (int).</p>
<p>You can also get this value via index [6].</p>
<h4 id="blob.rotation%28%29">blob.rotation()</h4>
<p>Returns the rotation of the color block (unit: radians). If the color block resembles a pencil or pen, then this value is the only value between 0-180. If the color block is round, then this value has no effect. If this color block has no symmetry at all, you can only get a 0-360 degree rotation.</p>
<p>You can also get this value via index [7].</p>
<h4 id="blob.code%28%29">blob.code()</h4>
<p>Returns a 16-bit binary number, where one bit is set for each color threshold, which is part of the color block. For example, if you use image.find_blobs to find three color thresholds, this color block can be set to 0/1/2 bits. Note: Unless you call image.find_blobs with merge=True, you can only set one bit per color block. Then multiple color blocks with different color thresholds can be merged together. You can also use this method and multiple thresholds to implement color code tracking.</p>
<p>You can also get this value through index [8].</p>
<h4 id="blob.count%28%29">blob.count()</h4>
<p>Returns the number of multiple color blocks merged into this color block. Only when you call image.find_blobs with merge=True, this number is not 1.</p>
<p>You can also get this value via index [9].</p>
<h4 id="blob.area%28%29">blob.area()</h4>
<p>Return the border area around the color block (w * h)</p>
<h4 id="blob.density%28%29">blob.density()</h4>
<p>Returns the density ratio of this color patch. This is the number of pixels in the bounding box area of ​​the color block. In general, a lower density ratio means that the object is not locked well.</p>
<h2 id="Line-class-line-object">Line class-line object</h2>
<p>The line object is returned by <code>image.find_lines</code>, <code>image.find_line_segments</code> or <code>image.get_regression</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.line</p>
<p>Please call image.find_lines(), image.find_line_segments(), or image.get_regression() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="line.line%28%29">line.line()</h4>
<p>Return a straight line tuple (x1, y1, x2, y2) for use in other image methods such as image.draw_line.</p>
<h4 id="line.x1%28%29">line.x1()</h4>
<p>Returns the x coordinate component of the p1 vertex of the line.</p>
<p>You can also get this value by index [0].</p>
<h4 id="line.y1%28%29">line.y1()</h4>
<p>Returns the p1 y component of the line.</p>
<p>You can also get this value via index [1].</p>
<h4 id="line.x2%28%29">line.x2()</h4>
<p>Returns the p2 x component of the line.</p>
<p>You can also get this value via index [2].</p>
<h4 id="line.y2%28%29">line.y2()</h4>
<p>Returns the p2 y component of the line.</p>
<p>You can also get this value through index [3].</p>
<h4 id="line.length%28%29">line.length()</h4>
<p>The length of the return line is sqrt(((x2-x1)^2) + ((y2-y1)^2).</p>
<p>You can also get this value via index [4].</p>
<h4 id="line.magnitude%28%29">line.magnitude()</h4>
<p>Returns the length of the straight line after Hough transformation.</p>
<p>You can also get this value via index [5].</p>
<h4 id="line.theta%28%29">line.theta()</h4>
<p>Returns the angle of the straight line after Hough transformation (0-179 degrees).</p>
<p>You can also get this value via index [7].</p>
<h4 id="line.rho%28%29">line.rho()</h4>
<p>Returns the p-value of the straight line after Hough transform.</p>
<p>You can also get this value through index [8].</p>
<h2 id="Circle-class-round-object">Circle class-round object</h2>
<p>The circular object is returned by <code>image.find_circles</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.circle</p>
<p>Please call the image.find_circles() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="circle.x%28%29">circle.x()</h4>
<p>Returns the x position of the circle.</p>
<p>You can also get this value by index [0].</p>
<h4 id="circle.y%28%29">circle.y()</h4>
<p>Returns the y position of the circle.</p>
<p>You can also get this value via index [1].</p>
<h4 id="circle.r%28%29">circle.r()</h4>
<p>Returns the radius of the circle.</p>
<p>You can also get this value via index [2].</p>
<h4 id="circle.magnitude%28%29">circle.magnitude()</h4>
<p>Returns the size of the circle.</p>
<p>You can also get this value through index [3].</p>
<h2 id="Rect-Class-Rectangle-Object">Rect Class-Rectangle Object</h2>
<p>The rectangle object is returned by <code>image.find_rects</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.rect</p>
<p>Please call the image.find_rects() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="rect.corners%28%29">rect.corners()</h4>
<p>Returns a list of four tuples (x, y) consisting of the four corners of the rectangular object. The four corners are usually returned in clockwise order starting from the upper left corner.</p>
<h4 id="rect.rect%28%29">rect.rect()</h4>
<p>Returns a rectangle tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of the bounding box of the rectangle.</p>
<h4 id="rect.x%28%29">rect.x()</h4>
<p>Returns the x position of the upper left corner of the rectangle.</p>
<p>You can also get this value by index [0].</p>
<h4 id="rect.y%28%29">rect.y()</h4>
<p>Returns the y position of the upper left corner of the rectangle.</p>
<p>You can also get this value via index [1].</p>
<h4 id="rect.w%28%29">rect.w()</h4>
<p>Returns the width of the rectangle.</p>
<p>You can also get this value via index [2].</p>
<h4 id="rect.h%28%29">rect.h()</h4>
<p>Returns the height of the rectangle.</p>
<p>You can also get this value through index [3].</p>
<h4 id="rect.magnitude%28%29">rect.magnitude()</h4>
<p>Returns the size of the rectangle.</p>
<p>You can also get this value via index [4].</p>
<h2 id="QRCode-class-QR-code-object">QRCode class-QR code object</h2>
<p>The QR code object is returned by <code>image.find_qrcodes</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.qrcode</p>
<p>Please call the image.find_qrcodes() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="qrcode.corners%28%29">qrcode.corners()</h4>
<p>Returns a list of four tuples (x, y) consisting of the four corners of the object. The four corners are usually returned in clockwise order starting from the upper left corner.</p>
<h4 id="qrcode.rect%28%29">qrcode.rect()</h4>
<p>Returns a rectangular tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of the bounding box of the QR code.</p>
<h4 id="qrcode.x%28%29">qrcode.x()</h4>
<p>Returns the x coordinate (int) of the bounding box of the QR code.</p>
<p>You can also get this value by index [0].</p>
<h4 id="qrcode.y%28%29">qrcode.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of the QR code.</p>
<p>You can also get this value via index [1].</p>
<h4 id="qrcode.w%28%29">qrcode.w()</h4>
<p>Returns the w coordinate (int) of the bounding box of the QR code.</p>
<p>You can also get this value via index [2].</p>
<h4 id="qrcode.h%28%29">qrcode.h()</h4>
<p>Returns the h coordinate (int) of the bounding box of the QR code.</p>
<p>You can also get this value through index [3].</p>
<h4 id="qrcode.payload%28%29">qrcode.payload()</h4>
<p>Returns the string of the QR code payload, such as URL.</p>
<p>You can also get this value via index [4].</p>
<h4 id="qrcode.version%28%29">qrcode.version()</h4>
<p>Returns the version number (int) of the QR code.</p>
<p>You can also get this value via index [5].</p>
<h4 id="qrcode.ecc_level%28%29">qrcode.ecc_level()</h4>
<p>Returns the ECC level of the QR code (int).</p>
<p>You can also get this value via index [6].</p>
<h4 id="qrcode.mask%28%29">qrcode.mask()</h4>
<p>Returns the mask (int) of the QR code.</p>
<p>You can also get this value via index [7].</p>
<h4 id="qrcode.data_type%28%29">qrcode.data_type()</h4>
<p>Returns the data type of the QR code.</p>
<p>You can also get this value through index [8].</p>
<h4 id="qrcode.eci%28%29">qrcode.eci()</h4>
<p>Returns the ECI of the QR code. ECI stores the code of the data bytes stored in the QR code. If you want to process a QR code that contains more than standard ASCII text, you need to check this value.</p>
<p>You can also get this value via index [9].</p>
<h4 id="qrcode.is_numeric%28%29">qrcode.is_numeric()</h4>
<p>If the data type of the QR code is digital, it returns True.</p>
<h4 id="qrcode.is_alphanumeric%28%29">qrcode.is_alphanumeric()</h4>
<p>If the data type of the QR code is alphanumeric, it returns True.</p>
<h4 id="qrcode.is_binary%28%29">qrcode.is_binary()</h4>
<p>If the data type of the QR code is binary, it returns True. If you are serious about handling all types of text, you need to check whether eci is True to determine the text encoding of the data. Usually it is just standard ASCII, but it may also be UTF8 with two-byte characters.</p>
<h4 id="qrcode.is_kanji%28%29">qrcode.is_kanji()</h4>
<p>If the data type of the QR code is Kanji, it returns True. After setting it to True, you need to decode the string yourself, because each character of the Kanji is 10 digits, and MicroPython does not support parsing this type of text.</p>
<h2 id="AprilTag-Class-%E2%80%93-AprilTag-Object">AprilTag Class – AprilTag Object</h2>
<p>The AprilTag object is returned by <code>image.find_apriltags</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.apriltag</p>
<p>Please call the image.find_apriltags() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="apriltag.corners%28%29">apriltag.corners()</h4>
<p>Returns a list of four tuples (x, y) consisting of the four corners of the object. The four corners are usually returned in clockwise order starting from the upper left corner.</p>
<h4 id="apriltag.rect%28%29">apriltag.rect()</h4>
<p>Return a rectangular tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of AprilTag bounding box.</p>
<h4 id="apriltag.x%28%29">apriltag.x()</h4>
<p>Returns the x coordinate (int) of the AprilTag bounding box.</p>
<p>You can also get this value by index [0].</p>
<h4 id="apriltag.y%28%29">apriltag.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of AprilTag.</p>
<p>You can also get this value via index [1].</p>
<h4 id="apriltag.w%28%29">apriltag.w()</h4>
<p>Returns the w coordinate (int) of the bounding box of AprilTag.</p>
<p>You can also get this value via index [2].</p>
<h4 id="apriltag.h%28%29">apriltag.h()</h4>
<p>Returns the h coordinate (int) of the bounding box of AprilTag.</p>
<p>You can also get this value through index [3].</p>
<h4 id="apriltag.id%28%29">apriltag.id()</h4>
<p>Returns the numeric ID of AprilTag.</p>
<p>TAG16H5 -&gt; 0 to 29<br />
TAG25H7 -&gt; 0 to 241<br />
TAG25H9 -&gt; 0 to 34<br />
TAG36H10 -&gt; 0 to 2319<br />
TAG36H11 -&gt; 0 to 586<br />
ARTOOLKIT -&gt; 0 to 511<br />
You can also get this value via index [4].</p>
<h4 id="apriltag.family%28%29">apriltag.family()</h4>
<p>Return to AprilTag's digital home.</p>
<p>image.TAG16H5<br />
image.TAG25H7<br />
image.TAG25H9<br />
image.TAG36H10<br />
image.TAG36H11<br />
image.ARTOOLKIT<br />
You can also get this value via index [5].</p>
<h4 id="apriltag.cx%28%29">apriltag.cx()</h4>
<p>Returns the center x position (int) of AprilTag.</p>
<p>You can also get this value via index [6].</p>
<h4 id="apriltag.cy%28%29">apriltag.cy()</h4>
<p>Returns the center y position (int) of AprilTag.</p>
<p>You can also get this value via index [7].</p>
<h4 id="apriltag.rotation%28%29">apriltag.rotation()</h4>
<p>Returns the curl of AprilTag in radians (int).</p>
<p>You can also get this value through index [8].</p>
<h4 id="apriltag.decision_margin%28%29">apriltag.decision_margin()</h4>
<p>Return the color saturation of AprilTag matching (value 0.0-1.0), where 1.0 is the best.</p>
<p>You can also get this value via index [9].</p>
<h4 id="apriltag.hamming%28%29">apriltag.hamming()</h4>
<p>Returns the acceptable digital error value of AprilTag.</p>
<p>TAG16H5 -&gt; can accept up to 0 bit errors<br />
TAG25H7 -&gt; can accept up to 1 bit error<br />
TAG25H9 -&gt; Accept up to 3 errors<br />
TAG36H10 -&gt; can accept up to 3 errors<br />
TAG36H11 -&gt; can accept up to 4 errors<br />
ARTOOLKIT -&gt; can accept up to 0 errors<br />
You can also get this value via index [10].</p>
<h4 id="apriltag.goodness%28%29">apriltag.goodness()</h4>
<p>Returns the color saturation of the AprilTag image (value 0.0-1.0), where 1.0 is the best.</p>
<blockquote>
<p>Currently this value is usually 0.0. In the future, we can enable a function called &quot;tag refinement&quot; to realize the detection of smaller AprilTags. However, this feature now reduces the frame rate below 1 FPS.</p>
</blockquote>
<p>You can also get this value through the index [11].</p>
<h4 id="apriltag.x_translation%28%29">apriltag.x_translation()</h4>
<p>Returns the transformation in the x direction from the camera. The unit of the distance is unknown.</p>
<p>This method is useful for determining the position of AprilTag far away from the camera. However, factors such as the size of AprilTag and the lens you use will affect the determination of the attribution of the X unit. For ease of use, we recommend that you use a lookup table to convert the output of this method into useful information for your application.</p>
<p>Note: The direction here is from left to right.</p>
<p>You can also get this value through the index [12].</p>
<h4 id="apriltag.y_translation%28%29">apriltag.y_translation()</h4>
<p>Returns the transformation in the y direction from the camera. The unit of the distance is unknown.</p>
<p>This method is useful for determining the position of AprilTag far away from the camera. However, the size of the AprilTag and the lens you use will affect the determination of the Y unit. For ease of use, we recommend that you use a lookup table to convert the output of this method into useful information for your application.</p>
<p>Note: The direction here is from top to bottom.</p>
<p>You can also get this value through the index [13].</p>
<h4 id="apriltag.z_translation%28%29">apriltag.z_translation()</h4>
<p>Returns the transformation in the z direction from the camera. The unit of the distance is unknown.</p>
<p>This method is useful for determining the position of AprilTag far away from the camera. However, the size of the AprilTag and the lens you use will affect the determination of the Z unit. For ease of use, we recommend that you use a lookup table to convert the output of this method into useful information for your application.</p>
<p>Note: The direction here is from front to back.</p>
<p>You can also get this value through the index [14].</p>
<h4 id="apriltag.x_rotation%28%29">apriltag.x_rotation()</h4>
<p>Returns the rotation of AprilTag on the X plane in radians. Example: Looking at AprilTag, move the camera from left to right.</p>
<p>You can also get this value via index [15].</p>
<h4 id="apriltag.y_rotation%28%29">apriltag.y_rotation()</h4>
<p>Returns the rotation of AprilTag in radians on the Y plane. Example: Visually observe AprilTag and move the camera from top to bottom.</p>
<p>You can also get this value via index [16].</p>
<h4 id="apriltag.z_rotation%28%29">apriltag.z_rotation()</h4>
<p>Returns the rotation of the AprilTag in radians on the Z plane. Example: Look at AprilTag and rotate the camera.</p>
<p>Note: This is just a renamed version of apriltag.rotation().</p>
<p>You can also get this value via index [17].</p>
<h2 id="DataMatrix-Class-Data-Matrix-Object">DataMatrix Class-Data Matrix Object</h2>
<p>The data matrix object is returned by <code>image.find_datamatrices</code>.</p>
<h2 id="Constructor">Constructor</h2>
<p>class image.datamatrix</p>
<p>Please call the image.find_datamatrices() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="datamatrix.corners%28%29">datamatrix.corners()</h4>
<p>Returns a list of four tuples (x, y) consisting of the four corners of the object. The four corners are usually returned in clockwise order starting from the upper left corner.</p>
<h4 id="datamatrix.rect%28%29">datamatrix.rect()</h4>
<p>Return a rectangular tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of the bounding box of the data matrix.</p>
<h4 id="datamatrix.x%28%29">datamatrix.x()</h4>
<p>Returns the x coordinate (int) of the bounding box of the data matrix.</p>
<p>You can also get this value by index [0].</p>
<h4 id="datamatrix.y%28%29">datamatrix.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of the data matrix.</p>
<p>You can also get this value via index [1].</p>
<h4 id="datamatrix.w%28%29">datamatrix.w()</h4>
<p>Returns the w width of the bounding box of the data matrix.</p>
<p>You can also get this value via index [2].</p>
<h4 id="datamatrix.h%28%29">datamatrix.h()</h4>
<p>Returns the h height of the bounding box of the data matrix.</p>
<p>You can also get this value through index [3].</p>
<h4 id="datamatrix.payload%28%29">datamatrix.payload()</h4>
<p>Returns the string of the payload of the data matrix. Example: string.</p>
<p>You can also get this value via index [4].</p>
<h4 id="datamatrix.rotation%28%29">datamatrix.rotation()</h4>
<p>Returns the curl (floating point number) of the data matrix in radians.</p>
<p>You can also get this value via index [5].</p>
<h4 id="datamatrix.rows%28%29">datamatrix.rows()</h4>
<p>Returns the number of rows of the data matrix (int).</p>
<p>You can also get this value via index [6].</p>
<h4 id="datamatrix.columns%28%29">datamatrix.columns()</h4>
<p>Returns the number of columns of the data matrix (int).</p>
<p>You can also get this value via index [7].</p>
<h4 id="datamatrix.capacity%28%29">datamatrix.capacity()</h4>
<p>Returns the number of characters that this data matrix can hold.</p>
<p>You can also get this value through index [8].</p>
<h4 id="datamatrix.padding%28%29">datamatrix.padding()</h4>
<p>Returns the number of unused characters in this data matrix.</p>
<p>You can also get this value via index [9].</p>
<h2 id="BarCode-Class-Barcode-Object">BarCode Class-Barcode Object</h2>
<p>The barcode object is returned by image.find_barcodes.</p>
<h2 id="Constructor">Constructor</h2>
<p>class image.barcode</p>
<p>Please call the image.find_barcodes() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="barcode.corners%28%29">barcode.corners()</h4>
<p>Returns a list of four tuples (x, y) consisting of the four corners of the object. The four corners are usually returned in clockwise order starting from the upper left corner.</p>
<h4 id="barcode.rect%28%29">barcode.rect()</h4>
<p>Return a rectangular tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of the bounding box of the data matrix.</p>
<h4 id="barcode.x%28%29">barcode.x()</h4>
<p>Returns the x coordinate (int) of the bounding box of the barcode.</p>
<p>You can also get this value by index [0].</p>
<h4 id="barcode.y%28%29">barcode.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of the barcode.</p>
<p>You can also get this value via index [1].</p>
<h4 id="barcode.w%28%29">barcode.w()</h4>
<p>Returns the w width (int) of the bounding box of the barcode.</p>
<p>You can also get this value via index [2].</p>
<h4 id="barcode.h%28%29">barcode.h()</h4>
<p>Returns the h height (int) of the bounding box of the barcode.</p>
<p>You can also get this value through index [3].</p>
<h4 id="barcode.payload%28%29">barcode.payload()</h4>
<p>Returns the string of the payload of the barcode. Example: Quantity.</p>
<p>You can also get this value via index [4].</p>
<h4 id="barcode.type%28%29">barcode.type()</h4>
<p>Returns the enumeration type (int) of the barcode.</p>
<p>You can also get this value via index [5].</p>
<p>image.EAN2<br />
image.EAN5<br />
image.EAN8<br />
image.UPCE<br />
image.ISBN10<br />
image.UPCA<br />
image.EAN13<br />
image.ISBN13<br />
image.I25<br />
image.DATABAR<br />
image.DATABAR_EXP<br />
image.CODABAR<br />
image.CODE39<br />
image.PDF417-To be enabled in the future (e.g. not yet available for normal use).<br />
image.CODE93<br />
image.CODE128</p>
<h4 id="barcode.rotation%28%29">barcode.rotation()</h4>
<p>Returns the curl (floating point number) of the barcode in radians.</p>
<p>You can also get this value via index [6].</p>
<h4 id="barcode.quality%28%29">barcode.quality()</h4>
<p>Returns the number of times the barcode was detected in the image (int).</p>
<p>When scanning a barcode, each new scan line can decode the same barcode. Each time this process is performed, the value of the barcode will increase accordingly.</p>
<p>You can also get this value via index [7].</p>
<h2 id="Displacement-class-displacement-object">Displacement class-displacement object</h2>
<p>The displacement object is returned by image.find_displacement.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.displacement</p>
<p>Please call the image.find_displacement() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="displacement.x_translation%28%29">displacement.x_translation()</h4>
<p>Returns the x translation pixel between two images. This is a precise sub-pixel, so it is a floating point number.</p>
<p>You can also get this value by index [0].</p>
<h4 id="displacement.y_translation%28%29">displacement.y_translation()</h4>
<p>Returns the y translation pixel between two images. This is a precise sub-pixel, so it is a floating point number.</p>
<p>You can also get this value via index [1].</p>
<h4 id="displacement.rotation%28%29">displacement.rotation()</h4>
<p>Returns the z-shift pixel between two images. This is a precise sub-pixel, so it is a floating point number.</p>
<p>You can also get this value via index [2].</p>
<h4 id="displacement.scale%28%29">displacement.scale()</h4>
<p>Returns the arc of rotation between two images.</p>
<p>You can also get this value through index [3].</p>
<h4 id="displacement.response%28%29">displacement.response()</h4>
<p>Returns the quality of the result of displacement matching between two images. Range 0-1. Displacement objects with a response less than 0.1 may be noise.</p>
<p>You can also get this value via index [4].</p>
<h2 id="Kptmatch-class-%E2%80%93-feature-point-object">Kptmatch class – feature point object</h2>
<p>The feature point object is returned by <code>image.match_descriptor</code>.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.kptmatch</p>
<p>Please call the image.match_descriptor() function to create this object.</p>
<h3 id="Method">Method</h3>
<h4 id="kptmatch.rect%28%29">kptmatch.rect()</h4>
<p>Return a rectangular tuple (x, y, w, h), used in other image methods such as image.draw_rectangle of the bounding box of the feature point.</p>
<h4 id="kptmatch.cx%28%29">kptmatch.cx()</h4>
<p>Returns the center x position of the feature point (int).</p>
<p>You can also get this value by index [0].</p>
<h4 id="kptmatch.cy%28%29">kptmatch.cy()</h4>
<p>Returns the center y position (int) of the feature point.</p>
<p>You can also get this value via index [1].</p>
<h4 id="kptmatch.x%28%29">kptmatch.x()</h4>
<p>Returns the x coordinate (int) of the bounding box of the feature point.</p>
<p>You can also get this value via index [2].</p>
<h4 id="kptmatch.y%28%29">kptmatch.y()</h4>
<p>Returns the y coordinate (int) of the bounding box of the feature point.</p>
<p>You can also get this value through index [3].</p>
<h4 id="kptmatch.w%28%29">kptmatch.w()</h4>
<p>Returns the w width (int) of the bounding box of the feature point.</p>
<p>You can also get this value via index [4].</p>
<h4 id="kptmatch.h%28%29">kptmatch.h()</h4>
<p>Returns the h height (int) of the bounding box of the feature point.</p>
<p>You can also get this value via index [5].</p>
<h4 id="kptmatch.count%28%29">kptmatch.count()</h4>
<p>Returns the number of matched feature points (int).</p>
<p>You can also get this value via index [6].</p>
<h4 id="kptmatch.theta%28%29">kptmatch.theta()</h4>
<p>Returns the curl of the estimated feature point (int).</p>
<p>You can also get this value via index [7].</p>
<h4 id="kptmatch.match%28%29">kptmatch.match()</h4>
<p>Returns a list of (x, y) tuples matching key points.</p>
<p>You can also get this value through index [8].</p>
<h2 id="ImageWriter-class-ImageWriter-object">ImageWriter class-ImageWriter object</h2>
<p>The ImageWriter object allows you to quickly write uncompressed images to disk.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.ImageWriter(path)</p>
<p>By creating an ImageWriter object, you can write uncompressed images to disk in the simple file format used for OpenMV Cams. Then the uncompressed image can be re-read using ImageReader.</p>
<h3 id="Method">Method</h3>
<h4 id="imagewriter.size%28%29">imagewriter.size()</h4>
<p>Returns the size of the file being written.</p>
<h4 id="imagewriter.add_frame%28img%29">imagewriter.add_frame(img)</h4>
<p>Write an image to disk. Because the image is not compressed, it executes quickly, but it takes up a lot of disk space.</p>
<h4 id="imagewriter.close%28%29">imagewriter.close()</h4>
<p>Close the image stream file. You must close the file or the file will be damaged.</p>
<h2 id="ImageReader-class-%E2%80%93-ImageReader-object">ImageReader class – ImageReader object</h2>
<p>The ImageReader object allows you to quickly read uncompressed images from disk.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.ImageReader(path)</p>
<p>Create an ImageReader object to play back the image data written by the ImageWriter object. The frames played back by the ImageWriter object will be played back at the same FPS as when they were written to disk.</p>
<h3 id="Method">Method</h3>
<h4 id="imagereader.size%28%29">imagereader.size()</h4>
<p>Returns the size of the file being read.</p>
<p>imagereader.next_frame([copy_to_fb=True, loop=True])<br />
Return the image object from the file written by ImageWriter. If copy_to_fb is True, the image object will be directly loaded into the frame buffer. Otherwise, the image object will be put into the heap. Note: Unless the image is small, the heap may not have enough space to store the image object. If loop is True, playback will restart after the last image of the stream is read. Otherwise, this method will return None after all frames have been read.</p>
<p>Note: imagereader.next_frame tries to limit the playback speed by pausing the playback after reading the frame to match the frame recording speed. Otherwise, this method will quickly read and play all images at a speed of 200+FPS.</p>
<h4 id="imagereader.close%28%29">imagereader.close()</h4>
<p>Close the file being read. You need to do this to prevent damage to the imagereader object. But because it is a read-only file, the file will not be damaged when it is not closed.</p>
<h2 id="Image-class-image-objects">Image class-image objects</h2>
<p>Image objects are the basic objects of machine vision operations.</p>
<h3 id="Constructor">Constructor</h3>
<p>class image.Image(path[, copy_to_fb=False])</p>
<p>Create a new image object from the file in path.</p>
<p>Support image files in bmp/pgm/ppm/jpg/jpeg format.</p>
<p>If copy_to_fb is True, the image will be directly loaded into the frame buffer, and you can load a large image. If False, the image will be loaded into the MicroPython heap, which is much smaller than the frame buffer.</p>
<p>In OpenMV Cam M4, if copy_to_fb is False, you should try to keep the image size below 8KB. If True, the image can be up to 160KB.<br />
In OpenMV Cam M7, if copy_to_fb is False, you should try to keep the image size below 16KB. If True, the maximum image size can be 320KB.<br />
The image supports the &quot;[]&quot; notation. Let image[index] = 8/16-bit value to allocate image pixels or image[index] and get an image pixel. If it is a 16-bit RGB565 grayscale image for RGB image, this pixel is 8 Bit.</p>
<p>For JPEG images, &quot;[]&quot; allows you to access JPEG image color patches in the form of a compressed section array. Since the JPEG image is a compressed byte stream, the reading and writing of the data group is opaque.</p>
<p>The image also supports read buffer operation. You can treat the image as a section array object and input the image into all types of MicroPython functions. If you want to transmit an image, you can pass it to the UART/SPI/I2C write function, which can realize automatic transmission.</p>
<h3 id="Method">Method</h3>
<h4 id="image.width%28%29">image.width()</h4>
<p>Returns the width of the image in pixels.</p>
<h4 id="image.height%28%29">image.height()</h4>
<p>Returns the height of the image in pixels.</p>
<h4 id="image.format%28%29">image.format()</h4>
<p>Return sensor.GRAYSCALE for grayscale images, sensor.RGB565 for RGB images, and sensor.JPEG for JPEG images.</p>
<h4 id="image.size%28%29">image.size()</h4>
<p>Returns the size of the image in bytes.</p>
<h4 id="image.get_pixel%28x%2C-y%5B%2C-rgbtuple%5D%29">image.get_pixel(x, y[, rgbtuple])</h4>
<p>Grayscale image: returns the grayscale pixel value at (x, y) position.</p>
<p>RGB565l: Returns the RGB888 pixel tuple (r, g, b) at position (x, y).</p>
<p>Bayer image: Returns the pixel value at position (x, y).</p>
<p>Does not support compressed images.</p>
<blockquote>
<p>image.get_pixel() and <code>image.set_pixel()</code> are the only methods that allow you to manipulate Bayer mode images. The Bayer mode image is a text image. For even rows, the pixels in the image are R/G/R/G/ etc. For odd lines, the pixels in the image are G/B/G/B/ etc. Each pixel is 8 bits.</p>
</blockquote>
<h4 id="image.set_pixel%28x%2C-y%2C-pixel%29">image.set_pixel(x, y, pixel)</h4>
<p>Grayscale image: Set the pixel at position (x, y) to the grayscale value pixel.</p>
<p>RGB image: Set the pixel at position (x, y) to RGB888 tuple (r, g, b) pixel.</p>
<p>Does not support compressed images.</p>
<blockquote>
<p>image.get_pixel() and <code>image.set_pixel()</code> are the only methods that allow you to manipulate Bayer mode images. The Bayer mode image is a text image. For even rows, the pixels in the image are R/G/R/G/ etc. For odd lines, the pixels in the image are G/B/G/B/ etc. Each pixel is 8 bits.</p>
</blockquote>
<h4 id="image.mean_pool%28x_div%2C-y_div%29">image.mean_pool(x_div, y_div)</h4>
<p>Find the average value of x_div * y_div squares in the image and return the modified image composed of the average value of each square.</p>
<p>This method allows you to quickly shrink the image on the original image.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.mean_pooled%28x_div%2C-y_div%29">image.mean_pooled(x_div, y_div)</h4>
<p>Find the average value of x_div * y_div squares in the image and return a new image composed of the average value of each square.</p>
<p>This method allows you to create a reduced image copy.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.midpoint_pool%28x_div%2C-y_div%5B%2C-bias%3D0.5%5D%29">image.midpoint_pool(x_div, y_div[, bias=0.5])</h4>
<p>Find the midpoint value of the x_div * y_div square in the image, and return the modified image composed of the midpoint value of each square.</p>
<p>A bias of 0.0 returns the minimum value of each region, and a ``bias'' of 1.0 returns the maximum value of each region.</p>
<p>This method allows you to quickly shrink the image on the original image.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.midpoint_pooled%28x_div%2C-y_div%5B%2C-bias%3D0.5%5D%29">image.midpoint_pooled(x_div, y_div[, bias=0.5])</h4>
<p>Find the midpoint value of the x_div * y_div squares in the image, and return a new image composed of the midpoint value of each square.</p>
<p>A bias of 0.0 returns the minimum value of each region, and a ``bias'' of 1.0 returns the maximum value of each region.</p>
<p>This method allows you to create a reduced image copy.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.to_grayscale%28%5Bcopy%3DFalse%5D%29">image.to_grayscale([copy=False])</h4>
<p>Convert the image to a grayscale image. This method will also modify the basic image pixels and change the image size in bytes, so it can only be performed on grayscale images or RGB565 images. Otherwise, copy must be True to create a new modified image on the heap.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.to_rgb565%28%5Bcopy%3DFalse%5D%29">image.to_rgb565([copy=False])</h4>
<p>Convert the image to a color image. This method will also modify the base image pixels and change the image size in bytes, so it can only be performed on RGB565 images. Otherwise, copy must be True to create a new modified image on the heap.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.to_rainbow%28%5Bcopy%3DFalse%5D%29">image.to_rainbow([copy=False])</h4>
<p>Convert the image to a rainbow image. This method will also modify the base image pixels and change the image size in bytes, so it can only be performed on RGB565 images. Otherwise, copy must be True to create a new modified image on the heap.</p>
<p>The rainbow image is a color image, and has a unique color value for each 8-bit mask gray-scale illumination value in the image. For example, it provides heat map colors for thermal images.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.compress%28%5Bquality%3D50%5D%29">image.compress([quality=50])</h4>
<p>JPEG compresses the image appropriately. Compared with compressed save heap space, using this method uses a higher quality compression ratio at the cost of destroying the original image.</p>
<p>quality is the compression quality (0-100) (int).</p>
<h4 id="image.compress_for_ide%28%5Bquality%3D50%5D%29">image.compress_for_ide([quality=50])</h4>
<p>JPEG compresses the image appropriately. Compared with compressed save heap space, using this method uses a higher quality compression ratio at the cost of destroying the original image.</p>
<p>This method compresses the image, and then formats the JPEG data by encoding every 6 bits into bytes between 128-191, and converts it to OpenMV IDE for display. This step is done to prevent the JPEG data from being mistaken for other text data in the byte stream.</p>
<p>You need to use this method to format the image data for display in the terminal window created by &quot;Open Terminal&quot; in OpenMV IDE.</p>
<p>quality is the compression quality (0-100) (int).</p>
<h4 id="image.compressed%28%5Bquality%3D50%5D%29">image.compressed([quality=50])</h4>
<p>Return a JPEG compressed image—the original image is unprocessed. However, this method requires a large allocation of heap space, so image compression quality and image resolution must be very low.</p>
<p>quality is the compression quality (0-100) (int).</p>
<h4 id="image.compressed_for_ide%28%5Bquality%3D50%5D%29">image.compressed_for_ide([quality=50])</h4>
<p>Return a JPEG compressed image—the original image is unprocessed. However, this method requires a large allocation of heap space, so image compression quality and image resolution must be very low.</p>
<p>This method compresses the image, and then formats the JPEG data by encoding every 6 bits into bytes between 128-191, and converts it to OpenMV IDE for display. This step is done to prevent the JPEG data from being mistaken for other text data in the byte stream.</p>
<p>You need to use this method to format the image data for display in the terminal window created by &quot;Open Terminal&quot; in OpenMV IDE.</p>
<p>quality is the compression quality (0-100) (int).</p>
<h4 id="image.copy%28%5Broi%5B%2C-copy_to_fb%3DFalse%5D%5D%29">image.copy([roi[, copy_to_fb=False]])</h4>
<p>Create a copy of the image object.</p>
<p>Roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, the ROI will copy the entire image rectangle. But this does not apply to JPEG images.</p>
<p>Remember that the image copy is stored in the MicroPython heap, not the frame buffer. Similarly, you need to control the image copy size below 8KB (OpenMV) or below 16KB (OpenMV Cam M7). If you want to use one copy operation to use all the heap space, this function will be abnormal. An image that is too large can easily trigger abnormalities.</p>
<p>If copy_to_fb is True, this method replaces the frame buffer with an image. The frame buffer has much larger space than the heap and can hold large images.</p>
<h4 id="image.save%28path%5B%2C-roi%5B%2C-quality%3D50%5D%5D%29">image.save(path[, roi[, quality=50]])</h4>
<p>Save a copy of the image to the file system in path.</p>
<p>Support image files in bmp/pgm/ppm/jpg/jpeg format. Note: You cannot save compressed images in jpeg format into uncompressed format.</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, the ROI will copy the entire image rectangle. But this does not apply to JPEG images.</p>
<p>quality refers to the JPEG compression quality that saves the image as JPEG format when the image has not been compressed.</p>
<h4 id="image.clear%28%29">image.clear()</h4>
<p>Set all pixels in the image to zero (very fast).</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images.</p>
<h4 id="image.draw_line%28x0%2C-y0%2C-x1%2C-y1%5B%2C-color%5B%2C-thickness%3D1%5D%5D%29">image.draw_line(x0, y0, x1, y1[, color[, thickness=1]])</h4>
<p>Draw a line from (x0, y0) to (x1, y1) on the image. You can pass x0, y0, x1, y1 individually, or to tuples (x0, y0, x1, y1).</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>thickness Controls the thickness of the line in pixels.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_rectangle%28x%2C-y%2C-w%2C-h%5B%2C-color%5B%2C-thickness%3D1%5B%2C-fill%3DFalse%5D%5D%5D%29">image.draw_rectangle(x, y, w, h[, color[, thickness=1[, fill=False]]])</h4>
<p>Draw a rectangle on the image. You can pass x, y, w, h individually or as a tuple (x, y, w, h).</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>thickness Controls the thickness of the line in pixels.</p>
<p>Set fill to True to fill the rectangle.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_ellipse%28cx%2C-cy%2C-rx%2C-ry%2C-rotation%5B%2C-color%5B%2C-thickness%3D1%5B%2C-fill%3DFalse%5D%5D%5D%29">image.draw_ellipse(cx, cy, rx, ry, rotation[, color[, thickness=1[, fill=False]]])</h4>
<p>Draws an ellipse on the image. You may either pass cx, cy, rx, ry, and the rotation (in degrees) separately or as a tuple (cx, yc, rx, ry, rotation).</p>
<p>color is an RGB888 tuple for Grayscale or RGB565 images. Defaults to white. However, you may also pass the underlying pixel value (0-255) for grayscale images or a RGB565 value for RGB565 images.</p>
<p>thickness controls how thick the edges are in pixels.</p>
<p>Pass fill set to True to fill the ellipse.</p>
<p>Returns the image object so you can call another method using . notation.</p>
<p>Not supported on compressed images or bayer images.</p>
<h4 id="image.draw_circle%28x%2C-y%2C-radius%5B%2C-color%5B%2C-thickness%3D1%5B%2C-fill%3DFalse%5D%5D%5D%29">image.draw_circle(x, y, radius[, color[, thickness=1[, fill=False]]])</h4>
<p>Draw a circle on the image. You can pass x, y, radius individually or as a tuple (x, y, radius).</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>thickness Controls the thickness of the line in pixels.</p>
<p>Set fill to True to fill the circle.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_string%28x%2C-y%2C-text%5B%2C-color%5B%2C-scale%3D1%5B%2C-x_spacing%3D0%5B%2C-y_spacing%3D0%5B%2C-mono_space%3DTrue%5D%5D%5D%5D%5D%29">image.draw_string(x, y, text[, color[, scale=1[, x_spacing=0[, y_spacing=0[, mono_space=True]]]]])</h4>
<p>Draw 8x10 text from the position (x, y) in the image. You can pass x, y individually or as a tuple (x, y).</p>
<p>text is the character string written into the image. The \n, \r, and \r\n end characters move the cursor to the next line.</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>You can increase the scale to increase the size of the text on the image.</p>
<p>Only integer values ​​(for example, 1/2/3/etc).</p>
<p>x_spacing allows you to add (if positive) or subtract (if negative) x pixels between characters to set the character spacing.</p>
<p>y_spacing allows you to add (if it is a positive number) or subtract (if it is a negative number) y pixels between characters to set line spacing.</p>
<p>mono_space defaults to True, which forces the text spacing to be fixed. For large text, this looks terrible. Set False to get non-fixed width character spacing, which looks much better.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_cross%28x%2C-y%5B%2C-color%5B%2C-size%3D5%5B%2C-thickness%3D1%5D%5D%5D%29">image.draw_cross(x, y[, color[, size=5[, thickness=1]]])</h4>
<p>Draw a cross on the image. You can pass x, y individually or as a tuple (x, y).</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>size controls the extension length of the crosshairs.</p>
<p>thickness controls the pixel thickness of the edge.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_arrow%28x0%2C-y0%2C-x1%2C-y1%5B%2C-color%5B%2C-thickness%3D1%5D%5D%29">image.draw_arrow(x0, y0, x1, y1[, color[, thickness=1]])</h4>
<p>Draw an arrow from (x0, y0) to (x1, y1) on the image. You can pass x0, y0, x1, y1 individually, or to tuples (x0, y0, x1, y1).</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>thickness Controls the thickness of the line in pixels.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_image%28image%2C-x%2C-y%5B%2C-x_scale%3D1.0%5B%2C-y_scale%3D1.0%5B%2C-mask%3DNone%5B%2C-alpha%3D256%5D%5D%5D%5D%29">image.draw_image(image, x, y[, x_scale=1.0[, y_scale=1.0[, mask=None[, alpha=256]]]])</h4>
<p>Draw an image whose upper left corner starts at position x, y. You can pass x, y individually or to a tuple (x, y).</p>
<p>x_scale controls the scale of the image in the x direction (floating point number).</p>
<p>y_scale controls the scale of the image in the y direction (floating point number).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. You can use the mask mask for drawing operations.</p>
<p>Alpha controls the transparency of the source image drawn into the target image. 256 is to draw an opaque source image, and a value less than 256 produces a blend between the source image and the target image. 0 means not to modify the target image.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.draw_keypoints%28keypoints%5B%2C-color%5B%2C-size%3D10%5B%2C-thickness%3D1%5B%2C-fill%3DFalse%5D%5D%5D%5D%29">image.draw_keypoints(keypoints[, color[, size=10[, thickness=1[, fill=False]]]])</h4>
<p>Draw each point of a feature point object on the image.</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>size controls the size of feature points.</p>
<p>thickness Controls the thickness of the line in pixels.</p>
<p>Set fill to True to fill the feature points.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.flood_fill%28x%2C-y%5B%2C-seed_threshold%3D0.05%5B%2C-floating_threshold%3D0.05%5B%2C-color%5B%2C-invert%3DFalse%5B%2C-clear_background%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%5D%5D%5D%5D%29">image.flood_fill(x, y[, seed_threshold=0.05[, floating_threshold=0.05[, color[, invert=False[, clear_background=False[, mask=None]]]]]]])</h4>
<p>Fill the area of ​​the image from position x, y. You can pass x, y individually or to a tuple (x, y).</p>
<p>seed_threshold controls the difference between the pixels in the filled area and the original starting pixels.</p>
<p>floating_threshold controls the difference between the pixels in the filled area and any adjacent pixels.</p>
<p>color is an RGB888 tuple for grayscale or RGB565 images. The default is white. However, you can also pass the basic pixel value (0-255) of the grayscale image or the byte inverted RGB565 value of the RGB565 image.</p>
<p>Pass invert as True to refill all content outside the flood_fill connection area.</p>
<p>Pass clear_background as True, and reset the remaining flood_fill pixels that have not been recolored.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask will be evaluated during flood_fill.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.binary%28thresholds%5B%2C-invert%3DFalse%5B%2C-zero%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%29">image.binary(thresholds[, invert=False[, zero=False[, mask=None]]])</h4>
<p>Set all pixels in the image to black or white according to whether the pixel is within the threshold in the threshold list thresholds.</p>
<p>thresholds must be a list of tuples. [(lo, hi), (lo, hi), ..., (lo, hi)] Define the color range you want to track. For grayscale images, each tuple needs to contain two values-the minimum gray value and the maximum gray value. Only the areas of pixels that fall between these thresholds are considered. For an RGB565 image, each tuple needs to have six values ​​(l_lo, l_hi, a_lo, a_hi, b_lo, b_hi)-the minimum and maximum values ​​of LAB L, A and B channels, respectively. For ease of use, this function will automatically repair the minimum and maximum values ​​of exchange. In addition, if the tuple is greater than six values, the remaining values ​​are ignored. Conversely, if the tuple is too short, it is assumed that the remaining thresholds are in the maximum range.</p>
<p>annotation</p>
<p>To obtain the threshold of the tracked object, simply select (click and drag) the tracked object in the IDE frame buffer. The histogram will be updated accordingly to the area. Then just write down the starting and falling positions of the color distribution in each histogram channel. These will be the low and high values ​​of thresholds. Since the difference between the upper and lower quartiles is small, it is better to manually determine the threshold.</p>
<p>You can also determine the color threshold by entering Tools -&gt; Machine Vision -&gt; Threshold Editor in OpenMV IDE and dragging the slider from the GUI window.</p>
<p>invert Inversion threshold operation, pixels are matched outside the known color range instead of in the known color range.</p>
<p>Set zero to True to make the threshold pixels zero and keep the pixels not in the threshold list unchanged.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.invert%28%29">image.invert()</h4>
<p>Change the binary image 0 (black) to 1 (white) and 1 (white) to 0 (black), flipping all the pixel values ​​in the binary image very quickly.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and Bayer images.</p>
<h4 id="image.b_and%28image%5B%2C-mask%3DNone%5D%29">image.b_and(image[, mask=None])</h4>
<p>Use another image to perform a logical AND operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.b_nand%28image%5B%2C-mask%3DNone%5D%29">image.b_nand(image[, mask=None])</h4>
<p>Use another image to perform logical AND operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.b_or%28image%5B%2C-mask%3DNone%5D%29">image.b_or(image[, mask=None])</h4>
<p>Use another image to perform a logical OR operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.b_nor%28image%5B%2C-mask%3DNone%5D%29">image.b_nor(image[, mask=None])</h4>
<p>Use another image to perform logical NOR operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.b_xor%28image%5B%2C-mask%3DNone%5D%29">image.b_xor(image[, mask=None])</h4>
<p>Use another image to perform a logical XOR operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.b_xnor%28image%5B%2C-mask%3DNone%5D%29">image.b_xnor(image[, mask=None])</h4>
<p>Use another image to perform logical XOR operation with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.erode%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.erode(size[, threshold[, mask=None]])</h4>
<p>Delete pixels from the edge of the divided area.</p>
<p>This method is implemented by convolving a kernel of ((size<em>2)+1)x((size</em>2)+1) pixels on the image. If the sum of the adjacent pixel sets is less than the threshold, the central pixel of the kernel Zero.</p>
<p>If the threshold is not set, this method functions as the standard corrosion method. If the threshold is set, you can specify the specific pixels to be eroded, for example: set the threshold value 2 for pixels below 2.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.dilate%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.dilate(size[, threshold[, mask=None]])</h4>
<p>Add pixels to the edges of the segmented area.</p>
<p>This method is implemented by convolving a kernel of ((size<em>2)+1)x((size</em>2)+1) pixels on the image. If the sum of the adjacent pixel sets is greater than the threshold, the central pixel of the kernel is Set up.</p>
<p>If the threshold is not set, this method functions as the standard corrosion method. If the threshold is set, you can specify the specific pixels to be eroded, for example: set the threshold value 2 for pixels below 2.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.open%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.open(size[, threshold[, mask=None]])</h4>
<p>Perform erosion and dilation on the image in sequence. For more information, see image.erode() and image.dilate().</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.close%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.close(size[, threshold[, mask=None]])</h4>
<p>Perform dilation and erosion on the image in sequence. For more information, see image.erode() and image.dilate().</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.top_hat%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.top_hat(size[, threshold[, mask=None]])</h4>
<p>Return the difference between the original image and the image after executing the image.open() function.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.black_hat%28size%5B%2C-threshold%5B%2C-mask%3DNone%5D%5D%29">image.black_hat(size[, threshold[, mask=None]])</h4>
<p>Return the difference between the original image and the image after executing the image.close() function.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.negate%28%29">image.negate()</h4>
<p>Flip (digitally invert) all pixel values ​​in the image very quickly. Perform numerical conversion on the pixel value of each color channel. Example: (255-pixel).</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.replace%28image%5B%2C-hmirror%3DFalse%5B%2C-vflip%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%29">image.replace(image[, hmirror=False[, vflip=False[, mask=None]]])</h4>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Set hmirror to True to mirror the replacement image horizontally.</p>
<p>Set vflip to True to flip the replacement image vertically.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.add%28image%5B%2C-mask%3DNone%5D%29">image.add(image[, mask=None])</h4>
<p>Add two images to each other pixel by pixel.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.sub%28image%5B%2C-reverse%3DFalse%5B%2C-mask%3DNone%5D%5D%29">image.sub(image[, reverse=False[, mask=None]])</h4>
<p>Subtract two images from each other pixel by pixel.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Setting reverse to True can reverse the subtraction operation from this_image-image to image-this_image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.mul%28image%5B%2C-invert%3DFalse%5B%2C-mask%3DNone%5D%5D%29">image.mul(image[, invert=False[, mask=None]])</h4>
<p>Multiply two images by pixel.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Set invert to True to change the multiplication operation from a<em>b to 1/((1/a)</em>(1/b)). In particular, this brightens the image instead of darkening it (for example, multiplication and burning operations).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.div%28image%5B%2C-invert%3DFalse%5B%2C-mask%3DNone%5D%5D%29">image.div(image[, invert=False[, mask=None]])</h4>
<p>Divide this image by another image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Set invert to True to change the division direction from a/b to b/a.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.min%28image%5B%2C-mask%3DNone%5D%29">image.min(image[, mask=None])</h4>
<p>At the pixel level, replace the pixels in this image with the smallest pixel value between this image and another image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV4.</p>
<h4 id="image.max%28image%5B%2C-mask%3DNone%5D%29">image.max(image[, mask=None])</h4>
<p>At the pixel level, replace the pixels in this image with the maximum pixel value between this image and another image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.difference%28image%5B%2C-mask%3DNone%5D%29">image.difference(image[, mask=None])</h4>
<p>Take the absolute value of the two images pixel by pixel. Example: For each color channel, change each pixel �� to ABS (this.pixel-image.pixel).</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.blend%28image%5B%2C-alpha%3D128%5B%2C-mask%3DNone%5D%5D%29">image.blend(image[, alpha=128[, mask=None]])</h4>
<p>Fuse another image image with this image.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Alpha controls how much other images are blended into this image. Alpha should be an integer value between 0 and 256. A value close to zero will blend more other images into this image, a value close to 256 is the opposite.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.histeq%28%5Badaptive%3DFalse%5B%2C-clip_limit%3D-1%5B%2C-mask%3DNone%5D%5D%5D%29">image.histeq([adaptive=False[, clip_limit=-1[, mask=None]]])</h4>
<p>Run the histogram equalization algorithm on the image. Histogram equalization normalizes the contrast and brightness in the image.</p>
<p>If adaptive is passed as True, then an adaptive histogram equalization method will be run on the image, which is usually better than non-adaptive histogram definition, but it takes longer to run.</p>
<p>clip_limit provides a way to limit the contrast of adaptive histogram equalization. Use a small value (for example, 10) to generate a good histogram equalization contrast limited image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.mean%28size%2C-%5Bthreshold%3DFalse%2C-%5Boffset%3D0%2C-%5Binvert%3DFalse%2C-%5Bmask%3DNone%5D%5D%5D%5D%5D%29">image.mean(size, [threshold=False, [offset=0, [invert=False, [mask=None]]]]])</h4>
<p>Standard mean fuzzy filtering using box filters.</p>
<p>Size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>median(size, percentile=0.5, threshold=False, offset=0, invert=False, mask])<br />
Run median filtering on the image. Under the condition of preserving the edges, the median filter is the best filter to smooth the surface, but it runs very slowly.</p>
<p>Size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>percentile controls the percentile of the value used in the kernel. By default, each pixel is replaced with the adjacent 50th percentile (center). You can set this value to 0 when using minimum filtering, 0.25 when using lower quartile filtering, 0.75 when using upper quartile filtering, and 1 when using maximum filtering.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.mode%28size%5B%2C-threshold%3DFalse%2C-offset%3D0%2C-invert%3DFalse%2C-mask%5D%29">image.mode(size[, threshold=False, offset=0, invert=False, mask])</h4>
<p>Run a mode filter on the image, replacing each pixel with the pattern of neighboring pixels. This method works well on grayscale images. However, due to the non-linear nature of this operation, many artifacts will be generated on the edges of the RGB image.</p>
<p>Size is the size of the kernel. Take 1 (3x3 core) and 2 (5x5 core).</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.midpoint%28size%5B%2C-bias%3D0.5%2C-threshold%3DFalse%2C-offset%3D0%2C-invert%3DFalse%2C-mask%5D%29">image.midpoint(size[, bias=0.5, threshold=False, offset=0, invert=False, mask])</h4>
<p>Run midpoint filtering on the image. This filter finds the midpoint ((max-min)/2) of the neighborhood of each pixel in the image.</p>
<p>size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>Bias controls the minimum/maximum degree of image blending. 0 only applies to minimum filtering, and 1 only applies to maximum filtering. You can use bias to perform minimum/maximum filtering on the image.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.morph%28size%2C-kernel%2C-mul%3DAuto%2C-add%3D0%29">image.morph(size, kernel, mul=Auto, add=0)</h4>
<p>Convolve the image through the filter kernel. This allows you to perform general convolution on the image.</p>
<p>size controls the size of the kernel to ((size<em>2)+1)x((size</em>2)+1) pixels.</p>
<p>kernel The kernel used to convolve the image, which can be a tuple or a list of values ​​[-128:127].</p>
<p>mul is the number used to multiply the result of the convolution pixel. If not set, it will default to a value which will prevent scaling in the convolution output.</p>
<p>add is the value used to add to the convolution result of each pixel.</p>
<p>mul can adjust the global contrast, add can adjust the global brightness.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.gaussian%28size%5B%2C-unsharp%3DFalse%5B%2C-mul%5B%2C-add%3D0%5B%2C-threshold%3DFalse%5B%2C-offset%3D0%5B%2C-invert%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%5D%5D%5D%5D%29">image.gaussian(size[, unsharp=False[, mul[, add=0[, threshold=False[, offset=0[, invert=False[, mask=None]]]]]]])</h4>
<p>Convolve the image by smoothing Gaussian kernel.</p>
<p>size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>If unsharp is set to True, this method will not only perform Gaussian filtering operations, but perform unsharp masking operations, thereby improving the image sharpness of the edges.</p>
<p>mul is the number used to multiply the result of the convolution pixel. If not set, it will default to a value which will prevent scaling in the convolution output.</p>
<p>add is the value used to add to the convolution result of each pixel.</p>
<p>mul can adjust the global contrast, add can adjust the global brightness.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.laplacian%28size%5B%2C-sharpen%3DFalse%5B%2C-mul%5B%2C-add%3D0%5B%2C-threshold%3DFalse%5B%2C-offset%3D0%5B%2C-invert%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%5D%5D%5D%5D%29">image.laplacian(size[, sharpen=False[, mul[, add=0[, threshold=False[, offset=0[, invert=False[, mask=None]]]]]]])</h4>
<p>The image is convolved by edge detection Laplacian kernel.</p>
<p>size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>If sharpen is set to True, then this method will change to sharpen the image instead of outputting only the edge detection image that has not been thresholded. Increase the kernel size and then increase the image clarity.</p>
<p>mul is the number used to multiply the result of the convolution pixel. If not set, it will default to a value which will prevent scaling in the convolution output.</p>
<p>add is the value used to add to the convolution result of each pixel.</p>
<p>mul can adjust the global contrast, add can adjust the global brightness.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.bilateral%28size%5B%2C-color_sigma%3D0.1%5B%2C-space_sigma%3D1%5B%2C-threshold%3DFalse%5B%2C-offset%3D0%5B%2C-invert%3DFalse%5B%2C-mask%3DNone%5D%5D%5D%5D%5D%5D%29">image.bilateral(size[, color_sigma=0.1[, space_sigma=1[, threshold=False[, offset=0[, invert=False[, mask=None]]]]]])</h4>
<p>The image is convolved through a bilateral filter. The bilateral filter smoothes the image while maintaining the edges in the image.</p>
<p>size is the size of the kernel. Take 1 (3x3 core), 2 (5x5 core) or higher.</p>
<p>color_sigma controls how close the color is matched with the bilateral filter. Increase this value to increase color blur.</p>
<p>space_sigma controls the degree of mutual blurring of pixels in space. Increase this value to increase pixel blur.</p>
<p>If you want to set the threshold adaptively on the output of the filter, you can pass the threshold=True parameter to start the adaptive threshold processing of the image, which will be based on the brightness of the environment pixel (related to the brightness of the pixels around the kernel function). Set to 1 or 0. A negative offset value sets more pixels to 1, while a positive value only sets the strongest contrast to 1. Set invert to invert the result output of the binary image.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.cartoon%28size%5B%2C-seed_threshold%3D0.05%5B%2C-floating_threshold%3D0.05%5B%2C-mask%3DNone%5D%5D%5D%29">image.cartoon(size[, seed_threshold=0.05[, floating_threshold=0.05[, mask=None]]])</h4>
<p>Walk through the image and use the flood-fills algorithm to fill all pixel areas in the image. This effectively removes texture from the image by flattening the colors in all areas of the image. For best results, the image should have a lot of contrast so that the areas do not penetrate each other too easily.</p>
<p>seed_threshold controls the difference between the pixels in the filled area and the original starting pixels.</p>
<p>floating_threshold controls the difference between the pixels in the filled area and any adjacent pixels.</p>
<p>mask is another image used as a pixel-level mask for drawing operations. The mask should be an image with only black or white pixels and should be the same size as the image you are drawing. Only the pixels set in the mask are modified.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.remove_shadows%28%5Bimage%5D%29">image.remove_shadows([image])</h4>
<p>Remove the shadow from the image.</p>
<p>If there is no &quot;shadow-free&quot; version of the current image, this method will try to remove the shadow from the image, but there is no real shadow-free image basis. This algorithm is suitable for removing shadows in a flat and uniform background. Please note that this method takes many seconds to run, and is only suitable for removing shadows in real time and dynamically generating a shadowless version of the image. Future versions of the algorithm will be applicable to more environments, but are equally slow.</p>
<p>If a &quot;no shadow&quot; version of the current image appears, this method will use the &quot;true source&quot; background non-shadow image to remove all shadows in the image to filter out the shadows. Non-shadow pixels are not filtered out, so you can add new objects to the scene that did not exist before, and any non-shadow pixels in those objects will be displayed.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Only supports RGB565 images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.chrominvar%28%29">image.chrominvar()</h4>
<p>Remove the lighting effect from the image, leaving only the color gradient. Faster than image.illuminvar() but affected by shadows.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Only supports RGB565 images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.illuminvar%28%29">image.illuminvar()</h4>
<p>Remove the lighting effect from the image, leaving only the color gradient. Slower than image.chrominvar() but not affected by shadows.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Only supports RGB565 images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.linpolar%28%5Breverse%3DFalse%5D%29">image.linpolar([reverse=False])</h4>
<p>The image is reprojected from Cartesian coordinates to linear polar coordinates.</p>
<p>Set reverse = True to reproject in the opposite direction.</p>
<p>Linear polar reprojection converts image rotation to x translation.</p>
<p>Does not support compressed images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.logpolar%28%5Breverse%3DFalse%5D%29">image.logpolar([reverse=False])</h4>
<p>The image is reprojected from Cartesian coordinates to log-polar coordinates.</p>
<p>Set reverse = True to reproject in the opposite direction.</p>
<p>Log polar reprojection transforms the rotation of the image into x translation and scaling to y translation.</p>
<p>Does not support compressed images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.lens_corr%28%5Bstrength%3D1.8%5B%2C-zoom%3D1.0%5D%5D%29">image.lens_corr([strength=1.8[, zoom=1.0]])</h4>
<p>Perform lens distortion correction to remove the fisheye effect of the image caused by the lens.</p>
<p>strength is a floating point number, which determines the degree of de-fishing effect on the image. By default, first try the value 1.8, and then adjust this value to make the image display the best effect.</p>
<p>zoom is the value used to zoom the image. The default value is 1.0.</p>
<p>Return the image object so that you can call another method using. Notation.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="img.rotation_corr%28%5Bx_rotation%3D0.0%5B%2C-y_rotation%3D0.0%5B%2C-z_rotation%3D0.0%5B%2C-x_translation%3D0.0%5B%2C-y_translation%3D0.0%5B%2C-zoom%3D1.0%5B%2C-fov%3D60.0%5B%2C-corners%5D%5D%5D%5D%5D-%5D%5D%5D%29">img.rotation_corr([x_rotation=0.0[, y_rotation=0.0[, z_rotation=0.0[, x_translation=0.0[, y_translation=0.0[, zoom=1.0[, fov=60.0[, corners]]]]] ]]])</h4>
<p>Correct the perspective problem in the image by 3D rotation of the frame buffer.</p>
<p><code>x_rotation</code> is the number of degrees that the image is rotated around the x axis in the frame buffer (that is, the image is rotated up and down).</p>
<p><code>y_rotation</code> refers to the number of degrees the image is rotated around the y axis in the frame buffer (ie, rotate the image left and right).</p>
<p><code>z_rotation</code> is the number of degrees the image is rotated around the z axis in the frame buffer (ie, the image is rotated to the appropriate position).</p>
<p><code>x_translation</code> is the number of units to move the image to the left or right after rotation. Because this conversion is applied to 3D space, the unit is not a pixel...</p>
<p><code>y_translation</code> is the number of units by which the image moves up or down after being rotated. Because this conversion is applied to 3D space, the unit is not a pixel...</p>
<p><code>zoom</code> is the multiple to zoom the image, 1.0 by default.</p>
<p><code>fov</code> is the field of view used internally before rotating the image in 3D space when performing 2D-&gt;3D projection. When this value is close to 0, the image is placed infinitely far from the viewport. When this value is close to 180, the image is placed in the viewport. Normally, you should not change this value, but you can modify it to change the 2D-&gt;3D mapping effect.</p>
<p><code>corners</code> is a list of four (x, y) tuples, representing four <code>corners</code> used to create four-point correspondence homography, mapping the first <code>corner</code> to (0,0), and the second A <code>corner</code> (image_width-1, 0), a third <code>corner</code> (image_width-1 image_height-1) and a fourth <code>corner</code> (0, image_height-1). Then apply 3D rotation after the image is remapped. This parameter allows you to use rotation_corr to do things, such as bird's-eye view conversion. E.g:</p>

<pre class="language-python"><code class="language-python">top_tilt = 10 # if the difference between top/bottom_tilt become to large this method will stop working
bottom_tilt = 0

points = [(tilt, 0), (img.width()-tilt, 0), (img.width()-1-bottom_tilt, img.height()-1), (bottom_tilt, img.height()- 1)]

img.rotation_corr(corners=points)
</code></pre>
<p>Return the image object so that you can use <code>.</code> to call another method.</p>
<p>Does not support compressed images or Bayer images.</p>
<h4 id="image.get_similarity%28image%29">image.get_similarity(image)</h4>
<p>Return a &quot;similarity&quot; object, describing two images using the SSIM algorithm to compare the similarity of the 8x8 pixel color patches between the two images.</p>
<p>image can be an image object, the path of an uncompressed image file (bmp/pgm/ppm), or a scalar value. If a scalar value, the value can be an RGB888 tuple or a basic pixel value (for example, 8-bit grayscale of a grayscale image or byte-reversed RGB565 value of an RGB image).</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.get_histogram%28%5Bthresholds%5B%2C-invert%3DFalse%5B%2C-roi%5B%2C-bins%5B%2C-l_bins%5B%2C-a_bins%5B%2C-b_bins%5D%5D%5D%5D%5D%5D%5D%29">image.get_histogram([thresholds[, invert=False[, roi[, bins[, l_bins[, a_bins[, b_bins]]]]]]])</h4>
<p>Perform normalized histogram operations on all color channels of roi and return a histogram object. Please refer to the histogram object for more information. You can also use image.get_hist or image.histogram to call this method. If you pass the thresholds list, the histogram information will only be calculated from the pixels in the threshold list.</p>
<p>thresholds must be a list of tuples. [(lo, hi), (lo, hi), ..., (lo, hi)] Define the color range you want to track. For grayscale images, each tuple needs to contain two values-the minimum gray value and the maximum gray value. Only the areas of pixels that fall between these thresholds are considered. For an RGB565 image, each tuple needs to have six values ​​(l_lo, l_hi, a_lo, a_hi, b_lo, b_hi)-the minimum and maximum values ​​of LAB L, A and B channels, respectively. For ease of use, this function will automatically repair the minimum and maximum values ​​of exchange. In addition, if the tuple is greater than six values, the remaining values ​​are ignored. Conversely, if the tuple is too short, it is assumed that the remaining thresholds are in the maximum range.</p>
<p>annotation</p>
<p>To obtain the threshold of the tracked object, simply select (click and drag) the tracked object in the IDE frame buffer. The histogram will be updated accordingly to the area. Then just write down the starting and falling positions of the color distribution in each histogram channel. These will be the low and high values ​​of thresholds. Since the difference between the upper and lower quartiles is small, it is better to manually determine the threshold.</p>
<p>You can also determine the color threshold by entering Tools -&gt; Machine Vision -&gt; Threshold Editor in OpenMV IDE and dragging the slider from the GUI window.</p>
<p>invert Inversion threshold operation, pixels are matched outside the known color range instead of in the known color range.</p>
<p>Unless you need to use color statistics for advanced operations, just use the <code>image.get_statistics()</code> method instead of this method to view the pixel area in the image.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>bins and other bins are the number of bins used for the histogram channel. For grayscale images, use bins, and for RGB565 images, use every other channel. The bin count of each channel must be greater than 2. In addition, setting the bin count to be greater than the number of unique pixel values ​​for each channel is meaningless. By default, the histogram will have the maximum number of bins per channel.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.get_statistics%28%5Bthresholds%5B%2C-invert%3DFalse%5B%2C-roi%5B%2C-bins%5B%2C-l_bins%5B%2C-a_bins%5B%2C-b_bins%5D%5D%5D%5D%5D%5D%5D%29">image.get_statistics([thresholds[, invert=False[, roi[, bins[, l_bins[, a_bins[, b_bins]]]]]]])</h4>
<p>Calculate the average, median, mode, standard deviation, minimum, maximum, lower quartile and upper quartile of each color channel in roi, and return a data object. See the statistics object for more information. You can also use image.get_stats or image.statistics to call this method. If you pass the thresholds list, the histogram information will only be calculated from the pixels in the threshold list.</p>
<p>thresholds must be a list of tuples. [(lo, hi), (lo, hi), ..., (lo, hi)] Define the color range you want to track. For grayscale images, each tuple needs to contain two values-the minimum gray value and the maximum gray value. Only the areas of pixels that fall between these thresholds are considered. For an RGB565 image, each tuple needs to have six values ​​(l_lo, l_hi, a_lo, a_hi, b_lo, b_hi)-the minimum and maximum values ​​of LAB L, A and B channels, respectively. For ease of use, this function will automatically repair the minimum and maximum values ​​of exchange. In addition, if the tuple is greater than six values, the remaining values ​​are ignored. Conversely, if the tuple is too short, it is assumed that the remaining thresholds are in the maximum range.</p>
<p>annotation</p>
<p>To obtain the threshold of the tracked object, simply select (click and drag) the tracked object in the IDE frame buffer. The histogram will be updated accordingly to the area. Then just write down the starting and falling positions of the color distribution in each histogram channel. These will be the low and high values ​​of thresholds. Since the difference between the upper and lower quartiles is small, it is better to manually determine the threshold.</p>
<p>You can also determine the color threshold by entering Tools -&gt; Machine Vision -&gt; Threshold Editor in OpenMV IDE and dragging the slider from the GUI window.</p>
<p>invert Inversion threshold operation, pixels are matched outside the known color range instead of in the known color range.</p>
<p>You can use this method when you need to obtain information about a pixel area in an image. For example: If you want to use the frame difference method to detect motion, you need to use this method to determine the change of the image color channel, thereby triggering the motion detection threshold.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>bins and other bins are the number of bins used for the histogram channel. For grayscale images, use bins, and for RGB565 images, use every other channel. The bin count of each channel must be greater than 2. In addition, setting the bin count to be greater than the number of unique pixel values ​​for each channel is meaningless. By default, the histogram will have the maximum number of bins per channel.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.get_regression%28thresholds%5B%2C-invert%3DFalse%5B%2C-roi%5B%2C-x_stride%3D2%5B%2C-y_stride%3D1%5B%2C-area_threshold%3D10%5B%2C-pixels_threshold%3D10%5B%2C-robust%3DFalse%5D%5D%5D%5D%5D%5D%5D%5D%29">image.get_regression(thresholds[, invert=False[, roi[, x_stride=2[, y_stride=1[, area_threshold=10[, pixels_threshold=10[, robust=False]]]]]]]])</h4>
<p>Perform linear regression calculation on all threshold pixels of the image. This calculation is performed by the least square method, which is usually faster, but cannot handle any outliers. If robust is True, the Theil index will be used. The Theil index calculates the median of all slopes between all threshold pixels in the image. If you set too many pixels after threshold conversion, this N^2 operation may drop your FPS below 5 even on an 80x60 image. However, as long as the number of pixels to be set after the threshold conversion is small, linear regression is still effective even when more than 30% of the threshold pixels are abnormal values.</p>
<p>This method returns an image.line object. How to easily use linear objects, please refer to the following blog post: <a href="https://openmv.io/blogs/news/linear-regression-line-following"  target="_blank">https://openmv.io/blogs/news/linear-regression-line-following</a></p>
<p>thresholds must be a list of tuples. [(lo, hi), (lo, hi), ..., (lo, hi)] Define the color range you want to track. For grayscale images, each tuple needs to contain two values-the minimum gray value and the maximum gray value. Only the areas of pixels that fall between these thresholds are considered. For an RGB565 image, each tuple needs to have six values ​​(l_lo, l_hi, a_lo, a_hi, b_lo, b_hi)-the minimum and maximum values ​​of LAB L, A and B channels, respectively. For ease of use, this function will automatically repair the minimum and maximum values ​​of exchange. In addition, if the tuple is greater than six values, the remaining values ​​are ignored. Conversely, if the tuple is too short, it is assumed that the remaining thresholds are in the maximum range.</p>
<blockquote>
<p>To obtain the threshold of the tracked object, just select (click and drag) the tracked object in the IDE frame buffer. The histogram will be updated accordingly to the area. Then just write down the starting and falling positions of the color distribution in each histogram channel. These will be the low and high values ​​of thresholds. Since the difference between the upper and lower quartiles is small, it is better to manually determine the threshold.</p>
</blockquote>
<p>You can also determine the color threshold by entering Tools -&gt; Machine Vision -&gt; Threshold Editor in OpenMV IDE and dragging the slider from the GUI window.</p>
<p>invert Inversion threshold operation, pixels are matched outside the known color range instead of in the known color range.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>x_stride is the number of x pixels to skip when calling the function.</p>
<p>y_stride is the number of y pixels to skip when calling the function.</p>
<p>If the bounding box area after regression is less than area_threshold, None is returned.</p>
<p>If the number of pixels after regression is less than pixel_threshold, None is returned.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.find_blobs%28thresholds%5B%2C-invert%3DFalse%5B%2C-roi%5B%2C-x_stride%3D2%5B%2C-y_stride%3D1%5B%2C-area_threshold%3D10%5B%2C-pixels_threshold%3D10%5B%2C-merge%3DFalse%5B%2C-margin%3D0%5B%2C-threshold_cb-%3DNone%5B%2C-merge_cb%3DNone%5D%5D%5D%5D%5D%5D%5D%5D%5D%5D%5D%29">image.find_blobs(thresholds[, invert=False[, roi[, x_stride=2[, y_stride=1[, area_threshold=10[, pixels_threshold=10[, merge=False[, margin=0[, threshold_cb =None[, merge_cb=None]]]]]]]]]]])</h4>
<p>Find all the color blocks in the image and return a list of color block objects including each color block. Please observe the image.blob object for more information.</p>
<p>thresholds must be a list of tuples. [(lo, hi), (lo, hi), ..., (lo, hi)] Define the color range you want to track. For grayscale images, each tuple needs to contain two values-the minimum gray value and the maximum gray value. Only the areas of pixels that fall between these thresholds are considered. For an RGB565 image, each tuple needs to have six values ​​(l_lo, l_hi, a_lo, a_hi, b_lo, b_hi)-the minimum and maximum values ​​of LAB L, A and B channels, respectively. For ease of use, this function will automatically repair the minimum and maximum values ​​of exchange. In addition, if the tuple is greater than six values, the remaining values ​​are ignored. Conversely, if the tuple is too short, it is assumed that the remaining thresholds are in the maximum range.</p>
<p>annotation</p>
<p>To obtain the threshold of the tracked object, simply select (click and drag) the tracked object in the IDE frame buffer. The histogram will be updated accordingly to the area. Then just write down the starting and falling positions of the color distribution in each histogram channel. These will be the low and high values ​​of thresholds. Since the difference between the upper and lower quartiles is small, it is better to manually determine the threshold.</p>
<p>You can also determine the color threshold by entering Tools -&gt; Machine Vision -&gt; Threshold Editor in OpenMV IDE and dragging the slider from the GUI window.</p>
<p>invert Inversion threshold operation, pixels are matched outside the known color range instead of in the known color range.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>x_stride is the number of x pixels that need to be skipped when searching for a color block. After finding the color block, the straight line filling algorithm will accurately pixel. If the color block is known to be large, you can increase x_stride to increase the speed of finding the color block.</p>
<p>y_stride is the number of y pixels that need to be skipped when searching for a color block. After finding the color block, the straight line filling algorithm will accurately pixel. If the color block is known to be large, y_stride can be increased to increase the speed of searching for the color block.</p>
<p>If the bounding box area of ​​a color block is smaller than area_threshold, it will be filtered out.</p>
<p>If the number of pixels of a color block is less than pixel_threshold, it will be filtered out.</p>
<p>If merge is True, all color blocks that have not been filtered out are merged. The bounding rectangles of these color blocks overlap each other. Margin can be used to increase or decrease the size of the color block boundary rectangle in the intersection test. For example: the color blocks whose edges are 1 and the mutual boundary rectangle is 1 will be merged.</p>
<p>Combining color blocks enables color code tracking. Each color block object has a code value code, which is a bit vector. For example: if you enter two color thresholds in image.find_blobs, the first threshold code is 1, the second code is 2 (the third code is 4, the fourth code is 8, and so on). The merged color block uses a logical OR operation on all codes so that you know the colors that produced them. This allows you to track two colors. If you use two colors to get a color block object, it may be a color code.</p>
<p>If you use a strict color range and cannot fully track all pixels of the target object, you may need to merge color blocks.</p>
<p>Finally, if you want to merge color blocks, but do not want to merge color blocks of two different threshold colors, just call image.find_blobs twice, and the color blocks of different threshold values ​​will not be merged.</p>
<p>threshold_cb can be set to call the function of each color block after threshold filtering, so as to filter it from the list of color blocks to be merged. The callback function will receive one parameter: the color block object to be filtered. Then the callback function needs to return True to keep the color blocks or return False to filter the color blocks.</p>
<p>merge_cb can be set as a function to call two color blocks to be merged to prohibit or permit the merge. The callback function will receive two parameters-two color patch objects to be merged. The callback function must return True to merge color blocks, or return False to prevent color blocks from merging.</p>
<p>Does not support compressed images and bayer images.</p>
<h4 id="image.find_lines%28%5Broi%5B%2C-x_stride%3D2%5B%2C-y_stride%3D1%5B%2C-threshold%3D1000%5B%2C-theta_margin%3D25%5B%2C-rho_margin%3D25%5D%5D%5D%5D%5D%5D%29">image.find_lines([roi[, x_stride=2[, y_stride=1[, threshold=1000[, theta_margin=25[, rho_margin=25]]]]]])</h4>
<p>Use Hough Transform to find all straight lines in the image. Return a list of image.line objects.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>x_stride is the number of x pixels that need to be skipped during Hough transform. If the known straight line is larger, you can increase x_stride.</p>
<p>y_stride is the number of y pixels that need to be skipped during Hough transform. If the known straight line is larger, you can increase y_stride.</p>
<p>threshold controls the straight line detected from the Hough transform. Only return lines greater than or equal to threshold. The correct threshold value for the application depends on the image. Note: The magnitude of a straight line (magnitude) is the sum of the pixel sizes of all Sobel filters that make up the straight line.</p>
<p>theta_margin controls the merging of the monitored lines. The part where the angle of the straight line is theta_margin is merged with the part where the p value of the straight line is rho_margin.</p>
<p>rho_margin controls the merging of the monitored lines. The part where the angle of the straight line is theta_margin and the part where the p value of the straight line is rho_margin are merged.</p>
<p>This method runs the Sobel filter on the image and uses the amplitude and gradient response of the filter to perform the Hough transform. No preprocessing of the image is required. However, cleaning the image filter can get more stable results.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_line_segments%28%5Broi%5B%2C-merge_distance%3D0%5B%2C-max_theta_difference%3D15%5D%5D%5D%29">image.find_line_segments([roi[, merge_distance=0[, max_theta_difference=15]]])</h4>
<p>Use Hough Transform to find line segments in the image. Return a list of image.line objects.</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle. The operating range is limited to pixels in the roi area.</p>
<p>merge_distance specifies the maximum number of pixels between two line segments that can be separated from each other without being merged.</p>
<p>max_theta_difference is the maximum angle difference between the two line segments to be merged by the above merge_distancede.</p>
<p>This method uses the LSD library (also used by OpenCV) to find line segments in the image. This is a bit slow, but very accurate, and the line segments will not jump.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_circles%28%5Broi%5B%2C-x_stride%3D2%5B%2C-y_stride%3D1%5B%2C-threshold%3D2000%5B%2C-x_margin%3D10%5B%2C-y_margin%3D10%5B%2C-r_margin%3D10%5D%5D%5D%5D%5D%5D%5D%29">image.find_circles([roi[, x_stride=2[, y_stride=1[, threshold=2000[, x_margin=10[, y_margin=10[, r_margin=10]]]]]]])</h4>
<p>Use the Hough transform to find the circle in the image. Return a list of image.circle objects (see above).</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle. The operating range is limited to pixels in the roi area.</p>
<p>x_stride is the number of x pixels that need to be skipped during Hough transform. If the circle is known to be larger, you can increase x_stride.</p>
<p>y_stride is the number of y pixels that need to be skipped during Hough transform. If the circle is known to be larger, y_stride can be increased.</p>
<p>threshold controls the circle detected from the Hough transform. Only return circles greater than or equal to threshold. The correct threshold value for the application depends on the image. Note: The size of a circle (magnitude) is the sum of the sizes of all Sobel filter pixels that make up the circle.</p>
<p>x_margin controls the merging of the detected circles. The round pixels are the partial merge of x_margin, y_margin and r_margin.</p>
<p>y_margin controls the merging of the detected circles. The round pixels are the partial merge of x_margin, y_margin and r_margin.</p>
<p>r_margin controls the merging of the detected circles. The round pixels are the partial merge of x_margin, y_margin and r_margin.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_rects%28%5Broi%3DAuto%2C-threshold%3D10000%5D%29">image.find_rects([roi=Auto, threshold=10000])</h4>
<p>Use the same quad detection algorithm used to find AprilTAg to find rectangles in the image. Best for rectangles that contrast sharply with the background. AprilTag's quad detection can handle arbitrary scaling/rotation/cutting of rectangles. Returns a list of image.rect objects.</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle. The operating range is limited to the pixels in the roi area.</p>
<p>Rectangles with a boundary size (by sliding the Sobel operator on all pixels on the edge of the rectangle and adding the value) less than threshold will be filtered from the returned list. The correct value of threshold depends on your application/scenario.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_qrcodes%28%5Broi%5D%29">image.find_qrcodes([roi])</h4>
<p>Find all QR codes in roi and return a list of image.qrcode objects. Please refer to the image.qrcode object for more information.</p>
<p>In order for this method to run successfully, the QR code on the image needs to be relatively flat. By using the sensor.set_windowing function to zoom in the center of the lens, the image.lens_corr function to eliminate the barrel distortion of the lens or by changing the lens with a narrower field of view, you can get a flatter QR code that is not affected by lens distortion. Some machine vision lenses do not cause barrel distortion, but their cost is much higher than the standard lenses provided by OpenMV, which are distortion-free lenses.</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<p>image.find_apriltags([roi[, families=image.TAG36H11[, fx[, fy[, cx[, cy]]]]]])<br />
Find all AprilTags in roi, and return a list of image.apriltag objects. Please refer to the image.apriltag object for more information.</p>
<p>Compared with two-dimensional codes, AprilTags can be detected at longer distances, poorer light, and more distorted image environments. AprilTags can deal with all kinds of image distortion problems, but two-dimensional codes cannot. In other words, AprilTags can only encode a digital ID as its payload.</p>
<p>AprilTags can also be used for localization. Each image.apriltag object returns its three-dimensional position information and rotation angle from the camera. The position information is determined by fx, fy, cx and cy, which are the focal length and center point of the image in the X and Y directions, respectively.</p>
<blockquote>
<p>Use the built-in tag generator tool of OpenMV IDE to create AprilTags. The label generator can create printable 8.5&quot;x11&quot; AprilTags.</p>
</blockquote>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>families is the bit mask of the tag family to be decoded. Is a logical OR:</p>
<p>image.TAG16H5<br />
image.TAG25H7<br />
image.TAG25H9<br />
image.TAG36H10<br />
image.TAG36H11<br />
image.ARTOOLKIT<br />
The default setting is the best image.TAG36H11 tag family. Note: Each time you enable a tag family, the speed of find_apriltags will slow down slightly.</p>
<p>fx is the focal length of the camera in the x direction in pixels. The value of the standard OpenMV Cam is (2.8 / 3.984) * 656, which is obtained by dividing the focal length in millimeters by the length of the photosensitive element in the X direction, and multiplying it by the number of pixels of the photosensitive element in the X direction (for OV7725 photosensitive element In terms of).</p>
<p>fy is the focal length of the camera in the y direction in pixels. The value of the standard OpenMV Cam is (2.8 / 2.952) * 488, which is obtained by dividing the focal length in millimeters by the length of the photosensitive element in the Y direction, and multiplying it by the number of pixels of the photosensitive element in the Y direction (for OV7725 photosensitive element In terms of).</p>
<p>cx is the center of the image, which is image.width()/2 instead of roi.w()/2.</p>
<p>cy is the center of the image, ie image.height()/2, not roi.h()/2.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<p>image.find_datamatrices([roi[, effort=200]])<br />
Find all data matrices in roi and return a list of image.datamatrix objects. Please refer to the image.datamatrix object for more information.</p>
<p>In order for this method to run successfully, the rectangular code on the image needs to be relatively flat. By using the sensor.set_windowing function to zoom in at the center of the lens, the image.lens_corr function to eliminate the barrel distortion of the lens, or by changing the lens with a narrower field of view, you can get a flatter rectangular code that is not affected by lens distortion. Some machine vision lenses do not cause barrel distortion, but their cost is much higher than the standard lens provided by OpenMV, which is a distortion-free lens.</p>
<p>roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>effort controls the time used to find a rectangle code match. The default value of 200 should apply to all use cases. But you may also increase detection at the expense of frame rate, or increase frame rate at the expense of detection. Note: If effort is set below about 160, you will not be able to perform any detection; instead, you can set it to any high value you need, but if the setting is higher than 240, the detection rate will not continue to increase.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_barcodes%28%5Broi%5D%29">image.find_barcodes([roi])</h4>
<p>Find all one-dimensional barcodes in roi and return a list of image.barcode objects. Please refer to the image.barcode object for more information.</p>
<p>For best results, please use a window of 640 length and 40/80/160 width. The lower the verticality, the faster the running speed. Since the barcode is a linear one-dimensional image, it only needs to have a higher resolution in one direction and a lower resolution in the other direction. Note: This function performs horizontal and vertical scanning, so you can use a window with a width of 40/80/160 and a length of 480. Finally, be sure to adjust the lens so that the barcode will be positioned where the focal length produces the clearest image. Fuzzy barcodes cannot be decoded.</p>
<p>This function supports all one-dimensional barcodes:</p>
<p>image.EAN2<br />
image.EAN5<br />
image.EAN8<br />
image.UPCE<br />
image.ISBN10<br />
image.UPCA<br />
image.EAN13<br />
image.ISBN13<br />
image.I25<br />
image.DATABAR (RSS-14)<br />
image.DATABAR_EXP (RSS-Expanded)<br />
image.CODABAR<br />
image.CODE39<br />
image.PDF417<br />
image.CODE93<br />
image.CODE128<br />
roi is a rectangular region of interest (x, y, w, h) to be copied. If not specified, ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Does not support compressed images and bayer images.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<p>image.find_displacement(template[, roi[, template_roi[, logpolar=False]]])<br />
Find the transformation offset of this image from the template. This method can be used to make optical flow. This method returns an image.displacement object that contains the result of the displacement calculation using phase correlation.</p>
<p>roi is the rectangular area (x, y, w, h) to be processed. If not specified, it is equal to the image rectangle.</p>
<p>template_roi is the rectangular area (x, y, w, h) to be processed. If not specified, it is equal to the image rectangle.</p>
<p>roi and template roi must have the same w/h, but x/y can be anywhere in the image. You can slide the smaller rois on the larger image to get the optical flow gradient image.</p>
<p>image.find_displacement usually calculates the x/y translation between two images. However, if you set logpolar = True, it will find changes in rotation and scaling between the two images. The same image.displacement object results in two possible feedbacks.</p>
<p>Does not support compressed images and bayer images.</p>
<p>annotation</p>
<p>Please use this method on images with the same length and width (such as ``sensor.B64X64'').</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.find_number%28roi%29">image.find_number(roi)</h4>
<p>Run LENET-6 CNN (Convolutional Neural Network) trained on the MINST dataset to detect numbers in 28x28 ROI located anywhere on the image. Return a tuple containing integers and floating-point numbers, representing the detected number (0-9) and the detection confidence (0-1).</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only grayscale images are supported.</p>
<p>annotation</p>
<p>This method is experimental. If you run any CNN trained on PC using Caffe in the future, this method may be deleted. The latest firmware version 3.0.0 has removed this function.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h4 id="image.classify_object%28roi%29">image.classify_object(roi)</h4>
<p>Run CIFAR-10 CNN on the ROI of the image to detect airplanes, cars, birds, cats, deer, dogs, frogs, horses, boats and trucks. This method automatically scales the image to 32x32 internally to feed the CNN.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only supports RGB565 images.</p>
<p>annotation</p>
<p>This method is experimental. If you run any CNN trained on PC using Caffe in the future, this method may be deleted.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<p>image.find_template(template, threshold[, roi[, step=2[, search=image.SEARCH_EX]]])<br />
Try to use the normalized cross-correlation (NCC) algorithm to find the first template match in the image. Returns the bounding box tuple (x, y, w, h) of the matching position, otherwise returns None.</p>
<p>template is a small image object that matches this image object. Note: Both images must be grayscale.</p>
<p>Threshold is a floating point number (0.0-1.0), where the smaller value increases the detection rate while increasing the false alarm rate. Conversely, a higher value will reduce the detection rate and at the same time reduce the false alarm rate.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>step is the number of pixels that need to be skipped when searching for a template. Skipping pixels can greatly increase the speed of the algorithm. This method is only applicable to algorithms in SERACH_EX mode.</p>
<p>search can be image.SEARCH_DS or image.SEARCH_EX. The algorithm used by image.SEARCH_DS to search for a template is faster than image.SEARCH_EX, but if the template is located around the edge of the image, the search may not succeed. image.SEARCH_EX can perform a more detailed search on images, but its running speed is much slower than image.SEARCH_DS.</p>
<p>Only grayscale images are supported.</p>
<h4 id="image.find_features%28cascade%5B%2C-threshold%3D0.5%5B%2C-scale%3D1.5%5B%2C-roi%5D%5D%5D%29">image.find_features(cascade[, threshold=0.5[, scale=1.5[, roi]]])</h4>
<p>This method searches images of all regions that match Haar Cascade and returns a list of bounding box rectangle tuples (x, y, w, h) about these features. If no features are found, a blank list is returned.</p>
<p>cascade is a Haar Cascade object. See image.HaarCascade() for details.</p>
<p>Threshold is a floating point number (0.0-1.0), where the smaller value increases the detection rate while increasing the false alarm rate. Conversely, a higher value will reduce the detection rate and at the same time reduce the false alarm rate.</p>
<p>scale is a floating point number that must be greater than 1.0. A higher scale factor runs faster, but its image matching is relatively poor. The ideal value is between 1.35-1.5.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only grayscale images are supported.</p>
<h4 id="image.find_eye%28roi%29">image.find_eye(roi)</h4>
<p>Find the pupil in the area of ​​interest (x, y, w, h) around the eye. Returns a tuple containing the position of the pupil (x, y) in the image. If no pupil is found, return (0,0).</p>
<p>Before using this function, first use image.find_features() and Haar operator frontalface to search for someone's face. Then use image.find_features and Haar operator find_eye to search for eyes on the face. Finally, call this method on each eye ROI returned after calling the image.find_features function to get the coordinates of the pupil.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only grayscale images are supported.</p>
<h4 id="image.find_lbp%28roi%29">image.find_lbp(roi)</h4>
<p>Extract LBP (local binary mode) key points from the ROI tuple (x, y, w, h). You can use the image.match_descriptor function to compare two sets of key points to get the matching distance.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only grayscale images are supported.</p>
<h4 id="image.find_keypoints%28%5Broi%5B%2C-threshold%3D20%5B%2C-normalized%3DFalse%5B%2C-scale_factor%3D1.5%5B%2C-max_keypoints%3D100%5B%2C-corner_detector%3Dimage.CORNER_AGAST%5D%5D%5D%5D%5D%5D%5D%29">image.find_keypoints([roi[, threshold=20[, normalized=False[, scale_factor=1.5[, max_keypoints=100[, corner_detector=image.CORNER_AGAST]]]]]]])</h4>
<p>Extract ORB key points from the ROI tuple (x, y, w, h). You can use the image.match_descriptor function to compare two sets of key points to get the matching area. If no key point is found, None is returned.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>threshold is a number that controls the number of extractions (values ​​0-255). For the default AGAST corner detector, this value should be around 20. For FAST corner detectors, this value is about 60-80. The lower the threshold, the more corner points you extract.</p>
<p>normalized is a boolean value. If True, turn off the key point extraction in multi-resolution. If you don't care about dealing with scaling issues and want the algorithm to run faster, set it to True.</p>
<p>scale_factor is a floating point number that must be greater than 1.0. A higher scale factor runs faster, but its image matching is relatively poor. The ideal value is between 1.35-1.5.</p>
<p>max_keypoints is the maximum number of keypoints that a keypoint object can hold. If the key point object is too large and causes memory problems, please lower the value.</p>
<p>corner_detector is the corner detector algorithm used to extract the key points from the image. Can be image.CORNER_FAST or image.CORNER_AGAST. The FAST corner detector runs faster, but its accuracy is lower.</p>
<p>Only grayscale images are supported.</p>
<h4 id="image.find_edges%28edge_type%5B%2C-threshold%5D%29">image.find_edges(edge_type[, threshold])</h4>
<p>Turn the image into black and white, leaving only the edges as white pixels.</p>
<p>image.EDGE_SIMPLE-Simple threshold high-pass filtering algorithm<br />
image.EDGE_CANNY-Canny edge detection algorithm<br />
threshold is a binary tuple containing a low threshold and a high threshold. You can control the edge quality by adjusting this value.</p>
<p>The default is (100, 200).</p>
<p>Only grayscale images are supported.</p>
<p>find_hog([roi[, size=8]])<br />
Replace the pixels in the ROI with HOG (Histogram of Oriented Gradient) lines.</p>
<p>roi is the rectangular tuple (x, y, w, h) of the region of interest. If not specified, the ROI is the image rectangle of the entire image. The operating range is limited to the pixels in the roi area.</p>
<p>Only grayscale images are supported.</p>
<p>This method is not available on OpenMV Cam M4.</p>
<h2 id="Constant">Constant</h2>
<h3 id="image.SEARCH_EX">image.SEARCH_EX</h3>
<p>Exhaustive template matching search.</p>
<h3 id="image.SEARCH_DS">image.SEARCH_DS</h3>
<p>Faster template matching search.</p>
<h3 id="image.EDGE_CANNY">image.EDGE_CANNY</h3>
<p>Use the Canny edge detection algorithm to perform edge detection on the image.</p>
<h3 id="image.EDGE_SIMPLE">image.EDGE_SIMPLE</h3>
<p>Use threshold high-pass filtering algorithm to detect the edge of the image.</p>
<h3 id="image.CORNER_FAST">image.CORNER_FAST</h3>
<p>High-speed and low-accuracy corner detection algorithm for ORB key points</p>
<h3 id="image.CORNER_AGAST">image.CORNER_AGAST</h3>
<p>Low-speed and high-accuracy algorithm for ORB key points.</p>
<h3 id="image.TAG16H5">image.TAG16H5</h3>
<p>Bit mask enumeration of TAG1H5 tag group. Used for AprilTags.</p>
<h3 id="image.TAG25H7">image.TAG25H7</h3>
<p>Bit mask enumeration of TAG25H7 tag group. Used for AprilTags.</p>
<h3 id="image.TAG25H9">image.TAG25H9</h3>
<p>Bitmask enumeration of TAG25H9 tag group. Used for AprilTags.</p>
<h3 id="image.TAG36H10">image.TAG36H10</h3>
<p>Bit mask enumeration of TAG36H10 tag group. Used for AprilTags.</p>
<h3 id="image.TAG36H11">image.TAG36H11</h3>
<p>Bit mask enumeration of TAG36H11 tag group. Used for AprilTags.</p>
<h3 id="image.ARTOOLKIT">image.ARTOOLKIT</h3>
<p>The bit mask enumeration of the ARTOOLKIT tag group. Used for AprilTags.</p>
<h3 id="image.EAN2">image.EAN2</h3>
<p>EAN2 barcode type enumeration.</p>
<h3 id="image.EAN5">image.EAN5</h3>
<p>EAN5 barcode type enumeration.</p>
<h3 id="image.EAN8">image.EAN8</h3>
<p>EAN8 barcode type enumeration.</p>
<h3 id="image.UPCE">image.UPCE</h3>
<p>UPCE barcode type enumeration.</p>
<h3 id="image.ISBN10">image.ISBN10</h3>
<p>ISBN10 barcode type enumeration.</p>
<h3 id="image.UPCA">image.UPCA</h3>
<p>UPCA barcode type enumeration.</p>
<h3 id="image.EAN13">image.EAN13</h3>
<p>EAN13 barcode type enumeration.</p>
<h3 id="image.ISBN13">image.ISBN13</h3>
<p>ISBN13 barcode type enumeration.</p>
<h3 id="image.I25">image.I25</h3>
<p>I25 barcode type enumeration.</p>
<h3 id="image.DATABAR">image.DATABAR</h3>
<p>DATABAR barcode type enumeration.</p>
<h3 id="image.DATABAR_EXP">image.DATABAR_EXP</h3>
<p>DATABAR_EXP barcode type enumeration.</p>
<h3 id="image.CODABAR">image.CODABAR</h3>
<p>Enumeration of CODABAR barcode types.</p>
<h3 id="image.CODE39">image.CODE39</h3>
<p>CODE39 barcode type enumeration.</p>
<h3 id="image.PDF417">image.PDF417</h3>
<p>PDF417 barcode type enumeration (currently not working).</p>
<h3 id="image.CODE93">image.CODE93</h3>
<p>CODE93 barcode type enumeration.</p>
<h3 id="image.CODE128">image.CODE128</h3>
<p>CODE128 barcode type enumeration.</p>

                        
                    </div>
                </div>
                <div id="previous_next">
                    <div id="previous">
                        
                        <a href="/soft/maixpy/en/api_reference/machine_vision/sensor.html">
                            <span class="icon"></span>
                            <span class="label">sensor</span>
                        </a>
                        
                    </div>
                    <div id="next">
                        
                        <a href="/soft/maixpy/en/api_reference/media/video.html">
                            <span class="label">video</span>
                            <span class="icon"></span>
                        </a>
                        
                    </div>
                </div>
                <div id="comments-container"></div>
            </div>
            <div id="toc_wrapper">
                <div id="toc">
                    <div id="toc_content">
                            
                    </div>
                </div>
            </div>
        </div>
    </div>
    <a id="to_top" href="#"></a>
    <div id="doc_footer">
        <div id="footer">
            <div id="footer_top">
                <ul>
<li><a>links</a><ul><li><a target="_blank" href="https://www.sipeed.com">Sipeed</a></li>
<li><a target="_blank" href="https://maixhub.com/">MaixHub</a></li>
<li><a target="_blank" href="https://sipeed.aliexpress.com/store/911876460">Sipeed AliExpress</a></li>
<li><a target="_blank" href="https://github.com/neutree/teedoc">Build by teedoc </a></li>
</ul>
</li>
<li><a>Source Code</a><ul><li><a target="_blank" href="https://github.com/sipeed/sipeed_wiki">Wiki source</a></li>
<li><a target="_blank" href="https://github.com/sipeed">Open Source Projects</a></li>
</ul>
</li>
<li><a>Follow us</a><ul><li><a target="_blank" href="https://twitter.com/SipeedIO">twitter</a></li>
<li><a target="_blank" href="https://sipeed.aliexpress.com/store/911876460">AliExpress</a></li>
<li><a target="_blank" href="https://github.com/sipeed">github</a></li>
<li><a><a>Wechat</a><img src='/static/image/wechat.png'></a>
</li>
</ul>
</li>
<li><a>Contact us</a><ul><li><a>Tel: +86 0755-27808509</a>
</li>
<li><a>Business support: support@sipeed.com</a>
</li>
<li><a>Address: 深圳市宝安区新湖路4008号蘅芳科技办公大厦A座-2101C</a>
</li>
<li><a  href="/join_us.html">Join us</a></li>
</ul>
</li>
</ul>

            </div>
            <div id="footer_bottom">
                <ul>
<li><a target="_blank" href="https://www.sipeed.com">©2018-2023 深圳矽速科技有限公司</a></li>
<li><a target="_blank" href="https://beian.miit.gov.cn/#/Integrated/index">粤ICP备19015433号</a></li>
</ul>

            </div>
        </div>
    </div>
    
        <script src="/teedoc-plugin-markdown-parser/mermaid.min.js"></script>
    
        <script>mermaid.initialize({startOnLoad:true});</script>
    
        <script type="text/javascript">
                var transLoaded = false;
                var loading = false;
                var domain = "translate.google.com";
                var domainDefault = domain;
                var storeDomain = localStorage.getItem("googleTransDomain");
                if(storeDomain){
                    domain = storeDomain;
                    console.log("load google translate domain from local storage:" + domain);
                }
                function getUrl(domain){
                    if(domain == "/")
                        return "/static/js/google_translate/element.js?cb=googleTranslateElementInit";
                    else
                        return "https://" + domain + "/translate_a/element.js?cb=googleTranslateElementInit";
                }
                var url = getUrl(domain);
                console.log("google translate domain:" + domain + ", url: " + url);
                function googleTranslateElementInit() {
                    new google.translate.TranslateElement({pageLanguage: "auto", layout: google.translate.TranslateElement.InlineLayout.SIMPLE}, 'google_translate_element');
                }
                function loadJS( url, callback ){
                    var script = document.createElement('script');
                    fn = callback || function(){ };
                    script.type = 'text/javascript';
                    if(script.readyState){
                        script.onreadystatechange = function(){
                            if( script.readyState == 'loaded' || script.readyState == 'complete' ){
                                script.onreadystatechange = null;
                                fn();
                            }
                        };
                    }else{
                        script.onload = function(){
                            fn();
                        };
                    }
                    script.src = url;
                    document.getElementsByTagName('head')[0].appendChild(script);
                }
                function removeHint(){
                    var hint = document.getElementById("loadingTranslate");
                    if(hint){
                        hint.remove();
                    }
                }
                var btn = document.getElementById("google_translate_element");
                btn.onclick = function(){
                    if(transLoaded) return;
                    if(loading){
                        var flag = confirm("loading from " + domain + ", please wait, or change domain?");
                        if(flag){
                            newDomain = prompt("domain, default: " + domainDefault + ", now: " + domain);
                            if(newDomain){
                                domain = newDomain;
                                console.log(domain);
                                url = getUrl(domain);
                                loadJS(url, function(){
                                    localStorage.setItem("googleTransDomain", domain);
                                    removeHint()
                                    transLoaded = true;
                                });
                            }
                        }
                        return;
                    }
                    btn.innerHTML = '<span id="loadingTranslate"><img class="icon" src="/static/image/google_translate/translate.svg"/>Loading ...</span>';
                    loading = true;
                    loadJS(url, function(){
                        localStorage.setItem("googleTransDomain", domain);
                        removeHint()
                        transLoaded = true;
                    });
                }
                </script>
            
    
        <script src="/static/js/plugin_blog/main.js"></script>
    
        <script src="/static/js/theme_default/tocbot.min.js"></script>
    
        <script src="/static/js/theme_default/main.js"></script>
    
        <script src="/static/js/theme_default/viewer.min.js"></script>
    
        <script src="/static/js/prism.js"></script>
    
        <script src="/static/js/search/search_main.js"></script>
    
        <script src="/static/js/gitalk/gitalk.min.js"></script>
    
        <script src="/static/js/gitalk/main.js"></script>
    
        <link rel="stylesheet" href="/static/js/add_hint/style.css" type="text/css"/>
    
        <script src="/static/js/add_hint/main.js"></script>
    
        <script src="/static/js/custom.js"></script>
    
        <script src="/static/js/thumbs_up/main.js"></script>
    
</body>

</html>