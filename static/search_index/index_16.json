{"/ai/en/maixhub/train_best.html":{"title":"Optimize MaixHub online train","content":" title: Optimize MaixHub online train >! this document need translation, help is welcome When using MaixHub to train a model, the recognition effect may not be very good or the actual running speed of the model may be slow. Here are some common tuning methods. Modifications and additions are welcome ## 识别效果优化 * 尽量多采集实际使用场景的图片，覆盖更多使用场景有利于提高最终识别率。 * 图片数量尽量不要太小，虽然平台限制最小数量为 20 张图才可以训练， 但要达到比较好的效果，显然一个分类 200 张都不算多，不要一直在 30 张训练图片上纠结为什么训练效果不好。。。 * 修改迭代次数，在发现`val_acc`仍然有上升趋势的情况下可以考虑适当增大迭代次数，但是迭代次数越大，训练时间越长，所以要根据实际情况权衡。 * 修改学习率和批数量大小，学习率不宜太大，否则会导致梯度爆炸出现`loss 为 0`或者`loss 为 inf`这样的错误，批数量大小不宜太小，否则会导致训练速度过慢，一般来说，学习率在 0.0001~0.001 之间，批数量大小在 16~64 之间都是比较合适的。另外需要注意批数量越大， 学习率就可以设置得稍微大一点。 * 每个标签的数据量都尽量多，而不是一个标签只有 20张，另一个500张图， 可以把训练参数处的“数据均衡“开关打开 * 默认分辨率但是 224x224， 是因为预训练模型是在 224x224 下训练的，当然也有其它分辨率的，比如 128x128，具体发现不支持的分辨率预训练模型，在训练日志中会打印警告信息。 * 为了让验证集的精确度的可信度更高（也就是在实际开发板上跑的精确度更接近训练时在验证集上的精确度），验证集的数据和实际应用的场景数据一致。比如训练集是在网上找了很多图片，那这些图片可能和实际开发板的摄像头拍出来的图有差距，可以往验证集上传一些实际设备拍的图来验证训练的模型效果。 这样我们就能在训练的时候根据验证集精确度（val_acc）来判断模型训练效果如何了，如果发现验证集精确度很低，那么就可以考虑增加训练集复杂度和数量，或者训练集用设备拍摄来训练。 * 对于检测训练项目，如果检测训练的物体很准，但是容易误识别到其它物体，可以在数据里面拍点其它的物体当背景；或者拍摄一些没有目标的图片，不添加任何标注也可以，然后在训练的时候勾选**允许负样本**来使能没有标注的图片。 * 检测任务可以同时检测到多个目标，如果你觉得识别类别不准，也有另外一种方式，先只检测模型检测到物体（一个类别），然后裁切出图片中的目标物体上传到分类任务，用分类任务来分辨类别。不过这样就要跑两个模型，需要写代码裁切图片（在板子跑就好了），以及需要考虑内存是否足够 ## 在开发板上运行速度慢 * 减小输入分辨率，比如在分类任务中可以使用`96x96`的小图来训练。 * 裁切一部分图像进行识别，识别时不对整张图片进行识别，可以裁切出部分图像进行识别。 * 选择更小的网络，比如分类选择`mobilenetV1 0.25`是`219KiB`, 而`mobilenetV1 0.75`则是`1.85MiB`，网络参数量减少了很多，不过相应地，模型识别精度也会降低。 ## 更快地标注数据 * 可以导入本地已经标注好了的数据到 MaixHub。 * MaixHub 支持视频辅助自动标注，只需拍摄视频上传的时候使用辅助标注功能即可，对于画面中只有单个物体的场景标注十分有用。 * 可以使用已经训练好的模型来辅助标注，虽然现在 MaixHub 不支持用训练好的模型来标注，但是你可以下载训练好模型到板子运行，写代码将识别到的物体坐标保存为`VOC`标注格式，就得到了新的标注数据，虽然可能会因为模型训练效果不够好标得不够准确，但是经过简单的手工筛选调整后，标注数据就可以新的训练了，如此反复，就会得到很多数据啦。 * MaixHub 未来可能会上线更好用的辅助标注工具哦~ 有建议欢迎通过 MaixHub 的反馈功能告诉我们哦~"},"/ai/en/maixhub/index.html":{"title":"Introduction to MaixHub","content":" title: Introduction to MaixHub [MaixHub](https://maixhub.com) is a platform released by Sipeed that integrates functions such as AI model service and community communication. It mainly provides the following functions: * Model library, directly download the model to the device to run and use, and share your own model to the model library. * Online training, you can easily train models without programming foundation and AI training experience, which is convenient for getting started with AI learning and accelerating AI application development. * Project sharing, share your own projects and works, exchange learning or find inspiration in the community. For more functions and more content, please visit [MaixHub](https://maixhub.com)."},"/ai/en/maixhub/faq.html":{"title":"MaixHub FAQ","content":" title: MaixHub FAQ ## Basic ### How to merge two account? Merging accounts are not supported, you can login one account and close this account in user center, then login another account and bind the first account. ## Online training ### How to optimize the model recognition effect? Refer to [Optimize MaixHub online train](./train_best.html)"},"/ai/en/index.html":{"title":"AI Guide","content":" title: AI Guide date: 2022 09 15 ## About this document `Sipeed` has launched a series of `AI` development boards, including: * `Maix I(M1)` series of `MaixBit`, `Maixduino` `M1s Dock` and other microcontroller development boards with hardware `AI` acceleration. * Cost effective `SOC` development boards with `AI` hardware acceleration, such as `M2 Dock` `MaixSense` of the `Maix II(M2)` series. * High performance `SOC` development boards such as `M3 AX Pi` of the `Maix III(M3)` series. In order to popularize the application of `AI` on edge devices, `Sipeed` has developed easy to use `MaixPy` and `MaixPy3` SDKs, and supplied [MaixHub](https://maixhub.com/) platform to make developers can easily train models without AI programming foundation and AI training experience. On this basis, this document is dedicated to providing developers with an `AI` development guide, the purpose is to allow novices to quickly get started with `AI` applications, or developers who have mastered `AI` related knowledge to quickly apply the results to the device or on the product. The content includes but is not limited to: * Basic knowledge of AI * Model training guide * AI tutorial recommendation * AI interesting project recommendation * Commonly used tools * Edge Device Deployment Guide ## Participate in contribution The content of the document will be continuously updated, and everyone is welcome to participate in the content writing. How to participate: * You can directly click the `Edit this page` button in the upper right corner of the document to jump to `GitHub` to edit and submit a `PR` (for specific methods, see [Contribution Documentation](/share_docs/zh/) or search engine search `GitHub' how to submit a PR`). * You can also directly send suggestions for changes or submit your manuscript to `support@sipeed.com`, the title should start with `[WiKi contribution]`, the text needs to indicate the author, the modified content and where to be modified, so that we can quickly transfer your Content updated to documentation."},"/ai/en/nn_models/mobilenet.html":{"title":"MobileNet Object Classification Model","content":" title: MobileNet Object Classification Model The Mobilenet network is a lightweight deep neural network proposed by Google for mobile phones and embedded scenarios. Its main feature is to use depthwise separable convolution instead of ordinary convolution, thereby reducing the amount of calculation and improving Computational efficiency of the network. The classification accuracy of the network on the ImageNet dataset has reached 70.8%. In the case of a small loss of accuracy, the amount of calculation is greatly reduced, making it possible for the neural network model to run smoothly on ordinary single chip computers! ## Target Input several images in order, output which category each image belongs to, and the confidence of the category. ## principle For those who are not very interested in the detailed principles, you can simply understand: * Convolution calculation has the function of extracting features. For example, the result of an image after a convolution calculation will extract the outline of the image, such as the classic Sobel edge detection. The result after a convolution calculation: ![](../../assets/sobel_edge2.jpg) It can be said that through a convolution calculation, the outline of the image is extracted, which is the feature extraction function of convolution calculation. After multiple convolution calculations, the features of different images will be extracted. You can see this process visualized at [tensorspace.org](https://tensorspace.org/html/playground/mobilenetv1.html). * After the entire network calculation consisting of countless convolution calculations and other calculations, an image with only 1000 pixels is finally output, and different classifications of image inputs are included. Among the 1000 pixels in the output layer, the value of one of the pixels It will be larger, and the classification corresponding to this pixel is the output result of the network. For example, if a panda image is input, if the value of the 388th pixel of the output layer is large and the value is 0.8, we will recognize that this image is a panda through this network, with a confidence level of 0.8. * As for the Mobilenet network claiming to use depthwise separable convolution (depthwise separable convolution) to replace ordinary convolution, thereby reducing the amount of calculation and improving the computational efficiency of the network, you can understand that compared to the previous convolutional network, it is still convolution Product calculation is just different from the ordinary convolution calculation method used on images, so that the number of calculations can be reduced and the calculation efficiency of the network can be improved. For the specific method, please read the relevant articles further. * As for training, as mentioned in the basic knowledge, by continuously inputting images to the network, then let the network output the correct classification, and then calculate the gap between the classification output by the network and the correct classification through the loss function, and then through backpropagation The algorithm continuously adjusts the parameters in the network, so that the gap between the classification output by the network and the correct classification becomes smaller and smaller, and finally the network can output the correct classification. Training is actually to find the parameters in the network suitable for our data, such as the values of all convolution kernels in the network, all bias values in the network, and so on. Regarding the specific principle of Mobilenet, we will not introduce it in detail here. Interested readers can refer to the original paper of Mobilenet ([MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) ) and Other tripartite tutorials ## Mobilenet V2 Mobilenet V2 is an upgraded version of Mobilenet. Its main improvement is to optimize the structure of the network, making the network more efficient and more accurate. ## Try to train a model Create a classification training project in [MaixHub](https://maixhub.com), then collect data, create a training task, and select the hardware platform you have as a parameter. If you don’t have a development board, you can use a mobile phone, and choose mobilenetv1 or mobilenetv2 for training That’s right, after the training is complete, you can deploy it with one click to see the effect."},"/ai/en/nn_models/fomo.html":{"title":"FOMO Lightweight Detection Model","content":" title: FOMO Lightweight Detection Model FOMO (Faster Objects, More Objects) is a lightweight target detection model proposed by Edgeimpulse engineers. Its main feature is that the model is very small and the amount of calculation is small. It is very fast and accurate. The advantage of FOMO is fast speed, and the disadvantage is that the accuracy is not high, but in some scenarios that do not require high accuracy, FOMO is a good choice. ## Target Input a picture, detect the position and size of the target, and recognize the category of the object, and can detect multiple objects at the same time. ## principle Original doc: [FOMO: Object detection for constrained devices](https://docs.edgeimpulse.com/docs/edge impulse studio/learning blocks/object detection/fomo object detection for constrained devices) Similar to YOLO v2, the purpose is to detect objects, but the post processing of YOLO v2 is still relatively complicated, there are a large number of bounding boxes to be processed, and it is still very difficult to run on a microcontroller without hardware acceleration. FOMO uses a simpler idea to detect: * Use a classic network as a feature extractor, such as MobileNet v1, and then truncate from the middle of the network to get a feature map. The size of this feature map is n x n x c, n is the width and height of the feature map, and c is the number of channels of the feature map. The value of n here depends on where to truncate from the network. For example, our input resolution is 128 x 128, and we want an 8 x 8 feature map output, which is equivalent to reducing the image resolution by 16 times. Find this layer truncation from the network Get an 8 x 8 feature map. * This n x n x c, c represents that there are c categories, each layer is used to find the position of a classified object, each layer has n x n pixels, and each pixel represents whether there is an object of this category at that position (Confidence ) * Traverse this n x n x c feature map, find the pixel coordinates whose confidence exceeds the set threshold, we think that there are objects in these places, and then map to the original image according to the scaling ratio, for example, when there is only one category, that is, c is 1, we need to detect One cup, the following results are obtained: ![](../../assets/fomo1.jpg) * After getting a lot of coordinate points that look valid, we think that there are objects in these places, but there are a lot of points, we can simply merge the points next to each other into a box, so that we get a big box ![](../../assets/fomo_result.jpg) * Because this box may not be very accurate sometimes, the official Edgeimpulse method is to use the center point of this box, so that the probability of this point on the object is relatively high. In the implementation of MaixHub, the box is directly given for you to choose Use the box directly, or if you want to use only the center point, you can modify the code yourself. In fact, the above processing has been encapsulated in the [code](https://github.com/sipeed/TinyMaix/tree/main/examples/maixhub_detection_fomo) library. In actual use, you only need to train the model and combine the code to run. ## Try to train a model Create a detection training project in [MaixHub](https://maixhub.com), then collect data and label (can be marked online), create a training task, select the TinyMaix platform as the parameter, select fomo, and select the backbone network according to the actual situation. For example, the name `mobilenetv1_0.25_8` means that the `mobilenetv1` network is used, and the alpha is 0.25. The smaller the value, the smaller the network and the lower the accuracy rate. The last 8 means that the output resolution is 1 of the input resolution. /8, for example, if the input is 128x128, the output is 16x16. The larger the output resolution, the more suitable for detecting smaller objects. Choose according to your MCU performance and the size of the object to be detected. Then just do the training. After the training is completed, the model is obtained. According to the instructions in [code](https://github.com/sipeed/TinyMaix/tree/main/examples/maixhub_detection_fomo), put the model file into the code. Then compile and run. You can test it under Linux now, and then transfer it to the single chip microcomputer to run."},"/ai/en/nn_models/yolov2.html":{"title":"YOLOv2 Object Detection Model","content":" title: YOLOv2 Object Detection Model YOLO (You Only Look Once) v2 has successfully achieved real time target detection and has a high detection accuracy. Just like its name, you only need to look at it to know the result. Because of its high detection efficiency, YOLO v2 can run on edge devices that are not very powerful, but it also has a disadvantage, that is, the detection ability for small objects is not enough. ## Target Input a picture, output the target category and position in the picture, and can detect multiple objects at the same time. ## principle Original paper: [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) The YOLOV2 backbone network is also a convolutional network, and the output layer uses a three dimensional structure such as S x S x (Bx5+C), where S is the size of the grid, B is the number of bounding boxes predicted by each grid, and C is the category quantity. That is, in S x S grids, each grid predicts B boxes, and each box has 5 parameters, namely x, y, w, h, p, where x, y are the center of the box The offset of the point relative to the upper left corner of the grid, w, h are the width and height of the box, p is the confidence of the box, and the last C parameters are the probability of the corresponding category. For these B boxes, we manually determined B anchors (pre selected boxes) in advance, each anchor has two parameters w, h, where w, h of the B prediction boxes are the scaling coefficients relative to the B anchors, In this way, the w and h of the prediction frame can be any value. > These B Anchors are obtained by using the K Means algorithm to cluster the width and height of all labeled boxes in the training data. The goal of clustering is to make the aspect ratio of each anchor close to the aspect ratio of the real box, so that the predicted box The aspect ratio of the frame will be closer to the aspect ratio of the real frame, thereby improving the detection accuracy. Therefore, there will be an anchors parameter in the code. This parameter is the B anchors obtained by counting the training data during training. In fact, when running the model, we must ensure that this parameter is consistent with the training time. After obtaining all the prediction frames, it is necessary to filter the prediction frames to remove some frames with low confidence, that is, the p mentioned above, which is the threshold (threshold) we see in the code; and nms (non maximum suppression/non maximum suppression) Calculate and remove boxes that predict the same classification and have too many overlapping parts, and keep the one with a higher probability, so we see in the code that there is an nms_threshold parameter, which is used to filter IOU (the intersection area (intersection) area/union area of two boxes) of boxes that are greater than this threshold. It looks a bit complicated. If you want to learn in detail, you can refer to more tripartite tutorials and official papers. In fact, the pure model of the YOLO v2 model we trained can output a three dimensional tensor such as S x S x (Bx5+C). Then the code library provides a decoding function. Call the function to input the three dimensional tensor to get the legal prediction frame output. Even if you don’t understand the principle, it doesn’t prevent you from using this model. ## YOLO v3 Compared with YOLO v2, YOLO v3 has more outputs, which is more conducive to detecting small objects, but because of the multiple outputs, the amount of calculation will be much larger, and the frame rate is naturally lower than YOLO v2. ## Try to train a model Create a detection training project in [MaixHub](https://maixhub.com), then collect data and label (can be marked online), create a training task, select the hardware platform you have, if there is no development board, you can use a mobile phone , select yolov2 for training, and after the training is completed, you can deploy it with one click to see the effect."},"/ai/en/basic/dnn_basic.html":{"title":"Basic knowledge of deep neural network (DNN)","content":" title: Basic knowledge of deep neural network (DNN) This article briefly introduces the relevant aspects of deep neural networks in a relatively popular way. The main purpose is to let students who have never been in contact with them learn the relevant knowledge of deep neural networks in machine learning from scratch. This article does not involve in depth knowledge introduction. If there is something wrong or needs to be supplemented, please submit the amendment. > This article is translated from Chinese, so may have some misexpressions, Pull request is welcome! ## How to solve a problem lead the machine to solve the problem A question is usually divided into **input** and **output (result)** For example: A straight line in the coordinate system is as follows, the value of the above data point is known: ![y kx+b](../../assets/dnn/ykxb.jpg) Now ask the question, if the data point law does not change, enter an x ​​coordinate of 20, what is the value of y? According to everyone’s knowledge, we all know that this is a one variable linear equation (`y kx + b` can be solved). Bring in the values ​​of two points and calculate the equation as `y 3x + 10`, then when `x 20 The value of `, `y` is `70`, so the input is `20` and the output is `70`. Here is input (`20`) + algorithm (one variable linear equation) output (`70`). This is the basic method we use to solve a problem, so the key is to find the one that conforms to the law of data points on this line segment algorithm. Humans are very powerful, they will summarize and learn from these data, and finally get this algorithm (equation), and then other people can directly use this algorithm to quickly solve similar problems, then, is there a way to let the machine automatically To find this algorithm? ## How to let the machine summarize the algorithm To let the machine automatically summarize the algorithm, that is, machine learning (ML, Machine Learning), let's take a look at how humans get this algorithm (equation). * Step 1: First, there are a large number of data points, and then based on these data points, humans found that the straight line conforms to the algorithm `y kx + b`, which adapts to all straight lines, but found that there are two unknowns `k` and ` b`, this is the parameter that adapts to any straight line * Step 2: Then what kind of straight line is the specific, because the equation has two unknowns, namely the parameters, put the actual two data points into this equation, and get `k 3` and `b 10` * Step 3: Then we use the online data points that are not used in step 2 to try whether the algorithm (equation) is correct, and finally we find that they are all verified correctly * Step 4: Then you need to know the value of `y` of other points through the value of `x`, just substitute `y 3x + 10` So, can machine learning also use this step? * We thoughtfully designed an algorithm structure, adding that we happened to design it directly as `y kx + b`, we left two parameters for the specific straight line, let’s call this structure **model structure** for now, because There are unknown parameters, which we call the untrained model structure. Where `x` is called **input**, and `y` is called **output** * Now, we substitute a few points of our straight line into this equation, we call this process **training**, to get the algorithm `y 3x + 10`, there are no unknown parameters, we now call it It is a **model** or a trained model, where `kb` is the parameter in the model, and `y kx + b` is the structure of the model. The data points brought into training are called **training data**, and their collective name is **training data set** * Then, we use several data points on the line segment that are not used in the training process as input, substitute this model for calculation, and get the result, such as `x 10`, get `y 40`, and then compare the output Whether the value is consistent with expectations, here we find that `x 10, y 40` is indeed on the straight line in the figure, and this point is not used during training, indicating that the model we got passed this verification. The process is called **verification**, `x 10, y 40` This data is called verification data. If we use multiple sets of data to verify this model, the collective term for these data is called **validation data set** * Now, we have obtained a **model**, and verified this model with **validation data set**, which seems to be very accurate, then we can assume that this model basically satisfies our future. x`, requires the value of `y` at any point on the line in the figure, you can enter `x` to give the `y` coordinate of the corresponding point on the line. This process is actually **using the model**, this process is called **reasoning** In fact, this is regarded as machine learning. What we humans need is to design the structure of `y kx + b`, and give **training data set** and **validation data set**, after **training** And **verify** to get a model we think is available, and then use `input + model` to get the correct `output (result)`. ## What is a deep neural network? Deep neural network (DNN) is a technology in the field of machine learning (ML). I mentioned a relatively simple example. According to a straight line data to predict any point on the straight line, the structure of `y kx + b` is artificially designed and very simple. When used for complex data, it is not found Applied, such as \"Is this picture a ball or a toy\" ![小球](../../assets/dnn/ball.jpg) ![toy](../../assets/dnn/toy.jpg) In order to store the information of the next straight line in the model, the structure `y kx + b` is used, and the features of the straight line are all stored in the model. The features used to store a picture now, the linear structure of `y kx + b`, and the two parameters of `k and b` obviously cannot be satisfied. A better structure needs to be designed. The network ** appeared, a kind of mesh structure, which can better remember the characteristic information of the picture, and this mesh structure is multi layered, that is, it has depth, so it is called a deep neural network (DNN). , Deep neural network), so DNN is a network structure and a means to realize machine learning. Each layer is composed of multiple nodes, as shown in the figure below, a DNN contains **input layer**, **hidden layer**, **output layer**, where the hidden layer consists of three layers (`A[1] , A[2], A[3]`layer), but collectively referred to as hidden layers: ![Deep Neural Network](../../assets/dnn/dnn.jpg) **Input layer**: The figure is a deep neural network structure, `x` is the input, for example, `x` here can be a picture, the input has multiple nodes, each node can be a pixel value, here the input layer draws 7 nodes, add We have a picture with a resolution of `10 x 10`, so the input layer requires a total of `100` nodes. Here the input layer is a one dimensional structure, and the actual situation may have a multi dimensional structure. For example, if the input is a grayscale image with a resolution of `3x3`, this is actually a two dimensional structure, that is, a matrix with two rows and two columns (about the matrix Please study the concept by yourself, or understand it as a two dimensional array for the time being), such as: ``` [[109 138 110] [220 37 166] [32 243 67] ] ``` The value range of each pixel is ∈[0, 255], and then we flatten it into a one dimensional array of 9 data for the input layer ``` [109 138 110 220 37 166 32 243 67] ``` > In addition, the value of the input layer will generally be normalized to the range `[0, 1]` If it is a color picture, it is three dimensional, that is `height, width, color channel`, color channels such as `RGB` three color channels, that is, the input has a shape (including the dimensions and the number of data in each dimension), For example, the one dimensional input shape above is `(9)`, and other images usually use `(height, width, number of channels)` to represent the shape, such as `(10, 10, 3)` for resolution of `10 x 10` , And there are three color channels, such as `RGB`. Here for the sake of getting started, the principle only introduces one dimensional situation **Output layer**: `y` is the output. The output here has two values. You can understand that the achievement is the `list` of MaixPy's two floating point values. `[Y1, Y2]`, `Y1` is the probability of being a small ball`, value ∈[0, 1], `Y2` is `probability of being a toy`. So in the end, we use this model, which is to give it a picture. The machine calculates according to the structure and algorithm specified by this model to get a `list`, and we know what is in the picture based on the output value. **Hidden layer**: The hidden layer connecting the input layer and the output layer, as well as the connection in between, are responsible for calculating the input data into a reasonable output value. ## Rest in between, summary So far, you know what a **model** is: it is a set of data structures that save the shape of a network and the parameters inside. Usually, the data of this model can be saved as a file, such as `.h5. Files such as tflite.kmodel` are used to explain the shape structure and parameters of this model, but they are used by different software. People only need to design the model structure and parameters to solve a class of problems, such as common object classification, such as the distinction between a ball or a toy in a picture as mentioned above. There are many parameters in this model. Specifically, when an object needs to be identified, a data set of known classification is used to allow the machine to automatically train a set of appropriate model parameters. Then we can enter the data and let the model infer the type of the input data. Therefore, if we don’t need to train the model and directly use the model trained by others, we only need: * Confirm the requirements and find a ready made model, because the model has been trained, and the meaning of the input and output shapes has been determined * Confirm the input shape of the model, such as the model input resolution `10x10` color image, you need to pass the required image to an input layer when using it * Confirm the meaning of the output layer, such as the aforementioned recognition of balls and toys, and the final output is a list representing the probability of the object, such as `[0.9, 0.1]`, the first value represents the probability of a ball, Then we know that there is a 90% probability of a small ball in this picture, and only a 10% probability of a toy * Put the model into the inference program to run. Don’t worry about the specific procedure, it will be introduced in the next chapter At this point, you should roughly understand the following things: * What is machine learning * What is a deep neural network (simple concept) * What is the model * What is the input layer, the output layer, what are the meanings of the classification applications in the example above, and what is the shape of the layer? * So far, I may not know what model training is * If I need a model, I know how to confirm the demand So, **if you only want to be able to use the model and don’t need training, you can do it here**, and you don’t need to know anything about the model, just use it as a **black box toolbox**. can. If you want a deeper understanding, please continue to read the following content. ## Continue: Deep Neural Networks (Continued Now that we have designed a multi layer design, let's go deeper: **Data flow**, **weight**, **bias**: When the model is inferring, the data flows from the input layer to the output layer, which is the direction of these mesh arrows (section 3 mesh diagram). The calculation from the previous layer to the next layer of each arrow can use a familiar formula: `y wx + b`, call `w` as **weight** (weight), `b` as **bias** (bias), note that each arrow has a separate `w, b `, that is to say, the value of the node of the next layer is equal to the value of the node of the previous layer after the calculation of this formula, the node of the next layer has multiple nodes of the previous layer, it is equal to the value of all the nodes of the previous layer after this The sum of the calculated values ​​of the formula. After so many calculations, the result finally appeared in the form of a value in the output layer, and the whole reasoning was completed. **Activation function**: Although the above model can get results through input, it will be found that all layer calculations are linear functions, so no matter how many layers there are, the whole is actually a linear function, that is, `y0 w1x + b1` + `y w2y0 + b2 ` > `y w2(w1x + b1) + b2` > `y w2w1x + w2b1 + b2`, in fact it is still a linear function, then the meaning of multiple layers is gone, so we need to add in the middle Non linear functions make the network a little more complicated, so I will do tricks on each node. Before each node outputs data, use a non linear function to calculate it, such as `sigmod` or `relu` function. It’s actually very simple to hear the name. Looking at the picture below, in short, x and y are not linear: ![sigmod](../../assets/dnn/sigmod.jpg) ![relu](../../assets/dnn/relu.jpg) That is, until now, except for the input layer, the output value of all nodes needs to go through `Sigmod(∑(Wn * x + Bn))`, and output a floating point value **softmax**: When the output layer is finally output, because of the previous calculations, the value range is not very uniform. Although we can compare the size, the largest value is considered the answer, but for the sake of uniformity and intuitively know the possibility of each category (In addition, for the accuracy of training, I won’t talk about it here.) As mentioned earlier, the probability of a category we finally output has a value range ∈[0, 1], and the sum of all output values ​​is `1`, so All values ​​of the output layer are processed after the output layer, the formula is ![softmax](../../assets/dnn/softmax.jpg) At this point, the inference process from input to output is over ## Deep neural network training Earlier we briefly introduced the structure and composition of deep neural networks and the forward process from the input layer to the output layer. When we use the model, this is the forward process. Then, the model is set, and the parameters (such as `w, b`) in it are all random values. How to make it automatically train to get the values ​​of the parameters in the model? As we mentioned earlier, we use some data input with known results to get the parameters. Similarly, here we also input data with known results to get the first output result **Judging the output accuracy (accuracy) (or error/loss)** and **loss function**: The results are obtained in the output layer. For example, `[0.6, 0.4]` represents the probability of a small ball `0.9`, and the probability of a toy is `0.1`, but because it is data with known answers, the actual correct answer is `[ 1.0, 0.0]`, which obviously does not meet the requirements. Therefore, the error between the correct answer and the calculated answer is: `[0.4, 0.4]`, but one problem is that the range of the error value is not very attractive. If the value range of the error is ∈`[0, ∞] `Just fine. In high school mathematics, there is a function `y log10(x)`, the coordinate diagram is as follows: ![log10](../../assets/dnn/log10x.jpg) It is found that when `x` takes the value ∈`[0, 1]`, the value of ` y` is exactly ∈`[0, ∞]`, and our output result is also exactly ∈`[0, 1]`! Therefore, we directly calculate the error like this: `error log10(output)`, that is, the closer the output is to `1`, the closer the error is to `0`. This method is called `CEE, Cross Entropy Error)`, in addition to this method, there are other methods such as Mean Squared Error (MSE, Mean Squared Error), etc. At this point, we know the error between the current result and the actual result **Back propagation of error** and **Parameter optimization (weight update)**: Because the parameters of the model do not meet our expectations, we need to modify the parameters. We use backpropagation. Earlier we got the error, because the parameters are not correct enough, we use this error to modify the parameters in the model to achieve the effect of fine tuning the parameters in the model. It is as if you are turning on a faucet. If the water hits (that is, the error is large), tighten the switch a little bit, loosen it a little bit if it is smaller, and adjust it. Just like our forward calculation, this time we changed it to reverse. From back to front, we can get the error value at each node, and then update the parameters in the model according to a certain learning rate. I won't elaborate on it for the time being. In short, after a round of reverse adjustment of parameters, a new model is obtained **Measure the quality of the model: training set error and validation set error**: We use the data in the training data set to repeatedly perform forward inference to obtain the error, and then adjust the process in the reverse direction. After using the training data set, we may get a relatively small error, but this only shows that the model is The data is more accurate, and some new data may not be accurate, so we need to use some data that is not in the training set to **verify** the effect of the model: We use **validation data set** to forward inference and get the error, because the validation data set is not involved in training, which means that the parameters of the model and the validation data set have nothing to do with each other. We use the error obtained to constant the model The better or worse, the smaller the error, the better the effect **Multiple iterations**: If you have trained all the data sets and find that the error is still large, you can continue training with multiple training methods, that is, **multiple iterations**. After each iteration, use the verification data set to verify the effect. If the error of the training set and the error of the validation set are small enough, we can temporarily assume that the model has good results. **Test Set**: At this time, we can use another batch of new data to test the effect of our model, because this is brand new data, did not participate in the training and did not participate in the verification (that is, determine when to stop training), theoretically more Credibility. If the test error is small, then the training is considered successful **Optimize training**: If the final effect is not very good, there are many places to adjust, such as * The number of training iterations is not the more the better. Too much training on a batch of data sets may cause the model to be effective only for this batch of data, and the generalization ability is not enough, that is, **overfitting** * The learning rate of each training can also be adjusted * Check the data set, whether there are some data that affect the classification * Optimize the network structure, whether it is input or output or internal structure and parameters, according to different data and tasks can have a better design, also called **feature engineering** ## Said at the end At this point, you should roughly understand the following things: * What is machine learning * What is a deep neural network * What is the model * What is the input layer, the output layer, what are the meanings of the classification applications in the example above, and what is the shape of the layer? * What is training and what is its function * What are the data training set, validation set, and test set, where are they used, and what needs to be paid attention to * What is the standard to measure the quality of the model If you still don’t understand, you can understand it again carefully, or refer to related materials. If you find a better way to explain it, please follow the document contribution method in the left directory to participate in the contribution."},"/ai/en/basic/courses.html":{"title":"A Summary of Tutorials in Machine Learning","content":" title: A Summary of Tutorials in Machine Learning keywords: machine learning, machine learning tutorial desc: Summary of tutorials in machine learning This article mainly summarizes some machine learning related resources. Edits and pull requests are welcome to add. First of all, you can simply read the first two articles [What is AI? ](./what_is_ai.html) and [Popular Explanation of Deep Learning](./dnn_basic.html) have a general understanding of machine learning in a popular way, and then learn theoretical knowledge. For developers, code is definitely the best way to get started. It can be understood more quickly. It is also possible to read the tutorial of the training framework and try it first before learning theoretical knowledge. * [Tensorflow Tutorials](https://www.tensorflow.org/tutorials) * [Tutorials for Pytorch](https://pytorch.org/tutorials/) Resources include but are not limited to: * books * github repository * website * video ## GitHub open source repository * [Mikoto10032/DeepLearning](https://github.com/Mikoto10032/DeepLearning): 深度学习入门教程, 优秀文章, Deep Learning Tutorial * [d2l ai/d2l zh](https://github.com/d2l ai/d2l zh): 《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被60个国家的400所大学用于教学。 * [scutan90/DeepLearning 500 questions](https://github.com/scutan90/DeepLearning 500 questions): 深度学习500问 * [AMAI GmbH/AI Expert Roadmap](https://github.com/AMAI GmbH/AI Expert Roadmap): Roadmap to becoming an Artificial Intelligence Expert in 2022 * [floodsung/Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep Learning Papers Reading Roadmap): Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech! * [mbadry1/DeepLearning.ai Summary](https://github.com/mbadry1/DeepLearning.ai Summary): personal notes and summaries on DeepLearning.ai specialization courses. ## Videos PR is welcome ## Books PR is welcome"},"/ai/en/basic/code_frameworks.html":{"title":"Common Code Frameworks and Toolkits in Machine Learning","content":" title: Common Code Frameworks and Toolkits in Machine Learning keywords: machine learning, code framework desc: A summary of common code frameworks and toolkits in machine learning Nowadays, writing machine learning programs does not require us to start from scratch. There are many good frameworks and many useful toolkits ## Pytorch Official website [pytorch.org](https://pytorch.org/), a framework preferred by the academic community, based on Python syntax, easy to use. ## Tensorflow The official website [www.tensorflow.org](https://www.tensorflow.org/), a machine learning framework from Google, has built in `keras` so that developers who are new to AI can get started quickly. The official website also provides many model examples and tutorials ## Numpy [Numpy](https://numpy.org/) is a mathematical library, which is usually competent for matrix related data expression and calculation. To install the Numpy package, you only need to execute `pip install numpy` to install successfully. ##Opencv [Opencv](https://opencv.org/) provides software packages in various languages ​​for graphics related processing, which is very easy to use. To install Python's Opencv package, you only need to execute `pip install opencv python` to install successfully. ## Matplotlib [Matplotlib](https://matplotlib.org/) is a commonly used library for visualizing data, which is very powerful. ## pandas [pandas](https://pandas.pydata.org/) is a library of data analysis and manipulation tools. ## More There are many other tools, welcome to add"},"/ai/en/basic/what_is_ai.html":{"title":"What is AI and machine learning?","content":" title: What is AI and machine learning? date: 2022 09 15 >! This document is not translate yet, translation is welcome AI（Artificial Intelligence） 想必大家都听之极多了，有人觉得 AI 要毁灭人类了；有人认为 AI 只是擅长某些特殊场景不必惊慌（比如 alpha Go 打败了人类围棋大师）；但大多数人接触到的 AI 可能更愿意称之为“人工智障”，比如手机里面的助手；对于很多工程师来说， AI 可能更多的是指机器学习，比如图像识别，语音识别，自然语言处理等等。 作为一个开发者， 首先我们需要了解大家经常听到的 `AI`，`机器学习`，`神经网络`等概念以及区别： * AI： 人工智能，是指让机器具有人类智能的能力，比如人类可以看到一张照片，然后判断出这是一只猫，这是一个人，这是一只狗等等，而机器也可以做到这一点，这就是人工智能。 * 机器学习： 一般是指让机器通过大量的数据，然后通过算法，让机器自己学习，比如通过大量的猫的图片，让机器自己学习，然后判断出一张图片是不是猫。 * 神经网络： 一般是指在机器学习中用到的一种数据结构，因为其类似人大脑的神经网络，各个数据节点互相连接互相通信和影响，故称之为神经网络。 * 模型： 指用来承载和表示机器学习过程中的相关参数的数据结构，一般可以保存为一个文件，可以将神经网络的结构和参数都存储在这个数据结构里面，方便用数学或者编程语言将其解析，比如取名叫`.model`格式的文件 所以可以理解为三者是包含关系： 神经网络 ∈ 机器学习 ∈ 人工智能， 模型文件则一般为机器学习中的产物， 另外你可能还听说过“深度神经网络”，其实也是属于神经网络，只不过是网络层数有深度不同一说。 而本文也大多阐述了如何利用各种神经网络模型和机器学习的方法来实现 AI 应用。 ## 机器学习过程简介 这里首先对机器学习的过程做一个通俗的介绍，不涉及数学公式，只是简单的介绍一下机器学习的过程。 ### 训练 以让机器区分猫咪和狗为例： 和教（训练）人类婴儿一样，可以把模型比作婴儿，为了让这个模型能认识猫和狗，我们需要一遍一遍地让它看各种猫猫狗狗，并让它去识别，错了我们就告诉它错了，对了我们就告诉它对了，这样一遍一遍地让它看，让它学习，最终它就能区分猫和狗了。 从这段话我们分析出训练的时候几个关键点： * 模型： 一个工具或者黑盒，给它一个输入，它能给我们一个输出结果。 * 输入： 这里是图像，猫或者狗的图像。 * 输出： 猫或者狗 * 判断错误的方法：也就是它的输出和真实的结果是否一样，这里是靠教学者判断正误的，也就是判断错误的方法是教学者。 * 学习方法：就是当我们告诉模型输出结果是错的时候，它如何去改进。 得到这几个关键点后，就可以很好地理解这个机器学习的过程了： * 定义模型的输入输出， 输入是图像，输出是猫或者狗。 * 为了让这个模型能够有学习的本领，也就是和人一样有足够的脑容量， 我们定义一个属于模型的“脑子”，也就是一个看起来和人脑突触结构类似的神经网络结构： ![神经网络](../../assets/dnn.jpg) 可以看到输入和中间每个节点间都有线连起来，每条线都是一个计算公式，具体是什么样的公式以及具体如何设计一个这样的结构这里先不细究，先有个概念就行。 * 然后就是判断错误和误差的方法，一般在代码中称之为损失函数，也就是模型的输出结果是否正确。 * 然后就是学习方法，比如结果不正确，如何去微调模型内部的参数，让下一次的输出结果更接近正确的结果。 ### 验证 经过很多数据的反复训练后，我们发现好像基本都能识别正确了，但是我们还是担心我们用的图片种类是否不够多，这个模型是否真的能够识别所有的猫和狗或者其它猫和狗，这个时候就需要验证了，验证的方法就是把模型拿出来，给它一些新的数据，即在训练的时候从来没用到过的数据，让它去识别，看看它的识别结果是否正确，如果正确，那我们就认为这个模型泛化效果不错，可以放心的使用这个模型去识别猫猫狗狗了。 一般我们会在训练是一段时间后拿出`验证集`（也就是用来验证的数据集，对应训练的数据叫`训练集`）来测试一下模型的效果，如果效果不错就可以停止训练了，如果效果不好，则需要继续训练或者考虑是不是训练集有问题，或者模型结构、损失函数、学习方法等有问题了。 ### 测试 验证效果的好坏决定了我们合适停止训练，也就是说模型效果如何和`验证集`紧密相关，相当于**验证集也变相地参与了模型的训练过程**， 所以在结束训练后，我们用一个新的数据集`测试集`来测试一下模型的效果，这个数据集是在训练和验证的时候从来没用过的，这样就可以更加客观地评估模型的效果了。 这里共提到了`训练集`，`验证集`，`测试集`，需要注意他们三个数据集的区别！前两者在训练过程参与，后者在训练过程不参与，只是用来评估模型的效果，并且三者互相不重合，防止训练过程中模型只对一小部分数据有效，到了新的场景就无法识别（也就是所谓的`过拟合`）。 ### 总结 这里简单阐述了机器学习的过程的通俗解释，你也可以到 [MaixHub](https://maixhub.com) 注册登录后体验自己体验一遍在线训练过程加深理解，无需懂代码，懂得这里描述的机器学习的过程就可以了，然后再进行进一步学习。"},"/ai/en/deploy/k210.html":{"title":"Deploy model to Maix-I(M1) K210 series development boards","content":" title: Deploy model to Maix I(M1) K210 series development boards date: 2022 09 15 >! This document is not translate yet, translation is welcome > 欢迎修改和补充 一般使用 `tensorflow` 训练出浮点模型， 再使用转换工具将其转换成 `K210` 所支持的 `Kmodel` 模型，然后将模型部署到 `K210` 开发板上。 ## K210 上的 KPU `K210` 上的 AI 硬件加速单元取名为`KPU`，`KPU` 实现了 卷积、批归一化、激活、池化 这 4 种基础操作的硬件加速， 但是它们不能分开单独使用，是一体的加速模块。 所以， 在 KPU 上面推理模型， 以下要求： ### 内存限制 K210 有 6MB 通用 RAM 和 2MB KPU 专用 RAM。模型的输入和输出特征图存储在 2MB KPU RAM 中。权重和其他参数存储在 6MB 通用 RAM 中，在转换模型时，会打印模型使用的内存大小以及临时最大内存使用情况。 ### 哪些算子可以被 KPU 完全加速？ nncase 支持的算子： * nncase v0.2.0 支持的算子： https://github.com/kendryte/nncase/blob/master/docs/tflite_ops.md * nncase v0.1.0 支持的算子： https://github.com/kendryte/nncase/tree/v0.1.0 rc5 下面的约束需要全部满足。 * 特征图尺寸：输入特征图小于等于 320x240 (宽x高) 同时输出特征图大于等于 4x4 (宽x高)，通道数在 1 到 1024。 * Same 对称 paddings (TensorFlow 在 stride 2 同时尺寸为偶数时使用非对称 paddings)。 * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，stride 为 1 或 2。 * 最大池化 MaxPool (2x2 或 4x4) 和 平均池化 AveragePool (2x2 或 4x4)。 * 任意逐元素激活函数 (ReLU, ReLU6, LeakyRelu, Sigmoid...), KPU 不支持 PReLU。 ### 哪些算子可以被 KPU 部分加速？ * 非对称 paddings 或 valid paddings 卷积， nncase 会在其前后添加必要的 Pad 和 Crop（可理解为 边框 与 裁切）。 * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，但 stride 不是 1 或 2。 nncase 会把它分解为 KPUConv2D 和一个 StridedSlice (可能还需要 Pad)。 * MatMul 算子， nncase 会把它替换为一个 Pad(到 4x4)+ KPUConv2D(1x1 卷积和) + Crop(到 1x1)。 * TransposeConv2D 算子， nncase 会把它替换为一个 SpaceToBatch + KPUConv2D + BatchToSpace。 > 以上说明来自[这里](https://github.com/kendryte/nncase/blob/master/docs/FAQ_ZH.md) ## 训练出浮点模型 对于 K210， 建议使用 TensorFlow，因为它的转换工具对其支持最好。 tensorflow 举个例子， 两分类模型， 这里是随便叠的层结构 ```python import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers input_shape (240, 320, 3) model tf.keras.models.Sequential() model.add(layers.ZeroPadding2D(input_shape input_shape, padding ((1, 1), (1, 1)))) model.add(layers.Conv2D(32, (3,3), padding 'valid', strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); #model.add(MaxPool2D()); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(64, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(64, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(64, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Flatten()) model.add(layers.Dropout(0.5)) model.add(layers.Dense(2)) model.add(layers.Activation('softmax')) model.summary() model.compile( loss 'sparse_categorical_crossentropy', optimizer 'adam', metrics ['accuracy']) mode.fit(...) ``` 这里你可能注意到了, 在 `conv` 层中`stride ! 1` 时, 都加了一个 `zeropadding` 层, 这是 K210 硬件支持的模式, 如果不这样做, 转换成 V3 模型时(使用 nncase v0.1.0 RC5) 则直接报错, 使用 V4 模型(nncase V0.2.0转换)可以通过,但是是使用软件运算的, 会消耗大量内存和时间, 会发现内存占用大了很多!!! 所以设计模型时也需要注意 ## 转换工具 使用 K210 芯片官方提供的 [nncase](https://github.com/kendryte/nncase) 工具来进行转换。 需要注意的是，工具版本更新迭代比较多， `K210`属于第一代芯片，算子支持有限，并且内存只有`6MiB（通用）+2MiB（AI专用）`内存，所以最新版本的工具可能并不是最好的选择，根据需求选择合适的版本。 由于代码更新， 在过程中**模型格式**产生了两个大版本， `V3` 和 `V4`， 其中 `V3` 模型是指用 [nncase v0.1.0 RC5](https://github.com/kendryte/nncase/releases/tag/v0.1.0 rc5) 转换出来的模型； `V4`模型指用 [nncase v0.2.0](https://github.com/kendryte/nncase/releases/tag/v0.2.0 beta4) 转换出来的模型，以及 V5 或更新版本等等。 两者有一定的不同，所以现在两者共存， `V3` 代码量更少，占用内存小，效率也高，但是支持的算子少； `V4` 支持的算子更多，但是都是软件实现的，没有硬件加速，内存使用更多，所以各有所长。 `MaixPy` 的固件也可以选择是否支持 `V4`， 如果你的模型 `V3` 能够满足算子支持，强烈建议用 `V3`，在遇到算子不支持而且一定要用那个算子时再用`V4`。 ## 运行测试模型 使用 [MaixPy](/maixpy) 来运行模型，也可以用 [C SDK](https://github.com/sipeed/LicheeDan_K210_examples) 写。 比如使用 `MaixPy`固件， 将模型放到 SD 卡， 然后使用代码加载 ```python import KPU as kpu import image m kpu.load(\"/sd/test.kmodel\") img image.Image(\"/sd/test.jpg\") img img.resize(224, 224) img.pix_to_ai() feature_map kpu.forward(m, img) p_list feature_map[:] ``` ## 更多参考 * [K210 MaixPy 从入门到飞升 AI视觉篇 完全教程（以及一些小问题处理比如内存不足）](https://neucrack.com/p/325) * [MaixPy AI 硬件加速基本知识](/soft/maixpy/zh/course/ai/basic/maixpy_hardware_ai_basic.html) ## 上传分享到 MaixHub 可以上传分享你的模型到到 [MaixHub](https://maixhub.com/) 的模型库，可以让更多人发现并使用你的模型~ 一起做出更多有趣的项目吧！（K210 模型支持加密分享） 另外你也可以使用 [MaixHub](https://maixhub.com/) 的模型库，下载别人分享的模型，或者使用在线训练出的模型，直接使用或者参考模型结构。"},"/ai/en/deploy/ax-pi.html":{"title":"Deploy models to AX-Pi (Maix-III(M3) series) board","content":" title: Deploy models to AX Pi (Maix III(M3) series) board date: 2022 09 21 update: date: 2025 04 17 author: Aristore content: Translated the documentation > This article is translated from Chinese, so may have some misexpressions, Pull request is welcome! <div id \"title_card\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/maix iii small.png\" alt \"AXera Pi Model Conversion and Deployment\"> <div class \"card_info card_purple\"> <div class \"title\">Maix III Series AXera Pi</div> <div class \"brief\"> <div>High computing power, unique AI ISP imaging system</div> <div>Up to 3.6Tops@INT8, rich operator support</div> </div> </div> </div> </div> <style> #title_card { width:100%; text align:center; background color: #fafbfe; margin bottom: 1em; } #title_card img { max height: 20em; } .card_purple { background color: #d1c4e9; color: #673ab7; } .dark .card_purple { background color: #370040; color: #ffffffba; } .title { font size: 1.5em; font weight: 800; padding: 0.8em; } </style> > Any thoughts or modification suggestions? Feel free to leave comments or directly click the \"Edit this page\" button in the top right corner to submit a PR. > [MaixHub](https://maixhub.com/model/zoo) model zoo has models that can run directly on AXera Pi. You can download and use them, and also share your models~ [Click to view Maix III(M3) series AXera Pi development board details and basic usage documentation](/hardware/en/maixIII/index.html) To deploy models to `AXera Pi`, you need to quantize the model to INT8 to reduce model size and improve runtime speed. Generally, `PTQ` (Post Training Quantization) is used. Steps: **1.** Prepare the floating point model. **2.** Use the model quantization and format conversion tool to convert it into a format supported by AXera Pi. The tool here is [pulsar](https://pulsar docs.readthedocs.io) provided by AXERA. > This document provides a quick start guide and process overview. It is strongly recommended to read through this document first, then check the pulsar documentation for more details. **3.** Run the model on AXera Pi. ## Prepare Floating Point Model Train the model using `Pytorch` or `TensorFlow`, then save it in `onnx` format. Note that only operators supported by `AXera Pi` can be used. See [Operator Support List](https://pulsar docs.readthedocs.io/zh_CN/latest/appendix/op_support_list.html). For some networks, post processing may need to be separated and handled by the `CPU`. ### Example Using [mobilenetv2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) from PyTorch Hub: ```python import torch model torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained True) model.eval() ``` Export to `onnx` format: ```python x torch.randn(1, 3, 224, 224) torch.onnx.export(model, x, \"mobilenetv2.onnx\", export_params True, verbose True, opset_version 11) ``` Some models can be simplified with `onnxsim` (not needed for this model): ``` pip install onnx simplifier python m onnxsim mobilenetv2.onnx mobilenetv2 sim.onnx ``` ## Model Quantization and Format Conversion ### Install `docker` [Installation Guide](https://docs.docker.com/engine/install/) Verify installation with: ``` docker version ``` On Linux, add the current user to the `docker` group to avoid needing `sudo`: ```shell sudo gpasswd a $USER docker newgrp docker ``` ### Download Conversion Tool The conversion tool is provided as a Docker image. Pull the image using: Source Description Command : : : [dockerhub](https://hub.docker.com/r/sipeed/pulsar/tags) Pull directly `docker pull sipeed/pulsar` Domestic Mirror (China) China domestic mirror for accelerated downloads 1. Edit `/etc/docker/daemon.json` to add `\"registry mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"]`<br>2. `docker pull sipeed/pulsar` After pulling, verify with `docker images` to see `sipeed/pulsar:latest`. > Note: The image name `sipeed/pulsar` is equivalent to `axera/neuwizard` mentioned in some documents. Create a container: ```shell docker run it net host rm shm size 32g v $PWD:/data sipeed/pulsar ``` > * Adjust ` shm size` based on your system's memory. > * Omit ` rm` to keep the container, and use ` name xxx` to name it for later reuse. > * ` v host_path:/data` mounts the host directory to `/data` in the container. Inside the container, use `pulsar h` to view commands. ### Perform Model Quantization and Conversion Refer to [pulsar](https://pulsar docs.readthedocs.io) documentation for conversion commands and configuration file setup. > Note: AXera Pi uses virtual NPU concepts to allocate computing resources between NPU and AI ISP. #### Example Using `mobilenetv2`: * Prepare configuration file `config_mobilenetv2.prototxt` (details in [Configuration Documentation](https://pulsar docs.readthedocs.io/zh_CN/latest/test_configs/config.html)): .. details:: config_mobilenetv2.prototxt ```protobuf # Basic configuration parameters: input/output input_type: INPUT_TYPE_ONNX output_type: OUTPUT_TYPE_JOINT # Hardware platform selection target_hardware: TARGET_HARDWARE_AX620 # CPU backend selection, default using AXE cpu_backend_settings { onnx_setting { mode: DISABLED } axe_setting { mode: ENABLED axe_param { optimize_slim_model: true } } } # ONNX model input data type description src_input_tensors { color_space: TENSOR_COLOR_SPACE_RGB } # Joint model input data type configuration dst_input_tensors { color_space: TENSOR_COLOR_SPACE_RGB } # NeuWizard tool configuration parameters neuwizard_conf { operator_conf { input_conf_items { attributes { input_modifications { # y x * (slope / slope_divisor) + (bias / bias_divisor) # Normalize data to [0, 1] range first affine_preprocess { slope: 1 slope_divisor: 255 bias: 0 } } input_modifications { # y (x mean) / std # Standardize using training parameters input_normalization { mean: [0.485,0.456,0.406] ## Mean values (order depends on src_input_tensors.color_space, here [R G B]) std: [0.229,0.224,0.255] ## Standard deviation values } } } } } dataset_conf_calibration { path: \"imagenet 1k images rgb.tar\" # PTQ calibration dataset path type: DATASET_TYPE_TAR # Dataset type: TAR package size: 256 # Number of images used for calibration batch_size: 1 } } # Pulsar compiler batch size configuration pulsar_conf { ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111 # Use virtual NPU, split resources between NPU and AI ISP (111 represents NPU) # AX620_VIRTUAL_NPU_MODE_0 # Disable virtual NPU, full resources to NPU # AX620_VIRTUAL_NPU_MODE_112 # Use virtual NPU, split resources (112 represents AI ISP exclusive use, do not modify casually) batch_size: 1 } ``` > Preprocessing must match the training pipeline (normalize to [0,1] then apply mean/std). > The `imagenet 1k images rgb.tar` dataset can be downloaded from [Baidu Cloud](https://pan.baidu.com/s/1TiZSIm0fpqbLn 2qLBX58g?pwd 1rpb) or [GitHub](https://github.com/sipeed/sipeed_wiki/releases/download/v0.0.0/imagenet 1k images rgb.tar). > The `ax620_virtual_npu` setting is critical for AI ISP compatibility. Then execute the following inside the `docker` container (note that the files are mounted from the host machine to the `docker` container using the ` v` parameter of the previous `docker run` command, so just copy them directly into the host's directory): ``` pulsar build input mobilenetv2.onnx output mobilenetv2.joint config config_mobilenetv2.prototxt output_config out_config_mobilenet_v2.prototxt ``` Be patient, as it may take a little while, and you will eventually get the converted model result `mobilenetv2.joint`. ### Using GPU for Model Quantization and Format Conversion in Docker By default, docker cannot use the graphics card driver, but if needed, it’s not difficult: * Install the graphics card driver on the host machine as usual. For example, on `ubuntu`, it can be installed directly via the package manager. * Follow the instructions at [nvidia docker](https://github.com/NVIDIA/nvidia docker) to install, then test whether it is usable: ``` docker run rm gpus all nvidia/cuda:11.0.3 base ubuntu20.04 nvidia smi ``` This will execute the `nvidia smi` command, allowing you to see the GPU information mapped into docker. * When creating containers that require the use of the GPU, add the ` gpus all` parameter to map all GPU drivers into the container, or specify particular GPU numbers with ` gpus '\"device 2,3\"'`, for example: ``` docker run it net host rm gpus all shm size 32g v $PWD:/data sipeed/pulsar ``` Note that the current version (0.6.1.20) of `pulsar build` only supports sm_37 sm_50 sm_60 sm_70 sm_75 architecture GPUs; 30/40 series GPUs are not yet supported. ## Testing Model Execution on AXera Pi After converting the model according to the documentation, transfer the model to `AXera Pi` via `scp` or `adb` and run the model using the model testing commands provided in the documentation. ### Example Still using `mobilenetv2` as an example: Save the test image as `cat.jpg`: <img src \"../../assets/cat.jpg\" style \"max height: 20em;\"> * First, compare the results with the `onnx` model on your computer: ``` pulsar run mobilenetv2.onnx mobilenetv2.joint input cat.jpg config out_config_mobilenet_v2.prototxt output_gt gt ``` You’ll obtain the cosine distance, which here is `0.9862`, indicating that the output similarity between the `joint` model and the `onnx` model is `98.62%`, within an acceptable range. If the value is too small, it indicates errors during quantization, suggesting potential issues with settings, input data, or model design. ```log Layer: 536 2 norm RE: 17.03% cosine sim: 0.9862 ``` * Copy the model to `AXera Pi` and run it directly (use the `scp` command to copy the `joint` format model file to the development board): Run the model on the board: ``` time run_joint mobilenetv2.joint repeat 100 warmup 10 ``` You’ll see the model execution time is `2.1ms`. Here we haven’t enabled the virtual NPU; if enabled, the time doubles to `4ms`. Additionally, `overhead 250.42 ms` represents other timing costs (e.g., model loading, memory allocation). ``` Run task took 2143 us (99 rounds for average) Run NEU took an average of 2108 us (overhead 9 us) ``` If you want to test inputs, first convert the image to binary content arranged in `HWC + RGB` order, and specify the binary file with ` data`. .. details:: Script to convert to binary file ```python from PIL import Image import sys out_path sys.argv[2] img Image.open(sys.argv[1]) img img.convert('RGB') img img.resize((224, 224)) rgb888 img.tobytes() with open(out_path, \"wb\") as f: f.write(rgb888) ``` Execute `python convert.py cat.jpg cat.bin` to obtain the `cat.bin` file. ``` run_joint mobilenetv2.joint data cat.bin bin out dir ./ ``` A `bin` file sized `4000` bytes, i.e., `1000` `float32` values, will be generated in the directory. You can load it with `python` to find the maximum value. ```python out np.fromfile(\"536.bin\", dtype np.float32) print(out.argmax(), out.max()) ``` The result is `282 8.638927`. In the [labels](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt), index `282`, or line `283`, corresponds to `tiger cat`. This matches the result of running the floating point model directly on the computer (`282 9.110947`), although there are slight differences, they are within an acceptable range. > Note that no `softmax` calculation was performed here, so `out.max()` is not a probability. ## Writing Code to Run the Model To formally run the model, you might need to modify the code, change input preprocessing, or add post processing. Currently, a `C/C++` SDK is provided, and reference code is available at [ax samples](https://github.com/AXERA TECH/ax samples). Cross compilation is possible, or you can compile directly on `AXera Pi`. Code for running classification models is located in [ax_classification_steps.cc](https://github.com/AXERA TECH/ax samples/blob/main/examples/ax_classification_steps.cc). After compiling according to the repository’s instructions, you'll get the executable `build/bin/install/ax_classification`, which you can copy to the development board to execute: ``` ./ax_classification m mobilenetv2.joint i cat.jpg ``` > The code uses `opencv` to read images in `BGR` format. When running the model, it automatically determines based on the conversion model configuration whether to convert to `RGB`. So, `mw::prepare_io` is used to copy the `BGR` image to the input buffer, and further processing is handed off to the underlying library. If your model isn't a simple classification model, you may need to add post processing code after model inference to parse the results. ## Using Cameras and Screens At this point, the model runs independently. To build an application, since it’s `Linux`, many general development methods and libraries are available. If you wish to use cameras and screens together, you can use [libmaix](https://github.com/sipeed/libmaix) or develop directly using [axpi_bsp_sdk](https://github.com/sipeed/axpi_bsp_sdk) (which is somewhat more challenging). * Refer to the [SDK Development Instructions](/hardware/en/maixIII/ax pi/sdk.html) to compile and execute the [camera screen display routine](https://github.com/sipeed/libmaix/tree/release/examples/axpi). Since the repositories are on `github`, having a good proxy server is recommended. * When merging the model execution code into the routine, one important issue to note: if you plan to use the camera, the default `AI ISP` will be activated (no way to turn it off for now, TODO for future updates). **Therefore, when converting the model, specify it to run on the virtual NPU by setting `ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111` in the configuration file, otherwise initialization will fail**. > You can directly use the [1000 classification routine](https://github.com/sipeed/libmaix/tree/release/examples/axpi_classification_cam). > After compilation on the board, execute `./dist/axpi_classification_cam m mobilenetv2.joint` to start recognition. Models can also be downloaded from the [MaixHub Model Zoo](https://maixhub.com/model/zoo/89). ## QAT Quantization and Other Optimization Methods `QAT` (Quantization Aware Training) involves simulating quantized inference during training to reduce quantization errors. Unlike `PTQ` (Post training Quantization), which quantizes already trained models, `QAT` offers higher accuracy but is more complex. It is not recommended to start with `QAT`. For more details, see [superpulsar](https://pulsar docs.readthedocs.io). The documentation will continue to be updated, and if you're proficient in this area, feel free to click `Edit this page` in the upper right corner to contribute. ## Other References and Shared Summaries > Feel free to share your work! Click `Edit this page` in the upper right corner to add your contributions. * [爱芯元智AX620A部署yolov5 6.0模型实录](https://zhuanlan.zhihu.com/p/569083585) * [AX620A运行yolov5s自训练模型全过程记录（windows）](http://t.csdn.cn/oNeYG) * [MOT：如何在爱芯派上实现多目标跟踪的神奇效果！](https://www.yuque.com/prophetmu/chenmumu/ax_tracker) * [MMPose：在爱芯派上玩转你的关键点检测！](https://www.yuque.com/prophetmu/chenmumu/m3axpi_keypoint) * [2023年最新 使用 YOLOv8 训练自己的数据集，并在 爱芯派硬件 上实现 目标检测 和 钢筋检测 ！！](https://www.yuque.com/prophetmu/chenmumu/m3axpi)"},"/ai/en/deploy/index.html":{"title":"Deploy models to edge devices methods","content":" title: Deploy models to edge devices methods date: 2022 09 21 class: heading_no_counter <div id \"maixhub\"> <a href \"https://maixhub.com/model/zoo\">Find</a>or<a href \"https://maixhub.com/model/zoo/share\">share</a> edge models on MaixHub </div> <div id \"deploy_items\"> <a href \"./k210.html\"> <div class \"card\"> <img src \"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt \"K210 model convert and deployment\"> <div class \"card_info card_red\"> <h2>Maix I Series K210</h2> <div class \"brief\"> <div>MCU with hardware AI acceleration</div> <div>1Tops@INT8, limited operators support</div> </div> </div> </div> </a> <a href \"./v831.html\"> <div class \"card\"> <img src \"/hardware/assets/maixII/m2dock.jpg\" alt \"V831 model convert and deployment\"> <div class \"card_info card_blue\"> <h2>Maix II Series v831</h2> <div class \"brief\"> <div>SOC with hardware AI acceleration, Linux OS</div> <div>0.2Tops@INT8, limited operators support</div> </div> </div> </div> </a> <a href \"./tinymaix.html\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/m0_small.png\" alt \"TinyMaix model convert and deployment\"> <div class \"card_info card_green\"> <h2>TinyMaix platform</h2> <div class \"brief\"> <div>For all MCU with optimization for many ARCH</div> <div>Depends on CPU, limited operators support</div> </div> </div> </div> </a> <a href \"./ax pi.html\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/maix iii small.png\" alt \"AX Pi model convert and deployment\"> <div class \"card_info card_purple\"> <h2>Maix III Series AX Pi</h2> <div class \"brief\"> <div>High computing power, special AI ISP</div> <div>Max 3.6Tops@INT8, many operators</div> </div> </div> </div> </a> </div> <style> #deploy_items { display: flex; justify content: space evenly; flex wrap: wrap; margin: 0 10px; } #deploy_items a:hover { background color: transparent; } #deploy_items > a { margin: 1em; } .card { display: flex; flex direction: column; justify content: space between; align items: center; box shadow: 5px 6px 20px 4px rgba(0, 0, 0, 0.1); border radius: 0.6rem; transition: 0.4s; background: white; } .card:hover { box shadow: 5px 6px 40px 4px rgba(0, 0, 0, 0.1); scale: 1.05; } .card_info { display: flex; flex direction: column; align items: center; border radius: 0 0 0.6rem 0.6rem; } .card img { height: 10em; width: 14em; object fit: cover; } .card_info > h2 { font size: 1.2em; margin: 0.2em; padding: 0.2em 1em; } .card_info > .brief { margin: 0.2em; padding: 0.2em 1em; display: flex; flex direction: column; align items: center; } .card_red { background color: #ffcdd2; color: #cf4f5a; } .card_blue { background color: #90caf9; color: #105aa9; } .card_green { background color: #b2dfdb; color: #009688; } .card_purple { background color: #d1c4e9; color: #673ab7; } #maixhub { display: flex; justify content: center; align items: center; margin: 1em 0; width: 100%; background color: #f5f5f5; color: #727272; border radius: 0.6rem; padding: 1em; } .dark #maixhub { background color: #2d2d2d; color: #bfbfbf; } .dark .card_blue { background color: #003c6c; color: #ffffffba; } .dark .card_red { background color: #5a0000; color: #ffffffba; } .dark .card_green { background color: #004e03; color: #ffffffba; } .dark .card_purple { background color: #370040; color: #ffffffba; } </style>"},"/ai/en/deploy/tinymaix.html":{"title":"Use TinyMaix to deploy AI models to MCU","content":" title: Use TinyMaix to deploy AI models to MCU date: 2022 09 15 [TinyMaix](https://github.com/sipeed/TinyMaix) is a model inference runtime designed for limited memory and compute resources MCU, can even run `MNIST` on `2KB` RAM's MCU `Arduino ATmega328`, especially optimized for many architecture like RISC V, ARM Cortex M etc. More details see [TinyMaix official repository](https://github.com/sipeed/TinyMaix)"},"/ai/en/deploy/v831.html":{"title":"Deploy models to V831","content":" title: Deploy models to V831 date: 2022 09 15 >! This document is not translate yet, translation is welcome ## 制作浮点模型 对于 V831， 强烈推荐使用`Pytorch`训练模型，因为模型转换工具对其支持较好。 这里直接使用 pytorch hub 的预训练模型为例。 这里省略了模型定义和训练过程， 直接使用 pytorch hub 的 resnet18 预训练模型进行简单介绍： https://pytorch.org/hub/pytorch_vision_resnet/ 注意 V831 支持的算子有限，具体请在 [MaixHub](https://maixhub.com/) 点击`工具箱 >模型转换 >v831`中的文档查看。 ## 在 PC 端测试模型推理 根据上面链接的使用说明， 使用如下代码可以运行模型 其中， label 下载： https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt ```python import os import torch from torchsummary import summary ## model model torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained True) model.eval() input_shape (3, 224, 224) summary(model, input_shape, device \"cpu\") ## test image filename \"out/dog.jpg\" if not os.path.exists(filename): if not os.path.exists(\"out\"): os.makedirs(\"out\") import urllib url, filename (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename) try: urllib.URLopener().retrieve(url, filename) except: urllib.request.urlretrieve(url, filename) print(\"test image:\", filename) ## preparing input data from PIL import Image import numpy as np from torchvision import transforms input_image Image.open(filename) # input_image.show() preprocess transforms.Compose([ transforms.Resize(max(input_shape[1:3])), transforms.CenterCrop(input_shape[1:3]), transforms.ToTensor(), transforms.Normalize(mean [0.485, 0.456, 0.406], std [0.229, 0.224, 0.225]), ]) input_tensor preprocess(input_image) print(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor))) input_batch input_tensor.unsqueeze(0) # create a mini batch as expected by the model ## forward model # move the input and model to GPU for speed if available if torch.cuda.is_available(): input_batch input_batch.to('cuda') model.to('cuda') with torch.no_grad(): output model(input_batch) ## result # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes # print(output[0]) # The output has unnormalized scores. To get probabilities, you can run a softmax on it. max_1000 torch.nn.functional.softmax(output[0], dim 0) max_idx int(torch.argmax(max_1000)) with open(\"imagenet_classes.txt\") as f: labels f.read().split(\"\\n\") print(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx])) ``` 运行后 结果： ``` Using cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0 Layer (type) Output Shape Param # Conv2d 1 [ 1, 64, 112, 112] 9,408 BatchNorm2d 2 [ 1, 64, 112, 112] 128 ReLU 3 [ 1, 64, 112, 112] 0 MaxPool2d 4 [ 1, 64, 56, 56] 0 Conv2d 5 [ 1, 64, 56, 56] 36,864 BatchNorm2d 6 [ 1, 64, 56, 56] 128 ReLU 7 [ 1, 64, 56, 56] 0 Conv2d 8 [ 1, 64, 56, 56] 36,864 BatchNorm2d 9 [ 1, 64, 56, 56] 128 ReLU 10 [ 1, 64, 56, 56] 0 BasicBlock 11 [ 1, 64, 56, 56] 0 Conv2d 12 [ 1, 64, 56, 56] 36,864 BatchNorm2d 13 [ 1, 64, 56, 56] 128 ReLU 14 [ 1, 64, 56, 56] 0 Conv2d 15 [ 1, 64, 56, 56] 36,864 BatchNorm2d 16 [ 1, 64, 56, 56] 128 ReLU 17 [ 1, 64, 56, 56] 0 BasicBlock 18 [ 1, 64, 56, 56] 0 Conv2d 19 [ 1, 128, 28, 28] 73,728 BatchNorm2d 20 [ 1, 128, 28, 28] 256 ReLU 21 [ 1, 128, 28, 28] 0 Conv2d 22 [ 1, 128, 28, 28] 147,456 BatchNorm2d 23 [ 1, 128, 28, 28] 256 Conv2d 24 [ 1, 128, 28, 28] 8,192 BatchNorm2d 25 [ 1, 128, 28, 28] 256 ReLU 26 [ 1, 128, 28, 28] 0 BasicBlock 27 [ 1, 128, 28, 28] 0 Conv2d 28 [ 1, 128, 28, 28] 147,456 BatchNorm2d 29 [ 1, 128, 28, 28] 256 ReLU 30 [ 1, 128, 28, 28] 0 Conv2d 31 [ 1, 128, 28, 28] 147,456 BatchNorm2d 32 [ 1, 128, 28, 28] 256 ReLU 33 [ 1, 128, 28, 28] 0 BasicBlock 34 [ 1, 128, 28, 28] 0 Conv2d 35 [ 1, 256, 14, 14] 294,912 BatchNorm2d 36 [ 1, 256, 14, 14] 512 ReLU 37 [ 1, 256, 14, 14] 0 Conv2d 38 [ 1, 256, 14, 14] 589,824 BatchNorm2d 39 [ 1, 256, 14, 14] 512 Conv2d 40 [ 1, 256, 14, 14] 32,768 BatchNorm2d 41 [ 1, 256, 14, 14] 512 ReLU 42 [ 1, 256, 14, 14] 0 BasicBlock 43 [ 1, 256, 14, 14] 0 Conv2d 44 [ 1, 256, 14, 14] 589,824 BatchNorm2d 45 [ 1, 256, 14, 14] 512 ReLU 46 [ 1, 256, 14, 14] 0 Conv2d 47 [ 1, 256, 14, 14] 589,824 BatchNorm2d 48 [ 1, 256, 14, 14] 512 ReLU 49 [ 1, 256, 14, 14] 0 BasicBlock 50 [ 1, 256, 14, 14] 0 Conv2d 51 [ 1, 512, 7, 7] 1,179,648 BatchNorm2d 52 [ 1, 512, 7, 7] 1,024 ReLU 53 [ 1, 512, 7, 7] 0 Conv2d 54 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 55 [ 1, 512, 7, 7] 1,024 Conv2d 56 [ 1, 512, 7, 7] 131,072 BatchNorm2d 57 [ 1, 512, 7, 7] 1,024 ReLU 58 [ 1, 512, 7, 7] 0 BasicBlock 59 [ 1, 512, 7, 7] 0 Conv2d 60 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 61 [ 1, 512, 7, 7] 1,024 ReLU 62 [ 1, 512, 7, 7] 0 Conv2d 63 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 64 [ 1, 512, 7, 7] 1,024 ReLU 65 [ 1, 512, 7, 7] 0 BasicBlock 66 [ 1, 512, 7, 7] 0 AdaptiveAvgPool2d 67 [ 1, 512, 1, 1] 0 Linear 68 [ 1, 1000] 513,000 Total params: 11,689,512 Trainable params: 11,689,512 Non trainable params: 0 Input size (MB): 0.57 Forward/backward pass size (MB): 62.79 Params size (MB): 44.59 Estimated Total Size (MB): 107.96 out/dog.jpg tensor(2.6400) tensor( 2.1008) idx:258, name:Samoyed, Samoyede ``` 可以看到模型有 `11,689,512`的参数， 即 `11MiB`左右， 这个大小也就几乎是实际在 831 上运行的模型的大小了 ## 将模型转换为 V831 能使用的模型文件 转换过程如下： ### 使用 Pytorch 将模型导出为 `onnx`模型， 得到`onnx`文件 ```python def torch_to_onnx(net, input_shape, out_name \"out/model.onnx\", input_names [\"input0\"], output_names [\"output0\"], device \"cpu\"): batch_size 1 if len(input_shape) 3: x torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype torch.float32, requires_grad True).to(device) elif len(input_shape) 1: x torch.randn(batch_size, input_shape[0], dtype torch.float32, requires_grad False).to(device) else: raise Exception(\"not support input shape\") print(\"input shape:\", x.shape) # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params True) torch.onnx.export(net, x, out_name, export_params True, input_names input_names, output_names output_names) onnx_out \"out/resnet_1000.onnx\" ncnn_out_param \"out/resnet_1000.param\" ncnn_out_bin \"out/resnet_1000.bin\" input_img filename torch_to_onnx(model, input_shape, onnx_out, device \"cuda:0\") ``` 如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字 ## 使用 `onnx2ncnn` 工具将`onnx`转成`ncnn`模型，得到一个`.param`文件和一个`.bin`文件 >! 这一步可以跳过。 > 按照[ncnn项目](https://github.com/Tencent/ncnn)的编译说明编译，在`build/tools/onnx`目录下得到`onnx2ncnn`可执行文件 ```python def onnx_to_ncnn(input_shape, onnx \"out/model.onnx\", ncnn_param \"out/conv0.param\", ncnn_bin \"out/conv0.bin\"): import os # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir cmd f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\" os.system(cmd) with open(ncnn_param) as f: content f.read().split(\"\\n\") if len(input_shape) 1: content[2] + \" 0 {}\".format(input_shape[0]) else: content[2] + \" 0 {} 1 {} 2 {}\".format(input_shape[2], input_shape[1], input_shape[0]) content \"\\n\".join(content) with open(ncnn_param, \"w\") as f: f.write(content) onnx_to_ncnn(input_shape, onnx onnx_out, ncnn_param ncnn_out_param, ncnn_bin ncnn_out_bin) ``` ## 使用全志提供的`awnn`工具将`ncnn`模型进行量化到`int8`模型 在 [MaixHub](https://maixhub.com/) 点击`工具箱 >模型转换 >v831`进入模型转换页面， 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做） 在转换页面有更多的转换说明，可以获得更多详细的转换说明 这里有几组参数： * 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值 mean ) / std`, `awnn`对输入的处理是 `(输入值 mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给`awnn`量化工具经过` (输入值 mean ) * norm` 计算后的值范围一致既可。 比如 这里打印了实际数据的输入范围是`[ 2.1008, 2.6400]`， 是代码中`preprocess` 对象处理后得到的，即`x (x mean) / std` > `(0 0.485)/0.229 2.1179`, 到`awnn`就是`x (x mean_2*255) * (1 / std * 255)` 即 `mean2 mean * 255`, `norm 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ ncnn produce wrong result#pre process)。 所以我们这里可以设置 均值为 `0.485 * 255 123.675`， 设置 归一化因子为`1/ (0.229 * 255) 0.017125`， 另外两个通道同理，但是目前 awnn 只能支持三个通道值一样。。。所以填`123.675, 123.675, 123.675`，`0.017125, 0.017125, 0.017125` 即可，因为这里用了`pytorch hub`的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下 * 图片输入层尺寸（问不是图片怎么办？貌似 awnn 暂时只考虑到了图片。。） * RGB 格式： 如果训练输入的图片是 RGB 就选 RGB * 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手 自己写的其它模型转换如果失败，多半是啥算子不支持，需要在 使用说明里面看支持的 算子，比如之前的版本view、 flatten、reshape 都不支持所以写模型要相当小心， 现在的版本会支持 flatten reshape 等 CPU 算子 如果不出意外， 终于得到了量化好的 awnn 能使用的模型， `*.param` 和 `*.bin` ## 使用模型，在v831上推理 可以使用 python 或者 C 写代码，以下两种方式 ### MaixPy3 python 请看[MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/) 不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了： ``` pip install upgrade maixpy3 ``` 然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py label 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py ```python from maix import nn from PIL import Image, ImageDraw from maix import camera, display test_jpg \"/root/test_input/input.jpg\" model { \"param\": \"/root/models/resnet_awnn.param\", \"bin\": \"/root/models/resnet_awnn.bin\" } camera.config(size (224, 224)) options { \"model_type\": \"awnn\", \"inputs\": { \"input0\": (224, 224, 3) }, \"outputs\": { \"output0\": (1, 1, 1000) }, \"first_layer_conv_no_pad\": False, \"mean\": [127.5, 127.5, 127.5], \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196], } print(\" load model:\", model) m nn.load(model, opt options) print(\" load ok\") print(\" read image\") img Image.open(test_jpg) print(\" read image ok\") print(\" forward model with image as input\") out m.forward(img, quantize True) print(\" read image ok\") print(\" out:\", out.shape) out nn.F.softmax(out) print(out.max(), out.argmax()) from classes_label import labels while 1: img camera.capture() if not img: time.sleep(0.02) continue out m.forward(img, quantize True) out nn.F.softmax(out) msg \"{:.2f}: {}\".format(out.max(), labels[out.argmax()]) print(msg) draw ImageDraw.Draw(img) draw.text((0, 0), msg, fill (255, 0, 0)) display.show(img) ``` ### C语言 SDK， libmaix 访问这里，按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet 上传编译成功后`dist`目录下的所有内容到 `v831`, 然后执行`./start_app.sh`即可 ## 参考 * [在V831上（awnn）跑 pytorch resnet18 模型](https://neucrack.com/p/358)"},"/ai/en/no_translate.html":{"title":"no translation","content":" title: no translation class: md_page <div id \"visit_from\"></div> <div id \"no_translate_hint\">This page not translated yet</div> <div> <span id \"visit_hint\">Please visit</span> <a id \"translate_src\"></a> </div> <div> <script> function getQueryVariable(variable) { var query window.location.search.substring(1); var vars query.split(\"&\"); for (var i 0;i<vars.length;i++) { var pair vars[i].split(\" \"); if(pair[0] variable){return pair[1];} } return(false); } var ref getQueryVariable(\"ref\"); var from getQueryVariable(\"from\"); var link document.getElementById(\"translate_src\"); var fromDis document.getElementById(\"visit_from\"); link.href ref; link.text ref; fromDis.innerHTML from; </script> </div>"}}