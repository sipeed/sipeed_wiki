{"/ai/zh/maixhub/train_best.html":{"title":"MaixHub 训练调优方法","content":" title: MaixHub 训练调优方法 在使用 MaixHub 训练模型时，可能会遇到识别效果不太好或者模型实际运行速度慢等情况，此处提供一些常见调优方法。 欢迎修改和补充 ## 识别效果优化 * 尽量多采集实际使用场景的图片，覆盖更多使用场景有利于提高最终识别率。 * 图片数量尽量不要太小，虽然平台限制最小数量为 20 张图才可以训练， 但要达到比较好的效果，显然一个分类 200 张都不算多，不要一直在 30 张训练图片上纠结为什么训练效果不好。。。 * 修改迭代次数，在发现`val_acc`仍然有上升趋势的情况下可以考虑适当增大迭代次数，但是迭代次数越大，训练时间越长，所以要根据实际情况权衡。 * 修改学习率和批数量大小，学习率不宜太大，否则会导致梯度爆炸出现`loss 为 0`或者`loss 为 inf`这样的错误，批数量大小不宜太小，否则会导致训练速度过慢，一般来说，学习率在 0.0001~0.001 之间，批数量大小在 16~64 之间都是比较合适的。另外需要注意批数量越大， 学习率就可以设置得稍微大一点。 * 每个标签的数据量都尽量多，而不是一个标签只有 20张，另一个500张图， 可以把训练参数处的“数据均衡“开关打开 * 默认分辨率但是 224x224， 是因为预训练模型是在 224x224 下训练的，当然也有其它分辨率的，比如 128x128，具体发现不支持的分辨率预训练模型，在训练日志中会打印警告信息。 * 为了让验证集的精确度的可信度更高（也就是在实际开发板上跑的精确度更接近训练时在验证集上的精确度），验证集的数据和实际应用的场景数据一致。比如训练集是在网上找了很多图片，那这些图片可能和实际开发板的摄像头拍出来的图有差距，可以往验证集上传一些实际设备拍的图来验证训练的模型效果。 这样我们就能在训练的时候根据验证集精确度（val_acc）来判断模型训练效果如何了，如果发现验证集精确度很低，那么就可以考虑增加训练集复杂度和数量，或者训练集用设备拍摄来训练。 * 对于检测训练项目，如果检测训练的物体很准，但是容易误识别到其它物体，可以在数据里面拍点其它的物体当背景；或者拍摄一些没有目标的图片，不添加任何标注也可以，然后在训练的时候勾选**允许负样本**来使能没有标注的图片。 * 检测任务可以同时检测到多个目标，如果你觉得识别类别不准，也有另外一种方式，先只检测模型检测到物体（一个类别），然后裁切出图片中的目标物体上传到分类任务，用分类任务来分辨类别。不过这样就要跑两个模型，需要写代码裁切图片（在板子跑就好了），以及需要考虑内存是否足够 ## 在开发板上运行速度慢 * 减小输入分辨率，比如在分类任务中可以使用`96x96`的小图来训练。 * 裁切一部分图像进行识别，识别时不对整张图片进行识别，可以裁切出部分图像进行识别。 * 选择更小的网络，比如分类选择`mobilenetV1 0.25`是`219KiB`, 而`mobilenetV1 0.75`则是`1.85MiB`，网络参数量减少了很多，不过相应地，模型识别精度也会降低。 ## 更快地标注数据 * 可以导入本地已经标注好了的数据到 MaixHub。 * MaixHub 支持视频辅助自动标注，只需拍摄视频上传的时候使用辅助标注功能即可，对于画面中只有单个物体的场景标注十分有用。 * 可以使用已经训练好的模型来辅助标注，虽然现在 MaixHub 不支持用训练好的模型来标注，但是你可以下载训练好模型到板子运行，写代码将识别到的物体坐标保存为`VOC`标注格式，就得到了新的标注数据，虽然可能会因为模型训练效果不够好标得不够准确，但是经过简单的手工筛选调整后，标注数据就可以新的训练了，如此反复，就会得到很多数据啦。 * MaixHub 未来可能会上线更好用的辅助标注工具哦~ 有建议欢迎通过 MaixHub 的反馈功能告诉我们哦~"},"/ai/zh/maixhub/index.html":{"title":"MaixHub 简介","content":" title: MaixHub 简介 [MaixHub](https://maixhub.com) 是 Sipeed 发布的集 AI 模型服务和社区沟通等功能的平台，主要提供了以下功能： * 模型库，直接下载模型到设备即可运行使用，以及分享自己的模型到模型库。 * 在线训练，无需编程基础和 AI 训练经验也能轻松训练模型，方便入门 AI 学习和加速 AI 应用开发。 * 项目分享，分享自己制作的项目和作品，在社区中交流学习或者寻找灵感。 更多功能更多内容，请访问[MaixHub](https://maixhub.com)。"},"/ai/zh/maixhub/faq.html":{"title":"MaixHub FAQ 常见问题汇总","content":" title: MaixHub FAQ 常见问题汇总 ## 基本使用 ### 已经用手机注册了账号，又用微信注册了账号，怎么合并账号？ 不支持合并账号，可以登录不常用的一个账号，在个人中心注销账号（解绑唯一的登录方式），然后登录另一个账号重新绑定。 比如 微信账号是刚注册的，里面没有什么内容，扫码登录微信账号，然后到个人中心解绑微信以注销账号，然后登录手机账号，个人中心绑定微信即可。 ## 在线训练 ### 训练模型识别效果不好，怎么优化？ 参考 [MaixHub 模型训练调优](./train_best.html)"},"/ai/zh/index.html":{"title":"AI 指南","content":" title: AI 指南 date: 2022 09 15 ## 关于本文档 `Sipeed` 推出了一系列 `AI` 开发板，包括: * `Maix I(M1)` 系列的 `MaixBit`，`Maixduino` `M1s Dock`等带 `AI` 硬件加速的单片机开发板。 * `Maix II(M2)` 系列的 `M2 Dock` `MaixSense`等带 `AI`硬件加速的高性价比`SOC`开发板。 * `Maix III(M3)`系列的`M3 AX Pi`等高性能`SOC`开发板。 为了普及 `AI` 在端侧设备(/边缘设备)上的应用，`Sipeed` 开发了易上手的 `MaixPy`和`MaixPy3` SDK, 以及上线了无需编程基础和 AI 训练经验也能轻松训练模型的 [MaixHub](https://maixhub.com/) 平台。 在此基础上，本文档致力于为开发者提供一份 `AI` 开发指南，目的是为了让新手快速上手 `AI` 应用， 或者已经掌握 `AI`相关知识的开发者将成果快速应用到设备或者产品上。 内容包括不限于: * AI 的基础知识 * 模型训练指南 * AI 教程推荐 * AI 有趣项目推荐 * 常用工具使用 * 边缘设备部署指南 ## 参与贡献 文档内容会持续更新，欢迎大家参与内容编写，参与方式： * 可以直接点击文档右上角的 `编辑本页` 按钮跳转到`GitHub`上进行编辑并提交 `PR`（具体方法可以看[贡献文档](/share_docs/zh/)或者搜索引擎搜`GitHub 如何 提交 PR`）。 * 也可以直接发送更改建议或者投稿到`support@sipeed.com`, 标题请以`[WiKi 投稿]`开头， 正文需要注明作者，以及需要修改的页面位置和内容，方便我们快速将您的内容更新到文档。"},"/ai/zh/nn_models/mobilenet.html":{"title":"MobileNet 物体分类模型","content":" title: MobileNet 物体分类模型 Mobilenet 网络是由 Google 针对手机和嵌入式场景提出的一种轻量级的深度神经网络，其主要特点是使用深度可分离卷积（depthwise separable convolution）来替代普通卷积，从而减少计算量，提高网络的计算效率。该网络在 ImageNet 数据集上的分类精度达到了 70.8%，在损失了不多精度的情况下，极大地减少了计算量，使神经网络模型在普通单片机上流畅运行也成为了可能！ ## 目标 按顺序输入若干张图像，输出每张图像属于哪个分类，以及该分类的置信度。 ## 原理 对于不太对详细的原理感兴趣的同学，可以简单理解： * 卷积计算具有提取特征的作用，比如一幅图像经过一次卷积计算后的结果，会将图像的轮廓提取出来，比如经典的索贝尔边缘检测，比如下图右边是图像输入，左边是经过一次卷积计算后的结果： ![](../../assets/sobel_edge2.jpg) 可以说，通过一次卷积计算，图像的轮廓就被提取出来了，这就是卷积计算的特征提取作用。 如果经过多次卷积计算，不同图像的特征就会被提取出来。 你可以在[tensorspace.org](https://tensorspace.org/html/playground/mobilenetv1.html) 可视化地看到这个过程。 * 经过整个由无数个卷积计算和其它计算组成的网络计算，最后输出一个只有 1000 个像素点的图像，不同分类的图输入，在输出层的 1000 个像素点中，其中一个像素点的值会较大，这个像素点对应的分类就是网络的输出结果。比如一只熊猫图像输入，如果输出层的第 388 个像素点的值较大，并且值为 0.8，我们就通过这个网络识别到了这张图像是一只熊猫，置信度为 0.8。 * 至于 Mobilenet 网络宣称使用了深度可分离卷积（depthwise separable convolution）来替代普通卷积，从而减少计算量，提高网络的计算效率，你可以理解成相比于之前的卷积网络，仍然是卷积计算，只是和在图像上使用的普通卷积计算的方式不同，这样就可以减少计算次数，提高网络的计算效率，具体方式怎么不同请自行进一步阅读相关文章。 * 至于训练，和在基础知识中说到的一样，通过不停向网络输入图像，然后让网络输出正确的分类，然后通过损失函数计算网络输出的分类和正确分类的差距，然后通过反向传播算法，不断调整网络中的参数，使得网络输出的分类和正确分类的差距越来越小，最终网络就能够输出正确的分类。训练实际就是找出适合我们的数据的网络中的参数，比如网络中所有卷积核的值，网络中所有的偏置值等等。 关于 Mobilenet 的具体原理，此处不进行详细介绍，感兴趣的读者可以参考 Mobilenet 论文原文（ [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) ） 和其它三方教程 ## Mobilenet V2 Mobilenet V2 是 Mobilenet 的升级版，它的主要改进点在于优化了网络的结构，使得网络的计算效率更高，同时网络的精度也更高。 ## 试试训练一个模型 在 [MaixHub](https://maixhub.com) 创建一个分类训练项目，然后采集数据，创建一个训练任务，参数选择你有的硬件平台，如果没有开发板可以使用手机，选择 mobilenetv1 或者 mobilenetv2 进行训练即可，训练完成后可以一键部署看看效果。"},"/ai/zh/nn_models/fomo.html":{"title":"FOMO 轻量检测模型","content":" title: FOMO 轻量检测模型 FOMO(Faster Objects, More Objects) 是由 Edgeimpulse 工程师提出的一种轻量级的目标检测模型，其主要特点是模型非常小，计算量也很小，表现出来就是速度非常快，精度也不错。 FOMO 的优点是速度快，缺点是精度不高，但是在一些对精度要求不高的场景下，FOMO 是一个不错的选择。 ## 目标 输入图片，检测目标的位置和大小，并识别出物体的类别，能同时检测多个物体。 ## 原理 原文： [FOMO: Object detection for constrained devices](https://docs.edgeimpulse.com/docs/edge impulse studio/learning blocks/object detection/fomo object detection for constrained devices) 和 YOLO v2 类似，目的都是检测到物体，但是 YOLO v2 的后处理还是比较复杂，有大量的框需要处理，想在没有硬件加速的单片机上运行还是很吃力的。 FOMO 采用更简单的思路来做检测： * 使用一个经典网络作为特征提取器，比如 MobileNet v1， 然后从网络中间截断，得到一个特征图，这个特征图的大小是 n x n x c, n 是特征图的宽高，c 是特征图的通道数。 这里 n 的取值取决于从网络哪里截断，比如我们输入分辨率是 128 x 128, 想要一个 8 x 8 的特征图输出，相当于把图片分辨率降低了 16 倍，从网络找到该层截断得到 8 x 8 的特征图。 * 这个 n x n x c， c 代表了有 c 个分类，每一层用来找一个分类的物体的位置，每层有 n x n 个像素，每个像素代表了在该位置是否有该分类的物体（的置信度） * 遍历这个 n x n x c 的特征图，找到置信度超过设置的阈值的像素坐标，我们就认为这些地方有物体存在，然后按照缩放比例映射到原图，比如只有一个分类即 c 为 1 时，我们要检测一个杯子，得到如下的结果： ![](../../assets/fomo1.jpg) * 在得到了一大堆看起来有效的坐标点后，我们认为这些地方有物体存在，但是有一大堆点，我们可以简单地将挨着的点合并成一个框，这样就得到了一个大框 ![](../../assets/fomo_result.jpg) * 因为这个框有时候可能不是很准，Edgeimpulse 官方的做法是值采用这个框的中心点，这样这个点在物体上的概率比较大，在 MaixHub 的实现中，直接给出了框让你可以选择直接使用框，或者如果想只使用中心点，可以自己修改一下代码即可。 实际上，上面的处理都已经在[代码](https://github.com/sipeed/TinyMaix/tree/main/examples/maixhub_detection_fomo)库中封装好了，实际使用时，只需要训练模型，结合代码即可运行。 ## 试试训练一个模型 在 [MaixHub](https://maixhub.com) 创建一个检测训练项目，然后采集数据并标注（可以在线标注），创建一个训练任务，参数选择 TinyMaix 平台，选择 fomo , 主干网络根据实际情况选择，比如`mobilenetv1_0.25_8`这样的名字代表使用了`mobilenetv1`网络， alpha 为 0.25, 这个数值越小网络越小，准确率越低，最后面的 8 则代表了输出分辨率是输入分辨率的 1/8 ，比如 输入 128x128，输出就是 16x16，输出的分辨率越大越适合检测小一点的物体，根据你的单片机性能和被检测的物体大小来选择。 然后进行训练即可，训练完成后得到模型，按照 [代码](https://github.com/sipeed/TinyMaix/tree/main/examples/maixhub_detection_fomo) 中的使用说明，将模型文件放到代码中，然后编译运行即可，可以现在 Linux 下测试，再搬运到单片机上运行。"},"/ai/zh/nn_models/yolov2.html":{"title":"YOLOv2 目标检测模型","content":" title: YOLOv2 目标检测模型 YOLO(You Only Look Once) v2 成功让目标检测达到实时的同时，有着较高检测准确率，正如其名字一样，你只需要看一眼，就能知道结果。 YOLO v2 因其检测效率高，可以在性能不是很强的边缘设备运行，不过也有缺点，就是对于小物体的检测能力不够。 ## 目标 输入一张图片，输出图片中的目标类别和位置，能同时检测多个物体。 ## 原理 论文原文：[YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) YOLOV2 主干网络也是卷积网络，输出层则使用了 S x S x B(4 + 1 + C) 这样一个三维结构，其中 S 是网格的大小，B 是每个网格预测的边界框数量(Anchor 数量）， 4 个预测框的坐标值（xmin,ymin,xmax,ymax）， 1 个置信度，C 是类别数量。 也就是说，在 S x S 个网格中，每个网格预测 B 个框，每个框有 5 个参数，分别是 x, y, w, h, p，其中 x, y 是框的中心点相对于网格左上角的偏移，w, h 是框的宽高，p 是框的置信度，最后 C 个参数分别是对应类别的概率。 对于这 B 个框，预先我们手动确定了 B 个 anchor（预选框），每个 anchor 有 w, h 两个参数，这里 B 个预测框的 w, h 是相对于这 B 个 anchor 的缩放系数，这样预测框的 w, h 就可以是任意值了。 > 这 B 个 Anchor 是使用 K Means 算法对训练的数据所有标注框的宽高聚类得到的，聚类的目标是让每个 anchor 的宽高比接近真实框的宽高比，这样预测框的宽高比就会更接近真实框的宽高比，从而提高检测精度。所以代码里面会有一个 anchors 参数，这个参数就是我们训练的时候统计训练数据得到的 B 个 anchor，实际在跑模型时一定要保证这个参数和训练时的一致。 在获得所有预测框后，还需要对预测框进行过滤，去除一些置信度低的框，即上面说的 p，这也就是我们在代码里看到的阈值（threshold）; 以及需要进行 nms（non maximum suppression/非极大值抑制） 计算去除预测出相同分类并且有太多重叠部分的框，保留概率大的一个即可，所以代码里面我们看到有一个 nms_threshold 参数，这个参数就是用来过滤 IOU（两个框的相交区域（交集）面积/并集面积）大于这个阈值的框的。 看起来有点复杂的样子，要想详细学习可以参考更多三方教程和官方论文，实际我们训练的 YOLO v2 模型纯粹的模型到输出 S x S x B(4 + 1 + C) 这样一个三维张量就行了，然后代码库提供了一个解码的函数，调用函数输入这个三维张量就可以得到合法的预测框输出了，就算你对原理不了解，也不妨碍你使用这个模型。 ## YOLO v3 YOLO v3 相比于 YOLO v2， 做了多输出，更利于检测小物体，不过因为多输出，在计算量上也会大很多，帧率自然也比 YOLO v2 更低。 ## 试试训练一个模型 在 [MaixHub](https://maixhub.com) 创建一个检测训练项目，然后采集数据并标注（可以在线标注），创建一个训练任务，参数选择你有的硬件平台，如果没有开发板可以使用手机，选择 yolov2 进行训练即可，训练完成后可以一键部署看看效果。"},"/ai/zh/basic/dnn_basic.html":{"title":"深度神经网络（DNN）基础知识","content":" title: 深度神经网络（DNN）基础知识 keywords: MaixHub, AIOT, 边缘计算, 深度神经网络, 机器学习基础, 神经网络入门 desc: 深度神经网络（DNN）基础知识 update: date: 2020.11.17 version: v1 author: neucrack content: 初始版本，根据 MaixPy 的需要介绍深度神经网络基本概念，初稿 update_open: false 本文用比较通俗的方式简单介绍深度神经网络相关只是， 主要目的是让没有接触过的同学从零开始学习机器学习中深度神经网络相关知识， 本篇中不涉及深入知识介绍。 如果有什么地方有误或者需要补充，欢迎提交修改。 ## 如何解决一个问题 引出机器解决问题 一个问题， 通常分为 **输入** 和 **输出（结果）** 比如： 坐标系中的一条直线如下， 上面的数据点值是已知的: ![y kx+b](../../assets/dnn/ykxb.jpg) 现在提问，假如数据点规律不变， 输入一个 x 坐标 20, y 的值是多少？ 按照大家的知识，都知道这是一个一元一次方程(`y kx + b`能解决的， 带入两个点的值，算出方程为`y 3x + 10`, 那么当 `x 20`, `y` 的值为`70`， 于是输入是`20`, 输出是`70`。 这里就是 输入(`20`) + 算法（一元一次方程） 输出（`70`）， 这就是我们在解决一个问题时的基本方法， 所以关键就是找到这个符合这条线段上数据点规律的一个算法。 人类很强大，会从这些数据中归纳总结学习，最终得到了这个算法（方程），然后其他的人直接使用这个算法就可以快速用于解决同类问题，那么，有没有一种方法， 让机器自动去寻找这个算法呢？ ## 如何让机器总结出算法 要让机器自动总结出算法，即机器学习（ML，Machine Learning）， 我们先看看，人类是如何得到这个算法（方程）的。 * 步骤1： 首先，有大量数据点，然后人类根据这些数据点发现了直线都符合`y kx + b`这个适应所有直线的算法, 但是发现，这里面有两个未知数`k`和`b`, 这就是适应任何直线的参数 * 步骤2： 然后具体的是什么样的直线，因为方程有两个未知数，即参数，将实际的两个数据点带入这个方程，得到了`k 3`和`b 10` * 步骤3： 然后我们用在步骤2中没有用到的在线上的数据点，去试试这个算法（方程）是否正确，最终发现都验证正确 * 步骤4： 然后要通过`x`的值知道其它的点的`y`的值，只需要代入`y 3x + 10` 即可 那么，机器学习是不是也可以利用这个步骤来做呢？ * 我们认为地设计一个算法结构， 加入我们碰巧直接设计成了`y kx + b`， 我们给具体的直线留下了两个参数，我们暂且称呼这个结构叫 **模型结构**，因为有未知参数，我们称之为未训练的模型结构。其中`x`称为**输入**, `y`称为**输出** * 现在，我们将我们这条直线的的几个点代入到这个方程， 我们称这个过程为 **训练**，得到`y 3x + 10` 这个算法， 已经没有未知参数了， 我们现在称它为**模型** 或者 训练好的模型，其中`k b`是模型内的参数，`y kx + b`是这个模型的结构。 而带入训练的数据点，就叫做**训练数据**，它们的统称就叫**训练数据集** * 然后，我们使用几个在 训练 过程中没有用到的在线段上的数据点作为输入，代入这个模型进行运算，得到结果，比如 `x 10`, 得到`y 40`, 然后对比输出值是否与预期相符，这里我们发现`x 10, y 40` 确实是在图中这条直线上的， 并且训练时没有使用这个点，说明我们得到的模型在此次核验中通过，这个过程叫 **验证**， `x 10, y 40` 这个数据叫验证数据。 如果我们用多组数据去验证这个模型， 这些数据的统称就叫**验证数据集** * 现在， 我们获得了一个**模型**，并且用**验证数据集**对这个模型进行了验证，貌似也是很准确了，那我们就可以假设这个模型基本满足了我们以后有一个`x`， 要求着图中线上任意一点的`y`值，都可以输入`x`给出这条直线上对应点的`y`坐标。 这个过程我们其实是在**使用模型**了，这个过程称之为**推理** 其实这就算是机器学习了， 我们人类需要的事就是设计`y kx + b`这个结构，以及给出**训练数据集**和**验证数据集**，经过**训练**和**验证**得到一个我们认为可用的模型，然后使用`输入 + 模型`就可以得到认为的正确`输出（结果）`了。 ## 什么是深度神经网络？ 深度神经网络（DNN）是机器学习（ML）领域中的一种技术。 前面说了一个比较简单的例子， 根据一条直线数据来预测直线上的任何一个点， `y kx + b`这个结构是人为设计的， 很简单，当用于复杂的数据，发现它就不适用了，比如“这张图片里面是球还是玩具” ![小球](../../assets/dnn/ball.jpg) ![玩具](../../assets/dnn/toy.jpg) 前面为了模型能够存下一条直线的信息， 用了结构`y kx + b`，直线的特征都存在模型里面了。 现在用来存一张图的特征，光是`y kx + b`这个线性结构， 以及`k 和 b`两个参数显然无法满足了， 需要设计一个更好的结构， 这时 **神经网络** 就出现了， 一种网状结构，能更好地记住图片的特征信息， 而这个网状结构又是多层的，也就是有深度的，所以称之为深度神经网络（DNN， deep neural network）， 所以说 DNN 是一种网络结构，是为了实现机器学习的一种手段。 每一层由多个节点组成， 如下图， 一个 DNN 包含了 **输入层**， **隐藏层**， **输出层**， 这里隐藏层由三层组成（`A[1], A[2], A[3]`层），但是统称隐藏层： ![深度神经网络](../../assets/dnn/dnn.jpg) **输入层**： 图中就是一个深度神经网络结构， `x` 是输入， 比如`x`这里可以是图片, 输入有多个节点，每个节点可以是一个像素点值， 这里输入层画了 7 个节点， 假如我们有一张图片是 `10 x 10`的分辨率，则输入层共需要 `100` 个节点。 这里输入层是一个一维结构，实际情况可能有多维结构， 比如输入如果是一张灰度图片，分辨率`3x3`，这其实是一个二维结构，即两行两列的矩阵（关于矩阵的概念请自行学习，或者暂且理解成二维数组），比如： ``` [[109 138 110] [220 37 166] [32 243 67] ] ``` 每个像素点的值取值范围∈[0, 255]，然后我们将其平铺后变成共 9 个数据的一维数组给输入层 ``` [109 138 110 220 37 166 32 243 67] ``` > 另外， 一般也会将输入层的值归一化到范围`[0, 1]` 如果是一张彩色图片，那就是三维，即`高、宽、颜色通道`，颜色通道比如`RGB`三个颜色通道，即，输入有形状（包含了维度和每个维度的数据数量），比如上面的一维输入形状为`(9)`，其它图像通常以`(高，宽，通道数)`来表示形状，比如`(10, 10, 3)`表示分辨率`10 x 10`， 并且有三个颜色通道， 比如`RGB`。 这里为了入门好理解，原理只介绍一维的情况 **输出层**： `y` 是输出，这里输出有两个值，你可以理解成就是 Python 中的两个浮点值的 `list` `[Y1, Y2]`， `Y1`是`是小球的概率`，值∈[0, 1], `Y2`是`是玩具的概率`。 所以最终我们使用这个模型， 就是给它一张图片， 机器按照这个模型规定的结构和算法进行计算后得到一个 `list`， 我们根据这个输出的值就知道图中是什么东西了。 **隐藏层**: 连接输入层和输出层的隐藏层，以及中间的连接，负责了将输入数据推算成合理的输出值。 ## 中间休息，总结 到现在为止， 你知道， **模型**是什么：就是一组数据结构，保存了一个网络的形状，以及里面的参数， 通常，这个模型的数据可以被保存成文件，比如`.h5 .tflite .kmodel`等文件，都是用来阐述这个模型的形状结构和参数，只不过是不同软件使用。 人们只需要设计模型结构以及参数，用来解决一类问题，比如常见的物体分类， 比如就是上面说的区分一张图里面是小球还是玩具。 这个模型里面有很多参数，具体在需要识别物体的时候，使用已知分类的数据集让机器自动训练得出一套合适的模型参数。 然后我们就可以输入数据，让通过模型推理出来输入的数据时什么类别了。 所以， 如果我们不需要训练模型，直接使用别人训练好的模型，只需要： * 确认需求，找到现成的模型，因为模型已经是训练好的了，输入和输出的形状的含义都已经定了 * 确认模型的输入形状，比如模型输入分辨率`10x10`的彩图，则使用时需要将符合要求的图片传个输入层 * 确认输出层的含义，比如前面说的识别小球和玩具，最后输出是分别代表是该物体的概率的 list， 比如 `[0.9, 0.1]`, 第一个值代表是小球的概率，那我们就知道这张图里有 90% 的概率是小球， 只有 10% 的概率是玩具 * 将模型放到推理程序进行运行。 具体用什么程序先不着急，会在下一章介绍 到这里，应该大致上明白了以下东西： * 什么是机器学习 * 什么是深度神经网络（简单概念） * 模型是什么 * 什么是输入层，输出层，在上面举例的分类应用中分别表示什么含义，层形状是什么样的 * 到此为止，我可能还不知道什么是模型训练 * 如果我需要一个模型，我知道如何确认需求 所以，**如果你只希望能够使用模型，不需要训练，到此即可**， 也不需要知道模型有些什么具体的东西，你就把它当成一个**黑盒工具箱**使用即可。 如果想要更深的了解，请继续看下面的内容。 ## 继续：深度神经网络（续 既然设计了多层设计，那我们继续深入： **数据流** ， **权重**， **偏置**: 在模型进行推理时，数据从输入层流动到输出层，就是这些网状箭头的方向（第三节网状图），每个箭头前一层到后一层的计算可以用一个熟悉的公式:`y wx + b`, 称`w`为**权重**(weight), `b`为**偏置**（bias）, 注意是每个箭头都有一个单独的`w, b`, 也就是说后一层节点的值等于前一层节点经过这个公式计算过后的值， 后一层的节点有多个前一层节点指向，那就等于所有前一层节点的值经过这个公式计算后的值的和。 就这样经历了无数次运算后，结果终于在输出层以一个值的形式出现了，整个推理也就完成了 **激活函数**： 上面的模型虽然可以通过输入得到结果，但是会发现，所有层计算都是线性函数，那么不管有多少层，整体其实还是一个线性函数，即`y0 w1x + b1` + `y w2y0 + b2` > `y w2(w1x + b1) + b2` > `y w2w1x + w2b1 + b2`, 其实还是一个线性函数，那么多层的意义就没有了，于是我们需要在中间加入非线性函数，让网络内部更加复杂一点， 于是就在每个节点上做手脚， 在每个节点输出数据前，先对其用一个非线性函数运算，比如`sigmod`或者`relu`函数，别听到名字害怕，其实很简单，看下图, 总之就是 x 和 y 不成线性关系： ![sigmod](../../assets/dnn/sigmod.jpg) ![relu](../../assets/dnn/relu.jpg) 即到现在为止， 除了输入层，所有节点输出的值都需要经过`Sigmod(∑(Wn * x + Bn))`, 输出一个浮点数值 **softmax**: 输出层在最后输出的时候，因为前面的运算，值的范围不是很统一，虽然我们可以通过比大小，值最大的即认为是答案，但是为了统一而且可以直观地知道每个类别的可能性（另外也为了训练的准确性，这里不讲），正如前面讲到，我们最后输出的一个类别的概率，取值范围∈[0, 1]， 且所有输出的值和为`1`，所以在输出层后面对输出层的所有值进行处理，公式为 ![softmax](../../assets/dnn/softmax.jpg) 到此，从输入到输出的推理过程就结束了 ## 深度神经网络训练 前面简单介绍了深度神经网络的结构组成， 以及从输入层到输出层的正向过程，在我们使用模型时，就是这个正向过程。 那么，模型定好了，里面的参数（比如`w,b`）都是随机的值，怎么让它自动训练得到模型中参数的值呢？ 在前面我们讲到， 使用一些我们已知结果的数据输入，来得到参数，同样地，这里我们也输入已知结果的数据，得到第一次的输出结果 **判定输出正确性(accuracy)（或者说误差/loss）** 和 **损失函数**： 在输出层得出结果，比如得到了`[0.6, 0.4]` 代表是小球的概率`0.6`, 是玩具的概率`0.4`, 但是因为是已知答案的数据， 实际正确答案是`[1.0, 0.0]`, 这明显不符合要求。 所以我们得出正确答案和推算的答案的误差为： `[0.4, 0.4]`, 但是发现一个问题就是这个误差值的范围不太好看，要是误差的取值范围∈`[0, ∞]` 就好了。 在高中数学中有个函数`y log10(x)`, 坐标图如下： ![log10](../../assets/dnn/log10x.jpg) 发现`x`取值∈`[0, 1]`时， ` y`的取值刚好∈`[0, ∞]`， 而我们的输出结果也刚好∈`[0, 1]`！ 所以，我们直接这样计算误差： `error log10(输出)`， 也就是输出越接近`1`，误差就越接近 `0`，这种方法称之为`交叉熵损失（CEE, Cross Entropy Error)`， 除了这种方法还有其它的比如均方误差（MSE，Mean Squared Error）等 至此，我们知道了现在结果和实际结果的误差 **误差的反向传播** 和 **参数优化（权重更新）**： 因为模型的参数还不符合我们的预期， 那我们需要对参数进行修正，我们使用反向传播的方式。 前面我们得出了误差， 因为参数不够正确， 我们用这个误差去修改模型中的参数，来达到微调模型内参数的效果。 就好像你在开一个水龙头， 水大了（即误差大了），就把开关拧紧一点，小了就拧松一点，对其做调整。 就像我们正向推算一样，这次换成了反向，从后往前，可以得到在每个节点处的误差值，然后再根据一定的学习率去更新模型内参数。这里暂时就不仔细展开讲了。 总之，经过一轮反向的调整参数之后，得到了新的模型 **衡量模型好坏：训练集误差和验证集误差**： 我们使用训练数据集里面的数据反复去进行正向推理得出误差，然后反向调整这个过程，在使用完训练数据集后，可能会得到误差比较小，但是这只能说明这个模型对这批数据来说比较准确，换一些新的数据可能就不准确了，所以我们要用一些训练集里没有的数据去**验证**模型的效果： 我们使用 **验证数据集** 去正向推算，得到误差，因为验证数据集没有参与训练，也就是说现在模型的参数和验证数据集没有任何关系，我们用这个得到的误差来恒定这个模型的好坏，误差越小则认为效果越好 **多次迭代**： 如果将所有数据集训练完了，发现误差依然很大，那么可以用多次训练的方法来继续训练，即**多次迭代**，每次迭代完成后都用 验证数据集 去验证效果如何， 如果训练集的误差和验证集的误差都足够小，我们就可以暂且认为模型已经有不错的效果了。 **测试集**： 这时，我们就可以用又一批新的数据去测试我们的模型效果如何，因为这是全新的数据，没有参与到训练也没参与到验证（即确定什么时候停止训练），理论上更有公信力。如果测试误差较小，那么训练就算成功了 **优化训练**： 如果最终效果不太好， 有很多地方可以调整， 比如 * 训练迭代的次数，并不是越多越好，过多的在一批数据集上训练可能导致模型只对这批数据有效，泛化能力不够， 也就是**过度拟合** * 每次训练的学习率也可以调整 * 检查数据集，是否有一些影响分类的数据存在 * 优化网络结构，不管是输入输出还是内部结构和参数，根据不同的数据和任务可以有更优的设计，也叫**特征工程** ## 说在最后 到这里，应该大致上明白了以下东西： * 什么是机器学习 * 什么是深度神经网络 * 模型是什么 * 什么是输入层，输出层，在上面举例的分类应用中分别表示什么含义，层形状是什么样的 * 什么是训练，有什么作用 * 数据训练集，验证集，测试集分别是什么，用在什么地方，需要注意什么 * 衡量模型好坏的标准是什么 如果还不明白的，可以再仔细理解一遍，或者查阅相关资料，如果你发现有更好的阐述方法，欢迎按照左边目录的文档贡献方法参与贡献"},"/ai/zh/basic/courses.html":{"title":"机器学习中教程汇总","content":" title: 机器学习中教程汇总 keywords: 机器学习, 机器学习教程 desc: 机器学习中教程汇总 本文主要汇总一些机器学习相关的教程。 欢迎编辑并提交合并请求以补充。 首先，可以先简单阅读下前两篇[什么是 AI？](./what_is_ai.html) 和 [深度学习通俗解释](./dnn_basic.html) 两篇文章，以通俗的理解方法对机器学习有个大概的了解，然后再去学习理论知识。 对于开发者来说，代码绝对是最好的入门接触方式，能更加快速地理解，可以看训练框架的教程先动手浅尝一下后再去学习理论知识也是可以的 * [Tensorflow 的教程](https://www.tensorflow.org/tutorials) * [Pytorch 的教程](https://pytorch.org/tutorials/) 资源包括不限于： * 书籍 * github 仓库 * 网站 * 视频 ## GitHub 开源仓库 * [Mikoto10032/DeepLearning](https://github.com/Mikoto10032/DeepLearning): 深度学习入门教程, 优秀文章, Deep Learning Tutorial * [d2l ai/d2l zh](https://github.com/d2l ai/d2l zh): 《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被60个国家的400所大学用于教学。 * [scutan90/DeepLearning 500 questions](https://github.com/scutan90/DeepLearning 500 questions): 深度学习500问 * [AMAI GmbH/AI Expert Roadmap](https://github.com/AMAI GmbH/AI Expert Roadmap): Roadmap to becoming an Artificial Intelligence Expert in 2022 * [floodsung/Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep Learning Papers Reading Roadmap): Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech! * [mbadry1/DeepLearning.ai Summary](https://github.com/mbadry1/DeepLearning.ai Summary): personal notes and summaries on DeepLearning.ai specialization courses. ## 视频教程 欢迎补充 ## 书籍 欢迎补充"},"/ai/zh/basic/code_frameworks.html":{"title":"机器学习中常见的代码框架和工具包","content":" title: 机器学习中常见的代码框架和工具包 keywords: 机器学习, 代码框架 desc: 机器学习中常见的代码框架和工具包汇总 如今编写机器学习程序，不需要我们一切从头开始了，有很多很好的框架，以及很多好用的工具包 ## Pytorch 官网 [pytorch.org](https://pytorch.org/)，学术界偏爱的框架，基于 Python 语法，简单易上手。 ## Tensorflow 官网 [www.tensorflow.org](https://www.tensorflow.org/)，来自 Google 的机器学习框架，内置了 `keras` 让刚接触 AI 的开发者也能快速入门。 官网还提供了很多模型例子和教程 ## Numpy [Numpy](https://numpy.org/) 是一个数学库，通常矩阵相关的数据表达和计算它都能胜任。 安装 Numpy 包只需要执行`pip install numpy` 即可安装成功。 ## Opencv [Opencv](https://opencv.org/) 提供各种语言的软件包，用来进行图形相关的处理，十分易用。 安装 Python 的 Opencv 包只需要执行`pip install opencv python` 即可安装成功。 ## Matplotlib [Matplotlib](https://matplotlib.org/) 是一个常用的用来可视化数据的库，非常强大。 ## pandas [pandas](https://pandas.pydata.org/) 是一个数据分析和操作工具库。 ## 更多 还有很多工具，欢迎补充"},"/ai/zh/basic/what_is_ai.html":{"title":"什么是人工智能(AI)和机器学习","content":" title: 什么是人工智能(AI)和机器学习 date: 2022 09 15 AI（Artificial Intelligence） 想必大家都听之极多了，有人觉得 AI 要毁灭人类了；有人认为 AI 只是擅长某些特殊场景不必惊慌（比如 alpha Go 打败了人类围棋大师）；但大多数人接触到的 AI 可能更愿意称之为“人工智障”，比如手机里面的助手；对于很多工程师来说， AI 可能更多的是指机器学习，比如图像识别，语音识别，自然语言处理等等。 作为一个开发者， 首先我们需要了解大家经常听到的 `AI`，`机器学习`，`神经网络`等概念以及区别： * AI： 人工智能，是指让机器具有人类智能的能力，比如人类可以看到一张照片，然后判断出这是一只猫，这是一个人，这是一只狗等等，而机器也可以做到这一点，这就是人工智能。 * 机器学习： 一般是指让机器通过大量的数据，然后通过算法，让机器自己学习，比如通过大量的猫的图片，让机器自己学习，然后判断出一张图片是不是猫。 * 神经网络： 一般是指在机器学习中用到的一种数据结构，因为其类似人大脑的神经网络，各个数据节点互相连接互相通信和影响，故称之为神经网络。 * 模型： 指用来承载和表示机器学习过程中的相关参数的数据结构，一般可以保存为一个文件，可以将神经网络的结构和参数都存储在这个数据结构里面，方便用数学或者编程语言将其解析，比如取名叫`.model`格式的文件 所以可以理解为三者是包含关系： 神经网络 ∈ 机器学习 ∈ 人工智能， 模型文件则一般为机器学习中的产物， 另外你可能还听说过“深度神经网络”，其实也是属于神经网络，只不过是网络层数有深度不同一说。 而本文也大多阐述了如何利用各种神经网络模型和机器学习的方法来实现 AI 应用。 ## 机器学习过程简介 这里首先对机器学习的过程做一个通俗的介绍，不涉及数学公式，只是简单的介绍一下机器学习的过程。 ### 训练 以让机器区分猫咪和狗为例： 和教（训练）人类婴儿一样，可以把模型比作婴儿，为了让这个模型能认识猫和狗，我们需要一遍一遍地让它看各种猫猫狗狗，并让它去识别，错了我们就告诉它错了，对了我们就告诉它对了，这样一遍一遍地让它看，让它学习，最终它就能区分猫和狗了。 从这段话我们分析出训练的时候几个关键点： * 模型： 一个工具或者黑盒，给它一个输入，它能给我们一个输出结果。 * 输入： 这里是图像，猫或者狗的图像。 * 输出： 猫或者狗 * 判断错误的方法：也就是它的输出和真实的结果是否一样，这里是靠教学者判断正误的，也就是判断错误的方法是教学者。 * 学习方法：就是当我们告诉模型输出结果是错的时候，它如何去改进。 得到这几个关键点后，就可以很好地理解这个机器学习的过程了： * 定义模型的输入输出， 输入是图像，输出是猫或者狗。 * 为了让这个模型能够有学习的本领，也就是和人一样有足够的脑容量， 我们定义一个属于模型的“脑子”，也就是一个看起来和人脑突触结构类似的神经网络结构： ![神经网络](../../assets/dnn.jpg) 可以看到输入和中间每个节点间都有线连起来，每条线都是一个计算公式，具体是什么样的公式以及具体如何设计一个这样的结构这里先不细究，先有个概念就行。 * 然后就是判断错误和误差的方法，一般在代码中称之为损失函数，也就是模型的输出结果是否正确。 * 然后就是学习方法，比如结果不正确，如何去微调模型内部的参数，让下一次的输出结果更接近正确的结果。 ### 验证 经过很多数据的反复训练后，我们发现好像基本都能识别正确了，但是我们还是担心我们用的图片种类是否不够多，这个模型是否真的能够识别所有的猫和狗或者其它猫和狗，这个时候就需要验证了，验证的方法就是把模型拿出来，给它一些新的数据，即在训练的时候从来没用到过的数据，让它去识别，看看它的识别结果是否正确，如果正确，那我们就认为这个模型泛化效果不错，可以放心的使用这个模型去识别猫猫狗狗了。 一般我们会在训练是一段时间后拿出`验证集`（也就是用来验证的数据集，对应训练的数据叫`训练集`）来测试一下模型的效果，如果效果不错就可以停止训练了，如果效果不好，则需要继续训练或者考虑是不是训练集有问题，或者模型结构、损失函数、学习方法等有问题了。 ### 测试 验证效果的好坏决定了我们合适停止训练，也就是说模型效果如何和`验证集`紧密相关，相当于**验证集也变相地参与了模型的训练过程**， 所以在结束训练后，我们用一个新的数据集`测试集`来测试一下模型的效果，这个数据集是在训练和验证的时候从来没用过的，这样就可以更加客观地评估模型的效果了。 这里共提到了`训练集`，`验证集`，`测试集`，需要注意他们三个数据集的区别！前两者在训练过程参与，后者在训练过程不参与，只是用来评估模型的效果，并且三者互相不重合，防止训练过程中模型只对一小部分数据有效，到了新的场景就无法识别（也就是所谓的`过拟合`）。 ### 总结 这里简单阐述了机器学习的过程的通俗解释，你也可以到 [MaixHub](https://maixhub.com) 注册登录后体验自己体验一遍在线训练过程加深理解，无需懂代码，懂得这里描述的机器学习的过程就可以了，然后再进行进一步学习。"},"/ai/zh/deploy/k210.html":{"title":"部署模型到 Maix-I(M1) K210 系列开发板","content":" title: 部署模型到 Maix I(M1) K210 系列开发板 date: 2022 09 15 <div id \"title_card\"> <div class \"card\"> <img src \"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt \"K210 模型转换和部署\"> <div class \"card_info card_red\"> <div class \"title\">Maix I 系列 K210</div> <div class \"brief\"> <div>高性价比带硬件 AI 加速的单片机</div> <div>1Tops@INT8，有限算子加速</div> </div> </div> </div> </div> <style> #title_card { width:100%; text align:center; margin bottom: 1em; } #title_card img { max height: 20em; } .card_red { background color: #ffcdd2; color: #cf4f5a; } .dark .card_red { background color: #5a0000; color: #ffffffba; } .title { font size: 1.5em; font weight: 800; padding: 0.8em; } </style> > 欢迎修改和补充 一般使用 `tensorflow` 训练出浮点模型， 再使用转换工具将其转换成 `K210` 所支持的 `Kmodel` 模型，然后将模型部署到 `K210` 开发板上。 ## K210 上的 KPU `K210` 上的 AI 硬件加速单元取名为`KPU`，`KPU` 实现了 卷积、批归一化、激活、池化 这 4 种基础操作的硬件加速， 但是它们不能分开单独使用，是一体的加速模块。 所以， 在 KPU 上面推理模型， 以下要求： ### 内存限制 K210 有 6MB 通用 RAM 和 2MB KPU 专用 RAM。模型的输入和输出特征图存储在 2MB KPU RAM 中。权重和其他参数存储在 6MB 通用 RAM 中，在转换模型时，会打印模型使用的内存大小以及临时最大内存使用情况。 ### 哪些算子可以被 KPU 完全加速？ nncase 支持的算子： * nncase v0.2.0 支持的算子： https://github.com/kendryte/nncase/blob/master/docs/tflite_ops.md * nncase v0.1.0 支持的算子： https://github.com/kendryte/nncase/tree/v0.1.0 rc5 下面的约束需要全部满足。 * 特征图尺寸：输入特征图小于等于 320x240 (宽x高) 同时输出特征图大于等于 4x4 (宽x高)，通道数在 1 到 1024。 * Same 对称 paddings (TensorFlow 在 stride 2 同时尺寸为偶数时使用非对称 paddings)。 * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，stride 为 1 或 2。 * 最大池化 MaxPool (2x2 或 4x4) 和 平均池化 AveragePool (2x2 或 4x4)。 * 任意逐元素激活函数 (ReLU, ReLU6, LeakyRelu, Sigmoid...), KPU 不支持 PReLU。 ### 哪些算子可以被 KPU 部分加速？ * 非对称 paddings 或 valid paddings 卷积， nncase 会在其前后添加必要的 Pad 和 Crop（可理解为 边框 与 裁切）。 * 普通 Conv2D 和 DepthwiseConv2D，卷积核为 1x1 或 3x3，但 stride 不是 1 或 2。 nncase 会把它分解为 KPUConv2D 和一个 StridedSlice (可能还需要 Pad)。 * MatMul 算子， nncase 会把它替换为一个 Pad(到 4x4)+ KPUConv2D(1x1 卷积和) + Crop(到 1x1)。 * TransposeConv2D 算子， nncase 会把它替换为一个 SpaceToBatch + KPUConv2D + BatchToSpace。 > 以上说明来自[这里](https://github.com/kendryte/nncase/blob/master/docs/FAQ_ZH.md) ## 训练出浮点模型 对于 K210， 建议使用 TensorFlow，因为它的转换工具对其支持最好。 tensorflow 举个例子， 两分类模型， 这里是随便叠的层结构 ```python import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers input_shape (240, 320, 3) model tf.keras.models.Sequential() model.add(layers.ZeroPadding2D(input_shape input_shape, padding ((1, 1), (1, 1)))) model.add(layers.Conv2D(32, (3,3), padding 'valid', strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); #model.add(MaxPool2D()); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(32, (3,3), padding 'valid',strides (2, 2)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(32, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.ZeroPadding2D(padding ((1, 1), (1, 1)))); model.add(layers.Conv2D(64, (3,3), padding 'valid',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(64, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Conv2D(64, (3,3), padding 'same',strides (1, 1)));model.add(layers.BatchNormalization());model.add(layers.Activation('relu')); model.add(layers.Flatten()) model.add(layers.Dropout(0.5)) model.add(layers.Dense(2)) model.add(layers.Activation('softmax')) model.summary() model.compile( loss 'sparse_categorical_crossentropy', optimizer 'adam', metrics ['accuracy']) mode.fit(...) ``` 这里你可能注意到了, 在 `conv` 层中`stride ! 1` 时, 都加了一个 `zeropadding` 层, 这是 K210 硬件支持的模式, 如果不这样做, 转换成 V3 模型时(使用 nncase v0.1.0 RC5) 则直接报错, 使用 V4 模型(nncase V0.2.0转换)可以通过,但是是使用软件运算的, 会消耗大量内存和时间, 会发现内存占用大了很多!!! 所以设计模型时也需要注意 ## 转换工具 使用 K210 芯片官方提供的 [nncase](https://github.com/kendryte/nncase) 工具来进行转换。 需要注意的是，工具版本更新迭代比较多， `K210`属于第一代芯片，算子支持有限，并且内存只有`6MiB（通用）+2MiB（AI专用）`内存，所以最新版本的工具可能并不是最好的选择，根据需求选择合适的版本。 由于代码更新， 在过程中**模型格式**产生了两个大版本， `V3` 和 `V4`， 其中 `V3` 模型是指用 [nncase v0.1.0 RC5](https://github.com/kendryte/nncase/releases/tag/v0.1.0 rc5) 转换出来的模型； `V4`模型指用 [nncase v0.2.0](https://github.com/kendryte/nncase/releases/tag/v0.2.0 beta4) 转换出来的模型，以及 V5 或更新版本等等。 两者有一定的不同，所以现在两者共存， `V3` 代码量更少，占用内存小，效率也高，但是支持的算子少； `V4` 支持的算子更多，但是都是软件实现的，没有硬件加速，内存使用更多，所以各有所长。 `MaixPy` 的固件也可以选择是否支持 `V4`， 如果你的模型 `V3` 能够满足算子支持，强烈建议用 `V3`，在遇到算子不支持而且一定要用那个算子时再用`V4`。 ## 运行测试模型 使用 [MaixPy](/maixpy) 来运行模型，也可以用 [C SDK](https://github.com/sipeed/LicheeDan_K210_examples) 写。 比如使用 `MaixPy`固件， 将模型放到 SD 卡， 然后使用代码加载 ```python import KPU as kpu import image m kpu.load(\"/sd/test.kmodel\") img image.Image(\"/sd/test.jpg\") img img.resize(224, 224) img.pix_to_ai() feature_map kpu.forward(m, img) p_list feature_map[:] ``` ## 更多参考 * [K210 MaixPy 从入门到飞升 AI视觉篇 完全教程（以及一些小问题处理比如内存不足）](https://neucrack.com/p/325) * [MaixPy AI 硬件加速基本知识](/soft/maixpy/zh/course/ai/basic/maixpy_hardware_ai_basic.html) ## 上传分享到 MaixHub 可以上传分享你的模型到到 [MaixHub](https://maixhub.com/) 的模型库，可以让更多人发现并使用你的模型~ 一起做出更多有趣的项目吧！（K210 模型支持加密分享） 另外你也可以使用 [MaixHub](https://maixhub.com/) 的模型库，下载别人分享的模型，或者使用在线训练出的模型，直接使用或者参考模型结构。"},"/ai/zh/deploy/ax-pi.html":{"title":"部署模型到 Maix-III(M3) 系列 AXera-Pi 开发板","content":" title: 部署模型到 Maix III(M3) 系列 AXera Pi 开发板 date: 2022 09 21 #update: # date: 2022 09 30 # author: neucrack # content: 初版文档 <div id \"title_card\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/maix iii small.png\" alt \"AXera Pi 模型转换和部署\"> <div class \"card_info card_purple\"> <div class \"title\">Maix III 系列之 AXera Pi（爱芯派）</div> <div class \"brief\"> <div>高算力、独特 AI ISP 影像系统</div> <div>最高 3.6Tops@INT8，丰富算子支持</div> </div> </div> </div> </div> <style> #title_card { width:100%; text align:center; background color: #fafbfe; margin bottom: 1em; } #title_card img { max height: 20em; } .card_purple { background color: #d1c4e9; color: #673ab7; } .dark .card_purple { background color: #370040; color: #ffffffba; } .title { font size: 1.5em; font weight: 800; padding: 0.8em; } </style> > 有任何想法或者修改建议，欢迎留言或者直接点击右上角`编辑本页`进行修改后提交 PR > [MaixHub](https://maixhub.com/model/zoo) 模型库有 AXera Pi 能直接运行的模型，可以直接下载使用，也欢迎上传分享你的模型~ [点击可以查看 Maix III(M3) 系列 AXera Pi 开发板详情和基础使用文档](/hardware/zh/maixIII/index.html) 要部署模型到`AXera Pi`，需要将模型量化到 INT8，减小模型大小的同时提高运行速度，一般采用 `PTQ`(训练后量化)的方式量化模型，步骤： **1.** 准备好浮点模型。 **2.** 用模型量化和格式转换工具转换成 AXera Pi 支持的格式，这里工具使用爱芯官方提供的 [pulsar](https://pulsar docs.readthedocs.io) 。 > 本文提供了快速入门和流程向导，强烈建议先通看一遍，再查看 pulsar 文档获得更多具体内容。 **3.** 在 AXera Pi 上运行模型。 ## 准备浮点模型 使用 `Pytorch` 或者 `TensorFlow` 训练好模型， 将模型保存为 `onnx` 格式备用。 需要注意只能使用 `AXera Pi` 所支持的算子，见[算子支持列表](https://pulsar docs.readthedocs.io/zh_CN/latest/appendix/op_support_list.html)。 对于某些网络，可能需要将后处理剥离出来，使用`CPU`处理。 ### 举例 比如我们用一个 PyTorch Hub 上面的 [mobilenetv2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/): ```python import torch model torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained True) model.eval() ``` 导出为 `onnx` 格式: ```python x torch.randn(1, 3, 224, 224) torch.onnx.export(model, x, \"mobilenetv2.onnx\", export_params True, verbose True, opset_version 11) ``` 有些模型可以使用 `onnxsim` 精简下网络结构（这个模型实际上不需要）： ``` pip install onnx simplifier python m onnxsim mobilenetv2.onnx mobilenetv2 sim.onnx ``` ## 模型量化和格式转换 ### 安装`docker` [安装教程](https://docs.docker.com/engine/install/) 安装好后查看 ``` docker version ``` `Linux`下添加当前用户到 `docker`组，这样就不需要使用 `sudo` 运行`docker`命令了。可以用以下命令来添加： ```shell sudo gpasswd a $USER docker newgrp docker ``` ### 下载转换工具 转换工具是以 docker 镜像的方式提供的，下载后用 docker 加载镜像即可。 下载站点 简介 使用方法 : : : [dockerhub](https://hub.docker.com/r/sipeed/pulsar/tags) 执行命令即可在线下载 `docker pull sipeed/pulsar` dockerhub 国内镜像 中国国内下载加速 1. 编辑`/etc/docker/daemon.json`添加`\"registry mirrors\": [\"https://docker.mirrors.ustc.edu.cn\"],`(也可以用其它镜像比如[阿里云](https://cr.console.aliyun.com/cn hangzhou/instances/mirrors))<br>2. 然后 `docker pull sipeed/pulsar` 拉取镜像 拉取完成后使用`docker images`命令可以看到`sipeed/pulsar:latest`镜像 > 注意这里镜像名叫 `sipeed/pulsar`， 在文档里面有些地方可能是 `axera/neuwizard`，是等效的，只是名字不同 然后创建容器： ```shell docker run it net host rm shm size 32g v $PWD:/data sipeed/pulsar ``` > * 这里` shm size`共享内run大小根据你的电脑内存大小设置。 > * 不用` rm`会保留容器，建议加个` name xxx`来命名容器，下次通过`docker start xxx && docker attach xxx`进入容器 > * ` v 宿主机路径:/data`是把宿主机的目录挂载到容器的`/data`目录，这样就可以在容器里面直接操作宿主机的文件了。 创建容器后会自动进入容器，使用 `pulsar h` 命令可以看到相关命令。 ### 进行模型量化和转换 然后看 [pulsar](https://pulsar docs.readthedocs.io) 文档中的转换命令以及配置文件方法进行模型量化和格式转换。 > 注意 `AXera Pi` 使用了虚拟 NPU 的概念来划分算力，以将实现全部算力给 NPU 或者 NPU 和 AI ISP 各分一半。 #### 举例 仍然以 `mobilenetv2` 为例： * 根据`pulsar`文档，准备好配置文件`config_mobilenetv2.prototxt`, (具体格式说明见[配置文件详细说明](https://pulsar docs.readthedocs.io/zh_CN/latest/test_configs/config.html)),内容如下： .. details:: config_mobilenetv2.prototxt ```protobuf # 基本配置参数：输入输出 input_type: INPUT_TYPE_ONNX output_type: OUTPUT_TYPE_JOINT # 硬件平台选择 target_hardware: TARGET_HARDWARE_AX620 # CPU 后端选择，默认采用 AXE cpu_backend_settings { onnx_setting { mode: DISABLED } axe_setting { mode: ENABLED axe_param { optimize_slim_model: true } } } # onnx 模型输入数据类型描述 src_input_tensors { color_space: TENSOR_COLOR_SPACE_RGB } # joint 模型输入数据类型设置 dst_input_tensors { color_space: TENSOR_COLOR_SPACE_RGB } # neuwizard 工具的配置参数 neuwizard_conf { operator_conf { input_conf_items { attributes { input_modifications { # y x * (slope / slope_divisor) + (bias / bias_divisor) # 这里就是先把数据归一到[0, 1] affine_preprocess { slope: 1 slope_divisor: 255 bias: 0 } } input_modifications { # y (x mean) / std # 按照训练的时候的参数标准化 input_normalization { mean: [0.485,0.456,0.406] ## 均值， 注意这里的顺序根据 src_input_tensors.color_space 而定， 比如这里是 [R G B] std: [0.229,0.224,0.255] ## 方差 } } } } } dataset_conf_calibration { path: \"imagenet 1k images rgb.tar\" # 设置 PTQ 校准数据集路径 type: DATASET_TYPE_TAR # 数据集类型：tar 包 size: 256 # 量化校准过程中实际使用的图片张数 batch_size: 1 } } # pulsar compiler 的 batch size 配置参数 pulsar_conf { ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111 # 使用虚拟 NPU， NPU 和 AI ISP 各分一半， 111 代表 NPU # AX620_VIRTUAL_NPU_MODE_0 # 不使用虚拟 NPU， 全部算力给 NPU # AX620_VIRTUAL_NPU_MODE_112 # 使用虚拟 NPU， NPU 和 AI ISP 各分一半， 112 代表 AI ISP 特用，千万别乱设置 batch_size: 1 } ``` > 注意这里的预处理要和训练模型时相同，即 [mobilenetV2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/) 的预处理说明，这里是先归一再减`mean`除以`std`。 > 以及`imagenet 1k images rgb.tar`是从训练集中抠出来的一部分图片，用于量化校准，在这里训练集是`imagenet`，测试用可以在[百度云](https://pan.baidu.com/s/1TiZSIm0fpqbLn 2qLBX58g?pwd 1rpb)或者[github](https://github.com/sipeed/sipeed_wiki/releases/download/v0.0.0/imagenet 1k images rgb.tar)下载，也可以你自己从[imagenet](https://www.image net.org/)里面抠图出来。TODO: 需不需要手动先缩放到输入尺寸？ > 这里 `ax620_virtual_npu` 设置为了 `AX620_VIRTUAL_NPU_MODE_111`，这很重要，如果要用摄像头的话，必须要设置为这个， 否则可能会初始化失败，因为默认使用摄像头开启了 `AI ISP` 会启用虚拟 NPU 并使用另一半算力，为保证我们第一次能使用，先这样设置，后面会详细介绍。 然后在 `docker`容器里面执行（注意文件通过前面`docker run`的` v`参数指定了挂载宿主机的目录到`docker`容器里面，直接拷贝到宿主机的目录就好了）： ``` pulsar build input mobilenetv2.onnx output mobilenetv2.joint config config_mobilenetv2.prototxt output_config out_config_mobilenet_v2.prototxt ``` 耐心等待，可能需要一小会儿时间，就会得到转换的模型结果`mobilenetv2.joint`了 ### 在 docker 中使用 GPU 进行模型量化和格式转换 默认 docker 不能使用显卡驱动，但是有要用的话方法也不难： * 宿主机正常安装显卡驱动，比如 `ubuntu` 直接可以在包管理器中安装 * 按照 [nvidia docker](https://github.com/NVIDIA/nvidia docker) 的说明安装， 然后测试是否可用： ``` docker run rm gpus all nvidia/cuda:11.0.3 base ubuntu20.04 nvidia smi ``` 这会执行 `nvidia smi` 命令，就可以看到映射到 docker 里面的显卡信息了。 * 在需要使用显卡的容器创建时加上 ` gpus all` 参数来将所有显卡驱动映射到容器内，也可以指定特定显卡编号 ` gpus '\"device 2,3\"'`, 比如： ``` docker run it net host rm gpus all shm size 32g v $PWD:/data sipeed/pulsar ``` 注意当前版本（0.6.1.20）的 `pulsar build` 仅支持 sm_37 sm_50 sm_60 sm_70 sm_75 架构 GPU，30/40 系列 GPU 暂不支持。 ## 在 AXera Pi 上测试运行模型 按照文档转换模型后，将模型通过 `scp` 或者 `adb` 传到 `AXera Pi` 上，使用文档中的模型测试运行命令运行模型即可。 ### 举例 仍然以`mobilenetv2`为例： 测试图片保存到`cat.jpg`： <img src \"../../assets/cat.jpg\" style \"max height: 20em;\"> * 先在电脑上跑一下和`onnx`的结果对比: ``` pulsar run mobilenetv2.onnx mobilenetv2.joint input cat.jpg config out_config_mobilenet_v2.prototxt output_gt gt ``` 得到余弦距离，这里为 `0.9862` ，说明 `joint` 模型和 `onnx` 模型的输出结果相似度 `98.62%`，在可以接受的范围内，如果值太小，则表示量化过程中出现了误差，需要考虑是不是设置有误或者量化输入数据有误或者模型设计有问题。 ```log Layer: 536 2 norm RE: 17.03% cosine sim: 0.9862 ``` * 拷贝模型到 `AXera Pi` 上直接跑一下(通过 `scp` 命令拷贝 `joint` 格式模型文件到开发板）： 在板子上运行模型： ``` time run_joint mobilenetv2.joint repeat 100 warmup 10 ``` 可以看到模型运行时间 `2.1ms`，这里我们没有启用虚拟 NPU，如果启用了虚拟 NPU，则时间加倍为 `4ms`。以及 `overhead 250.42 ms` 即其它耗时（比如 模型加载 内存分配 等）。 ``` Run task took 2143 us (99 rounds for average) Run NEU took an average of 2108 us (overhead 9 us) ``` 如果要测试输入，需要先将图片转换为二进制内容，`HWC + RGB` 排列，用 ` data` 指定二进制文件。 .. details::转换为二进制文件脚本 ```python from PIL import Image import sys out_path sys.argv[2] img Image.open(sys.argv[1]) img img.convert('RGB') img img.resize((224, 224)) rgb888 img.tobytes() with open(out_path, \"wb\") as f: f.write(rgb888) ``` 执行`python convert.py cat.jpg cat.bin`，得到`cat.bin`文件。 ``` run_joint mobilenetv2.joint data cat.bin bin out dir ./ ``` 然后目录下会生成一个`bin`文件，大小是`4000`个字节，即`1000`个`float32`，可以用`python`加载找出最大值 ```python out np.fromfile(\"536.bin\", dtype np.float32) print(out.argmax(), out.max()) ``` 得到结果`282 8.638927`，在[labels](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt)中找到下标第`282`即`283`行，是`tiger cat`，结果和直接电脑运行浮点模型结果`282 9.110947`也一致，虽然值有略微差异，但是在可以接受的范围内。 > 注意这里没有进行`softmax`计算，`out.max()`值不是概率值。 ## 编写代码运行模型 要正式地将模型跑起来，你可能需要需要修改代码，更改输入预处理或者增加后处理，目前提供 `C/C++` SDK，代码参考 [ax samples](https://github.com/AXERA TECH/ax samples)，可以交叉编译，也可以直接在 `AXera Pi` 上编译。 运行分类模型的代码在[ax_classification_steps.cc](https://github.com/AXERA TECH/ax samples/blob/main/examples/ax_classification_steps.cc)，按照仓库的编译说明编译后得到`build/bin/install/ax_classification`可执行文件，拷贝到开发板执行 ``` ./ax_classification m mobilenetv2.joint i cat.jpg ``` > 在代码中使用了 `opencv` 读取了图片，格式为 `BGR`，在运行模型时会根据转换模型时的配置自动判断是否转成 `RGB`，所以直接用了 `mw::prepare_io` 拷贝 `BGR` 的图到输入缓冲区了，后面就交给底层库处理了。 如果你的模型不是简单的分类模型，那你可能需要在模型推理结束后添加后处理的代码以达到解析结果的目的。 ## 使用摄像头和屏幕 到此，模型单独运行已经走通了，如果要基于此做应用，因为是 `Linux`，有很多通用的开发方法和库，如果你希望将摄像头和屏幕联合起来使用，可以使用[libmaix](https://github.com/sipeed/libmaix)，或者你也可以直接使用[axpi_bsp_sdk](https://github.com/sipeed/axpi_bsp_sdk)进行开发（难度会比较大一点）。 * 看[SDK 开发说明](/hardware/zh/maixIII/ax pi/sdk.html) 编译并执行[摄像头屏幕显示例程](https://github.com/sipeed/libmaix/tree/release/examples/axpi)，因为仓库都在`github`，所以最好是有个好的代理服务器。 * 将模型运行代码合并到例程中，有个问题要十分注意，如果要使用摄像头，默认`AI ISP`会打开（暂未提供关闭的方式，后面更新TODO:），**所以模型转换的时候要指定模型为虚拟NPU运行，即配置文件中设置`ax620_virtual_npu: AX620_VIRTUAL_NPU_MODE_111`，否则初始化会失败**。 > 可以直接使用[1000分类例程](https://github.com/sipeed/libmaix/tree/release/examples/axpi_classification_cam). > 在板子编译通过后执行`./dist/axpi_classification_cam m mobilenetv2.joint` 即可开始识别。模型也可以到[MaixHub 模型库下载](https://maixhub.com/model/zoo/89) ## QAT 量化 和其它优化方法 `QAT`(Quantization aware training)即量化感知训练，和 `PTQ` 在对训练好的模型进行浏览量化的做法不同，`QAT` 是在训练时就模拟量化推理，以减少量化误差， 和训练后量化 `PTQ` 相比，有更高的精度，但是过程会更复杂，不建议一开始就使用。 更多详情看[superpulsar](https://pulsar docs.readthedocs.io)， 文档会持续更新，如果你擅长这方面，也欢迎点击右上角`编辑本页`在这里添加说明。 ## 其它参考和分享汇总 > 欢迎贴上你的分享，点击右上角`编辑本页`添加。 * [爱芯元智AX620A部署yolov5 6.0模型实录](https://zhuanlan.zhihu.com/p/569083585) * [AX620A运行yolov5s自训练模型全过程记录（windows）](http://t.csdn.cn/oNeYG) * [MOT：如何在爱芯派上实现多目标跟踪的神奇效果！](https://www.yuque.com/prophetmu/chenmumu/ax_tracker) * [MMPose：在爱芯派上玩转你的关键点检测！](https://www.yuque.com/prophetmu/chenmumu/m3axpi_keypoint) * [2023年最新 使用 YOLOv8 训练自己的数据集，并在 爱芯派硬件 上实现 目标检测 和 钢筋检测 ！！](https://www.yuque.com/prophetmu/chenmumu/m3axpi)"},"/ai/zh/deploy/index.html":{"title":"制作可部署到边缘设备的模型和部署方法汇总","content":" title: 制作可部署到边缘设备的模型和部署方法汇总 keywords: 模型部署, 模型转换, 模型部署边缘设备 desc: 汇总如何部署模型到边缘设备的相关文档，边缘设备包含单片机、SBC、SOC、NPU等。 date: false class: heading_no_counter <div id \"maixhub\"> 到<a href \"https://maixhub.com/model/zoo\">MaixHub 查看</a>或<a href \"https://maixhub.com/model/zoo/share\">上传分享</a>能直接部署到边缘设备的模型 </div> <div id \"deploy_items\"> <a href \"./k210.html\"> <div class \"card\"> <img src \"/hardware/zh/maix/assets/dk_board/maix_duino/maixduino_0.png\" alt \"K210 模型转换和部署\"> <div class \"card_info card_red\"> <h2>Maix I 系列 K210</h2> <div class \"brief\"> <div>高性价比带硬件 AI 加速的单片机</div> <div>1Tops@INT8，有限算子加速</div> </div> </div> </div> </a> <a href \"./v831.html\"> <div class \"card\"> <img src \"/hardware/assets/maixII/m2dock.jpg\" alt \"V831 模型转换和部署\"> <div class \"card_info card_blue\"> <h2>Maix II 系列 v831</h2> <div class \"brief\"> <div>高性价比带硬件 AI 加速，支持 Linux</div> <div>0.2Tops@INT8，有限算子加速</div> </div> </div> </div> </a> <a href \"./tinymaix.html\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/m0_small.png\" alt \"TinyMaix 模型转换和部署\"> <div class \"card_info card_green\"> <h2>TinyMaix 平台</h2> <div class \"brief\"> <div>单片机通用，为各种指令集优化</div> <div>算力具体看硬件 CPU，有限算子加速</div> </div> </div> </div> </a> <a href \"./ax pi.html\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/maix iii small.png\" alt \"AXera Pi 模型转换和部署\"> <div class \"card_info card_purple\"> <h2>Maix III 系列 AXera Pi</h2> <div class \"brief\"> <div>高算力、独特 AI ISP 影像系统</div> <div>最高 3.6Tops@INT8，丰富算子支持</div> </div> </div> </div> </a> </div> <style> #deploy_items { display: flex; justify content: space evenly; flex wrap: wrap; margin: 0 10px; } #deploy_items a:hover { background color: transparent; } #deploy_items > a { margin: 1em; } .card { display: flex; flex direction: column; justify content: space between; align items: center; box shadow: 5px 6px 20px 4px rgba(0, 0, 0, 0.1); border radius: 0.6rem; transition: 0.4s; background: white; } .card:hover { box shadow: 5px 6px 40px 4px rgba(0, 0, 0, 0.1); scale: 1.05; } .card_info { display: flex; flex direction: column; align items: center; border radius: 0 0 0.6rem 0.6rem; } .card img { height: 10em; width: 14em; object fit: cover; } .card_info > h2 { font size: 1.2em; margin: 0.2em; padding: 0.2em 1em; } .card_info > .brief { margin: 0.2em; padding: 0.2em 1em; display: flex; flex direction: column; align items: center; } .card_red { background color: #ffcdd2; color: #cf4f5a; } .card_blue { background color: #90caf9; color: #105aa9; } .card_green { background color: #b2dfdb; color: #009688; } .card_purple { background color: #d1c4e9; color: #673ab7; } #maixhub { display: flex; justify content: center; align items: center; margin: 1em 0; width: 100%; background color: #f5f5f5; color: #727272; border radius: 0.6rem; padding: 1em; } .dark #maixhub { background color: #2d2d2d; color: #bfbfbf; } .dark .card_blue { background color: #003c6c; color: #ffffffba; } .dark .card_red { background color: #5a0000; color: #ffffffba; } .dark .card_green { background color: #004e03; color: #ffffffba; } .dark .card_purple { background color: #370040; color: #ffffffba; } </style>"},"/ai/zh/deploy/tinymaix.html":{"title":"使用 TinyMaix 将模型部署到单片机","content":" title: 使用 TinyMaix 将模型部署到单片机 date: 2022 09 15 <div id \"title_card\"> <div class \"card\" style \"background color: #fafbfe\"> <img src \"../../assets/m0_small.png\" alt \"TinyMaix 模型转换和部署\"> <div class \"card_info card_green\"> <div class \"title\">TinyMaix 平台</div> <div class \"brief\"> <div>单片机通用，为各种指令集优化</div> <div>算力具体看硬件 CPU，有限算子加速</div> </div> </div> </div> </div> <style> #title_card { width:100%; text align:center; background color: #fafbfe; margin bottom: 1em; } #title_card img { max height: 20em; } .card_green { background color: #b2dfdb; color: #009688; } .dark .card_green { background color: #004e03; color: #ffffffba; } .title { font size: 1.5em; font weight: 800; padding: 0.8em; } </style> [TinyMaix](https://github.com/sipeed/TinyMaix) 是针对小算力小内存的芯片设计的轻量级推理框架，甚至能在`2KB`内存的`Arduino ATmega328`单片机上运行`MNIST`，对各种架构的单片机都提供了支持和优化，包括 RISC V、ARM Cortex M 等。 详细使用方法请看[TinyMaix 官方仓库](https://github.com/sipeed/TinyMaix)"},"/ai/zh/deploy/v831.html":{"title":"将模型部署到 V831","content":" title: 将模型部署到 V831 date: 2022 09 15 <div id \"title_card\"> <div class \"card\"> <img src \"/hardware/assets/maixII/m2dock.jpg\" alt \"V831 模型转换和部署\"> <div class \"card_info card_blue\"> <div class \"title\">Maix II 系列 v831</div> <div class \"brief\"> <div>高性价比带硬件 AI 加速，支持 Linux</div> <div>0.2Tops@INT8，有限算子加速</div> </div> </div> </div> </div> <style> #title_card { width:100%; text align:center; background color: white; margin bottom: 1em; } #title_card img { max height: 20em; } .card_blue { background color: #90caf9; color: #105aa9; } .dark .card_blue { background color: #003c6c; color: #ffffffba; } .title { font size: 1.5em; font weight: 800; padding: 0.8em; } </style> ## 制作浮点模型 对于 V831， 强烈推荐使用`Pytorch`训练模型，因为模型转换工具对其支持较好。 这里直接使用 pytorch hub 的预训练模型为例。 这里省略了模型定义和训练过程， 直接使用 pytorch hub 的 resnet18 预训练模型进行简单介绍： https://pytorch.org/hub/pytorch_vision_resnet/ 注意 V831 支持的算子有限，具体请在 [MaixHub](https://maixhub.com/) 点击`工具箱 >模型转换 >v831`中的文档查看。 ## 在 PC 端测试模型推理 根据上面链接的使用说明， 使用如下代码可以运行模型 其中， label 下载： https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt ```python import os import torch from torchsummary import summary ## model model torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained True) model.eval() input_shape (3, 224, 224) summary(model, input_shape, device \"cpu\") ## test image filename \"out/dog.jpg\" if not os.path.exists(filename): if not os.path.exists(\"out\"): os.makedirs(\"out\") import urllib url, filename (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename) try: urllib.URLopener().retrieve(url, filename) except: urllib.request.urlretrieve(url, filename) print(\"test image:\", filename) ## preparing input data from PIL import Image import numpy as np from torchvision import transforms input_image Image.open(filename) # input_image.show() preprocess transforms.Compose([ transforms.Resize(max(input_shape[1:3])), transforms.CenterCrop(input_shape[1:3]), transforms.ToTensor(), transforms.Normalize(mean [0.485, 0.456, 0.406], std [0.229, 0.224, 0.225]), ]) input_tensor preprocess(input_image) print(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor))) input_batch input_tensor.unsqueeze(0) # create a mini batch as expected by the model ## forward model # move the input and model to GPU for speed if available if torch.cuda.is_available(): input_batch input_batch.to('cuda') model.to('cuda') with torch.no_grad(): output model(input_batch) ## result # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes # print(output[0]) # The output has unnormalized scores. To get probabilities, you can run a softmax on it. max_1000 torch.nn.functional.softmax(output[0], dim 0) max_idx int(torch.argmax(max_1000)) with open(\"imagenet_classes.txt\") as f: labels f.read().split(\"\\n\") print(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx])) ``` 运行后 结果： ``` Using cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0 Layer (type) Output Shape Param # Conv2d 1 [ 1, 64, 112, 112] 9,408 BatchNorm2d 2 [ 1, 64, 112, 112] 128 ReLU 3 [ 1, 64, 112, 112] 0 MaxPool2d 4 [ 1, 64, 56, 56] 0 Conv2d 5 [ 1, 64, 56, 56] 36,864 BatchNorm2d 6 [ 1, 64, 56, 56] 128 ReLU 7 [ 1, 64, 56, 56] 0 Conv2d 8 [ 1, 64, 56, 56] 36,864 BatchNorm2d 9 [ 1, 64, 56, 56] 128 ReLU 10 [ 1, 64, 56, 56] 0 BasicBlock 11 [ 1, 64, 56, 56] 0 Conv2d 12 [ 1, 64, 56, 56] 36,864 BatchNorm2d 13 [ 1, 64, 56, 56] 128 ReLU 14 [ 1, 64, 56, 56] 0 Conv2d 15 [ 1, 64, 56, 56] 36,864 BatchNorm2d 16 [ 1, 64, 56, 56] 128 ReLU 17 [ 1, 64, 56, 56] 0 BasicBlock 18 [ 1, 64, 56, 56] 0 Conv2d 19 [ 1, 128, 28, 28] 73,728 BatchNorm2d 20 [ 1, 128, 28, 28] 256 ReLU 21 [ 1, 128, 28, 28] 0 Conv2d 22 [ 1, 128, 28, 28] 147,456 BatchNorm2d 23 [ 1, 128, 28, 28] 256 Conv2d 24 [ 1, 128, 28, 28] 8,192 BatchNorm2d 25 [ 1, 128, 28, 28] 256 ReLU 26 [ 1, 128, 28, 28] 0 BasicBlock 27 [ 1, 128, 28, 28] 0 Conv2d 28 [ 1, 128, 28, 28] 147,456 BatchNorm2d 29 [ 1, 128, 28, 28] 256 ReLU 30 [ 1, 128, 28, 28] 0 Conv2d 31 [ 1, 128, 28, 28] 147,456 BatchNorm2d 32 [ 1, 128, 28, 28] 256 ReLU 33 [ 1, 128, 28, 28] 0 BasicBlock 34 [ 1, 128, 28, 28] 0 Conv2d 35 [ 1, 256, 14, 14] 294,912 BatchNorm2d 36 [ 1, 256, 14, 14] 512 ReLU 37 [ 1, 256, 14, 14] 0 Conv2d 38 [ 1, 256, 14, 14] 589,824 BatchNorm2d 39 [ 1, 256, 14, 14] 512 Conv2d 40 [ 1, 256, 14, 14] 32,768 BatchNorm2d 41 [ 1, 256, 14, 14] 512 ReLU 42 [ 1, 256, 14, 14] 0 BasicBlock 43 [ 1, 256, 14, 14] 0 Conv2d 44 [ 1, 256, 14, 14] 589,824 BatchNorm2d 45 [ 1, 256, 14, 14] 512 ReLU 46 [ 1, 256, 14, 14] 0 Conv2d 47 [ 1, 256, 14, 14] 589,824 BatchNorm2d 48 [ 1, 256, 14, 14] 512 ReLU 49 [ 1, 256, 14, 14] 0 BasicBlock 50 [ 1, 256, 14, 14] 0 Conv2d 51 [ 1, 512, 7, 7] 1,179,648 BatchNorm2d 52 [ 1, 512, 7, 7] 1,024 ReLU 53 [ 1, 512, 7, 7] 0 Conv2d 54 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 55 [ 1, 512, 7, 7] 1,024 Conv2d 56 [ 1, 512, 7, 7] 131,072 BatchNorm2d 57 [ 1, 512, 7, 7] 1,024 ReLU 58 [ 1, 512, 7, 7] 0 BasicBlock 59 [ 1, 512, 7, 7] 0 Conv2d 60 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 61 [ 1, 512, 7, 7] 1,024 ReLU 62 [ 1, 512, 7, 7] 0 Conv2d 63 [ 1, 512, 7, 7] 2,359,296 BatchNorm2d 64 [ 1, 512, 7, 7] 1,024 ReLU 65 [ 1, 512, 7, 7] 0 BasicBlock 66 [ 1, 512, 7, 7] 0 AdaptiveAvgPool2d 67 [ 1, 512, 1, 1] 0 Linear 68 [ 1, 1000] 513,000 Total params: 11,689,512 Trainable params: 11,689,512 Non trainable params: 0 Input size (MB): 0.57 Forward/backward pass size (MB): 62.79 Params size (MB): 44.59 Estimated Total Size (MB): 107.96 out/dog.jpg tensor(2.6400) tensor( 2.1008) idx:258, name:Samoyed, Samoyede ``` 可以看到模型有 `11,689,512`的参数， 即 `11MiB`左右， 这个大小也就几乎是实际在 831 上运行的模型的大小了 ## 将模型转换为 V831 能使用的模型文件 转换过程如下： ### 使用 Pytorch 将模型导出为 `onnx`模型， 得到`onnx`文件 ```python def torch_to_onnx(net, input_shape, out_name \"out/model.onnx\", input_names [\"input0\"], output_names [\"output0\"], device \"cpu\"): batch_size 1 if len(input_shape) 3: x torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype torch.float32, requires_grad True).to(device) elif len(input_shape) 1: x torch.randn(batch_size, input_shape[0], dtype torch.float32, requires_grad False).to(device) else: raise Exception(\"not support input shape\") print(\"input shape:\", x.shape) # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params True) torch.onnx.export(net, x, out_name, export_params True, input_names input_names, output_names output_names) onnx_out \"out/resnet_1000.onnx\" ncnn_out_param \"out/resnet_1000.param\" ncnn_out_bin \"out/resnet_1000.bin\" input_img filename torch_to_onnx(model, input_shape, onnx_out, device \"cuda:0\") ``` 如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字 ## 使用 `onnx2ncnn` 工具将`onnx`转成`ncnn`模型，得到一个`.param`文件和一个`.bin`文件 >! 这一步可以跳过。 > 按照[ncnn项目](https://github.com/Tencent/ncnn)的编译说明编译，在`build/tools/onnx`目录下得到`onnx2ncnn`可执行文件 ```python def onnx_to_ncnn(input_shape, onnx \"out/model.onnx\", ncnn_param \"out/conv0.param\", ncnn_bin \"out/conv0.bin\"): import os # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir cmd f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\" os.system(cmd) with open(ncnn_param) as f: content f.read().split(\"\\n\") if len(input_shape) 1: content[2] + \" 0 {}\".format(input_shape[0]) else: content[2] + \" 0 {} 1 {} 2 {}\".format(input_shape[2], input_shape[1], input_shape[0]) content \"\\n\".join(content) with open(ncnn_param, \"w\") as f: f.write(content) onnx_to_ncnn(input_shape, onnx onnx_out, ncnn_param ncnn_out_param, ncnn_bin ncnn_out_bin) ``` ## 使用全志提供的`awnn`工具将`ncnn`模型进行量化到`int8`模型 在 [MaixHub](https://maixhub.com/) 点击`工具箱 >模型转换 >v831`进入模型转换页面， 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做） 在转换页面有更多的转换说明，可以获得更多详细的转换说明 这里有几组参数： * 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值 mean ) / std`, `awnn`对输入的处理是 `(输入值 mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给`awnn`量化工具经过` (输入值 mean ) * norm` 计算后的值范围一致既可。 比如 这里打印了实际数据的输入范围是`[ 2.1008, 2.6400]`， 是代码中`preprocess` 对象处理后得到的，即`x (x mean) / std` > `(0 0.485)/0.229 2.1179`, 到`awnn`就是`x (x mean_2*255) * (1 / std * 255)` 即 `mean2 mean * 255`, `norm 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ ncnn produce wrong result#pre process)。 所以我们这里可以设置 均值为 `0.485 * 255 123.675`， 设置 归一化因子为`1/ (0.229 * 255) 0.017125`， 另外两个通道同理，但是目前 awnn 只能支持三个通道值一样。。。所以填`123.675, 123.675, 123.675`，`0.017125, 0.017125, 0.017125` 即可，因为这里用了`pytorch hub`的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下 * 图片输入层尺寸（问不是图片怎么办？貌似 awnn 暂时只考虑到了图片。。） * RGB 格式： 如果训练输入的图片是 RGB 就选 RGB * 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手 自己写的其它模型转换如果失败，多半是啥算子不支持，需要在 使用说明里面看支持的 算子，比如之前的版本view、 flatten、reshape 都不支持所以写模型要相当小心， 现在的版本会支持 flatten reshape 等 CPU 算子 如果不出意外， 终于得到了量化好的 awnn 能使用的模型， `*.param` 和 `*.bin` ## 使用模型，在v831上推理 可以使用 python 或者 C 写代码，以下两种方式 ### MaixPy3 python 请看[MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/) 不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了： ``` pip install upgrade maixpy3 ``` 然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py label 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py ```python from maix import nn from PIL import Image, ImageDraw from maix import camera, display test_jpg \"/root/test_input/input.jpg\" model { \"param\": \"/root/models/resnet_awnn.param\", \"bin\": \"/root/models/resnet_awnn.bin\" } camera.config(size (224, 224)) options { \"model_type\": \"awnn\", \"inputs\": { \"input0\": (224, 224, 3) }, \"outputs\": { \"output0\": (1, 1, 1000) }, \"first_layer_conv_no_pad\": False, \"mean\": [127.5, 127.5, 127.5], \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196], } print(\" load model:\", model) m nn.load(model, opt options) print(\" load ok\") print(\" read image\") img Image.open(test_jpg) print(\" read image ok\") print(\" forward model with image as input\") out m.forward(img, quantize True) print(\" read image ok\") print(\" out:\", out.shape) out nn.F.softmax(out) print(out.max(), out.argmax()) from classes_label import labels while 1: img camera.capture() if not img: time.sleep(0.02) continue out m.forward(img, quantize True) out nn.F.softmax(out) msg \"{:.2f}: {}\".format(out.max(), labels[out.argmax()]) print(msg) draw ImageDraw.Draw(img) draw.text((0, 0), msg, fill (255, 0, 0)) display.show(img) ``` ### C语言 SDK， libmaix 访问这里，按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet 上传编译成功后`dist`目录下的所有内容到 `v831`, 然后执行`./start_app.sh`即可 ## 参考 * [在V831上（awnn）跑 pytorch resnet18 模型](https://neucrack.com/p/358)"}}